project_structure: C:\Users\rsbiiw\Projects\ECE_Core
scan_config:
  tokenLimit: 1000000
  maxFileSize: 5242880
  maxLinesPerFile: 5000
  includeExtensions:
    - .js
    - .ts
    - .jsx
    - .tsx
    - .py
    - .java
    - .cpp
    - .c
    - .h
    - .cs
    - .go
    - .rs
    - .rb
    - .php
    - .html
    - .css
    - .scss
    - .sass
    - .less
    - .json
    - .yaml
    - .yml
    - .xml
    - .sql
    - .sh
    - .bash
    - .zsh
    - .md
    - .txt
    - .csv
    - .toml
    - .ini
    - .cfg
    - .conf
    - .env
    - .dockerfile
    - dockerfile
    - .gitignore
    - .npmignore
    - .prettierignore
    - makefile
    - cmakelists.txt
    - readme.md
    - readme.txt
    - readme
    - license
    - license.md
    - changelog
    - changelog.md
    - contributing
    - contributing.md
    - code_of_conduct
    - code_of_conduct.md
  excludeExtensions:
    - .png
    - .jpg
    - .jpeg
    - .gif
    - .bmp
    - .ico
    - .svg
    - .webp
    - .exe
    - .bin
    - .dll
    - .so
    - .dylib
    - .zip
    - .tar
    - .gz
    - .rar
    - .7z
    - .pdf
    - .doc
    - .docx
    - .xls
    - .xlsx
    - .ppt
    - .pptx
    - .mp3
    - .mp4
    - .avi
    - .mov
    - .wav
    - .flac
    - .ttf
    - .otf
    - .woff
    - .woff2
    - .o
    - .obj
    - .a
    - .lib
    - .out
    - .class
    - .jar
    - .war
    - .swp
    - .swo
    - .lock
    - .cache
    - .log
    - .tmp
    - .temp
    - .DS_Store
    - Thumbs.db
  excludeDirectories:
    - .git
    - node_modules
    - archive
    - backups
    - logs
    - context
    - .vscode
    - .idea
    - .pytest_cache
    - __pycache__
    - dist
    - build
    - target
    - venv
    - env
    - .venv
    - .env
    - Pods
    - Carthage
    - CocoaPods
    - .next
    - .nuxt
    - public
    - static
    - assets
    - images
    - img
    - codebase
  excludeFiles:
    - combined_context.yaml
    - package-lock.json
    - yarn.lock
    - pnpm-lock.yaml
    - Gemfile.lock
    - Pipfile.lock
    - Cargo.lock
    - composer.lock
    - go.sum
    - go.mod
    - requirements.txt
    - poetry.lock
    - "*.db"
    - "*.sqlite"
    - "*.sqlite3"
    - "*.fdb"
    - "*.mdb"
    - "*.accdb"
    - "*~"
    - "*.tmp"
    - "*.temp"
    - "*.cache"
    - "*.swp"
    - "*.swo"
files:
  - path: .env
    content: "# ECE Core Environment Variables\r\n# Consolidated from sovereign.yaml and config.yaml\r\n\r\n# ============================================================\r\n# Core Server Configuration\r\n# ============================================================\r\nPORT=3000\r\nHOST=0.0.0.0\r\nAPI_KEY=ece-secret-key\r\nLOG_LEVEL=INFO\r\n\r\n# ============================================================\r\n# Infrastructure\r\n# ============================================================\r\nOVERLAY_PORT=3001\r\n\r\n# ============================================================\r\n# LLM & Inference Configuration\r\n# ============================================================\r\n# Paths are relative to engine/models or absolute\r\nLLM_MODEL_PATH=glm-edge-1.5b-chat.Q5_K_M.gguf\r\nLLM_CTX_SIZE=8192\r\nLLM_GPU_LAYERS=11\r\n\r\n\r\n# Orchestrator Model\r\nORCHESTRATOR_MODEL_PATH=glm-edge-1.5b-chat.Q5_K_M.gguf\r\nORCHESTRATOR_CTX_SIZE=2048\r\nORCHESTRATOR_GPU_LAYERS=11\r\n\r\n# Vision Model\r\nVISION_MODEL_PATH=C:/Users/rsbiiw/Projects/models/gemma-3-4b-it-abliterated-v2.i1-Q4_K_S.gguf\r\n# VISION_PROJECTOR_PATH=C:/Users/rsbiiw/Projects/models/mmproj-Qwen2-VL-2B-Instruct-f16.gguf\r\n\r\n# Embedding & Vector Search\r\nEMBEDDING_BATCH_SIZE=50\r\nVECTOR_INGEST_BATCH=100\r\nSIMILARITY_THRESHOLD=0.8\r\nLLM_EMBEDDING_DIM=768\r\n\r\n# ============================================================\r\n# Features\r\n# ============================================================\r\nFEATURE_CONTEXT_STORAGE=true\r\nFEATURE_MEMORY_RECALL=true\r\nFEATURE_CODA_ENABLED=true\r\nFEATURE_ARCHIVIST_ENABLED=true\r\nFEATURE_WEAVER_ENABLED=true\r\n\r\n# ============================================================\r\n# Tuning\r\n# ============================================================\r\nDREAMER_BATCH_SIZE=500\r\nMARKOVIAN_ENABLED=true\r\nCONTEXT_RECENT_TURNS=5\r\n"
    tokens: 565
    size: 1751
  - path: .gitignore
    content: |
      # Sensitive Data & Secrets
      .env
      .env.*
      *.env

      # Database & Storage
      context.db/
      backups/
      logs/
      *.log
      *.sqlite
      *.db

      # Model Archives & Heavy Binaries
      archive/
      models/
      *.gguf
      *.bin
      *.pth

      # Code Dependencies
      node_modules/
      dist/
      build/
      .pnpm-store/

      # IDE & System
      .vscode/
      .idea/
      .DS_Store
      Thumbs.db
      *.bak

      # Temporary/User Specific
      desktop-overlay/
      codebase/combined_context.yaml
      read_all.js
    tokens: 147
    size: 391
  - path: chk_db.ts
    content: "\r\nimport { db } from './engine/src/core/db.js';\r\n\r\nasync function check() {\r\n    await db.init();\r\n    const res = await db.run('?[]:=*memory{}');\r\n    console.log(`[DB Count] Rows: ${res.rows.length}`);\r\n    process.exit(0);\r\n}\r\n\r\ncheck().catch(console.error);\r\n"
    tokens: 101
    size: 263
  - path: desktop-overlay\package.json
    content: "{\r\n    \"name\": \"@ece/desktop-overlay\",\r\n    \"version\": \"1.0.0\",\r\n    \"main\": \"dist/main.js\",\r\n    \"scripts\": {\r\n        \"start\": \"electron .\",\r\n        \"build\": \"tsc\"\r\n    },\r\n    \"devDependencies\": {\r\n        \"electron\": \"^28.1.0\",\r\n        \"typescript\": \"^5.3.3\"\r\n    }\r\n}"
    tokens: 95
    size: 274
  - path: desktop-overlay\src\main.ts
    content: "\r\nimport { app, BrowserWindow, screen } from 'electron';\r\nimport path from 'path';\r\n\r\n// Config\r\nconst FRONTEND_URL = process.env.FRONTEND_URL || 'http://localhost:3000';\r\n\r\nlet mainWindow: BrowserWindow | null = null;\r\n\r\nfunction createWindow() {\r\n    const primaryDisplay = screen.getPrimaryDisplay();\r\n    const { width, height } = primaryDisplay.workAreaSize;\r\n\r\n    mainWindow = new BrowserWindow({\r\n        width: 600,\r\n        height: 800,\r\n        x: width - 650,\r\n        y: 50,\r\n        frame: false, // Overlay style\r\n        alwaysOnTop: true,\r\n        transparent: true,\r\n        webPreferences: {\r\n            nodeIntegration: false,\r\n            contextIsolation: true,\r\n        },\r\n    });\r\n\r\n    mainWindow.loadURL(FRONTEND_URL);\r\n\r\n    mainWindow.on('closed', () => {\r\n        mainWindow = null;\r\n    });\r\n}\r\n\r\napp.on('ready', createWindow);\r\n\r\napp.on('window-all-closed', () => {\r\n    if (process.platform !== 'darwin') {\r\n        app.quit();\r\n    }\r\n});\r\n\r\napp.on('activate', () => {\r\n    if (mainWindow === null) {\r\n        createWindow();\r\n    }\r\n});\r\n"
    tokens: 359
    size: 1074
  - path: desktop-overlay\tsconfig.json
    content: "{\r\n    \"compilerOptions\": {\r\n        \"target\": \"ES6\",\r\n        \"module\": \"CommonJS\",\r\n        \"outDir\": \"./dist\",\r\n        \"rootDir\": \"./src\",\r\n        \"strict\": true,\r\n        \"esModuleInterop\": true,\r\n        \"skipLibCheck\": true\r\n    },\r\n    \"include\": [\r\n        \"src/**/*\"\r\n    ]\r\n}"
    tokens: 89
    size: 287
  - path: engine\bin\llama.cpp.txt
    content: "MIT License\r\n\r\nCopyright (c) 2023-2024 The ggml authors\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the \"Software\"), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n"
    tokens: 447
    size: 1099
  - path: engine\package.json
    content: |
      {
          "name": "sovereign-context-engine",
          "version": "3.0.0",
          "type": "module",
          "description": "Headless Context Engine & Knowledge Graph",
          "main": "src/index.js",
          "bin": "src/index.js",
          "scripts": {
              "start": "node dist/index.js",
              "dev": "ts-node src/index.ts",
              "migrate": "node src/migrate_history.js",
              "read-all": "node src/read_all.js",
              "hydrate": "node src/hydrate.js",
              "test": "node tests/all_routes_and_services.js",
              "test:routes": "node tests/all_routes_and_services.js",
              "test:quick": "node tests/suite.js",
              "test:benchmark": "node tests/benchmark.js",
              "test:context": "node tests/context_experiments.js",
              "benchmark": "node tests/benchmark.js",
              "build": "tsc",
              "build:standalone": "tsc && pkg .",
              "lint": "eslint src/ --ext .ts,.js",
              "lint:fix": "eslint src/ --ext .ts,.js --fix"
          },
          "pkg": {
              "assets": [
                  "src/**/*",
                  "node_modules/cozo-node/native/**/*",
                  "context/**/*",
                  "codebase/**/*",
                  "specs/**/*",
                  "shared/**/*",
                  "!.env",
                  "!.env.*",
                  "!*.log",
                  "!logs/",
                  "!node_modules/@types/"
              ],
              "targets": [
                  "node18-win-x64"
              ],
              "outputPath": "dist",
              "compress": "GZip"
          },
          "dependencies": {
              "@ece/shared": "workspace:*",
              "@types/turndown": "^5.0.6",
              "@xenova/transformers": "^2.17.2",
              "axios": "^1.13.2",
              "body-parser": "^1.20.2",
              "cheerio": "^1.1.2",
              "chokidar": "^3.6.0",
              "cors": "^2.8.5",
              "cozo-node": "^0.7.6",
              "dotenv": "^16.3.1",
              "express": "^4.18.2",
              "got": "^14.6.6",
              "got-scraping": "^4.1.3",
              "js-yaml": "^4.1.1",
              "node-llama-cpp": "^3.15.0",
              "sharp": "^0.34.5",
              "turndown": "^7.2.2",
              "wink-eng-lite-web-model": "^1.7.1",
              "wink-nlp": "^2.3.0"
          },
          "devDependencies": {
              "@types/cors": "^2.8.19",
              "@types/express": "^5.0.6",
              "@types/js-yaml": "^4.0.9",
              "@types/node": "^25.0.7",
              "eslint": "^8.56.0",
              "pkg": "^5.8.1",
              "ts-node": "^10.9.2",
              "typescript": "^5.9.3"
          }
      }
    tokens: 846
    size: 2347
  - path: engine\python_vision\vision_engine.py
    content: "import sys\r\nimport json\r\nimport base64\r\nimport os\r\n\r\n# Placeholder for U-MARVEL or Qwen2.5-VL loading\r\n# Ideally we use llama-cpp-python for GGUF support if available, \r\n# or transformers for raw weights if we have the VRAM.\r\n\r\ndef main():\r\n    print(json.dumps({\"status\": \"ready\", \"model\": \"vision_sidecar_v1\"}), flush=True)\r\n\r\n    # Simple loop to read requests from stdin (or we can make this an HTTP server)\r\n    # For now, let's assume it runs as a script for a single inference or a persistent process.\r\n    # Persistent is better for keeping model loaded.\r\n    \r\n    while True:\r\n        try:\r\n            line = sys.stdin.readline()\r\n            if not line:\r\n                break\r\n            \r\n            data = json.loads(line)\r\n            command = data.get(\"command\")\r\n            \r\n            if command == \"analyze\":\r\n                image_b64 = data.get(\"image\") # base64 string\r\n                prompt = data.get(\"prompt\", \"Describe this image.\")\r\n                \r\n                # TODO: Decode image and run model\r\n                # img_data = base64.b64decode(image_b64)\r\n                \r\n                # Stub Response\r\n                response = {\r\n                    \"text\": f\"[VISION SIMULATION] I see an image! You asked: '{prompt}'. (Model pending integration)\"\r\n                }\r\n                print(json.dumps(response), flush=True)\r\n                \r\n            elif command == \"exit\":\r\n                break\r\n                \r\n        except Exception as e:\r\n            error_response = {\"error\": str(e)}\r\n            print(json.dumps(error_response), flush=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
    tokens: 592
    size: 1650
  - path: engine\run_context_read.js
    content: "const { createFullCorpusRecursive } = require('./src/read_all.js');\r\n\r\n// Run the function to aggregate content from the context directory\r\n// This is a wrapper to run the read_all functionality from the server directory\r\n// where all dependencies are properly installed\r\n\r\nconsole.log('Starting context aggregation from server directory...');\r\ncreateFullCorpusRecursive();"
    tokens: 138
    size: 373
  - path: engine\src\config\index.ts
    content: "/**\r\n * Configuration Module for Sovereign Context Engine\r\n * \r\n * This module manages all configuration for the context engine including\r\n * paths, model settings, and system parameters.\r\n */\r\n\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { fileURLToPath } from 'url';\r\nimport yaml from 'js-yaml';\r\n\r\n// For __dirname equivalent in ES modules\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\nimport dotenv from 'dotenv';\r\n// .env is in the ECE_Core root, 3 levels up from engine/src/config\r\ndotenv.config({ path: path.join(__dirname, '..', '..', '..', '.env') });\r\n\r\n// Define configuration interface\r\ninterface Config {\r\n  // Core\r\n  PORT: number;\r\n  HOST: string;\r\n  API_KEY: string;\r\n  LOG_LEVEL: string;\r\n  OVERLAY_PORT: number;\r\n\r\n  // Tuning\r\n  DEFAULT_SEARCH_CHAR_LIMIT: number;\r\n  DREAM_INTERVAL_MS: number;\r\n  SIMILARITY_THRESHOLD: number;\r\n  TOKEN_LIMIT: number;\r\n  DREAMER_BATCH_SIZE: number;\r\n\r\n  VECTOR_INGEST_BATCH: number;\r\n\r\n  // Extrapolated Settings\r\n  WATCHER_DEBOUNCE_MS: number;\r\n  CONTEXT_RELEVANCE_WEIGHT: number;\r\n  CONTEXT_RECENCY_WEIGHT: number;\r\n  DREAMER_CLUSTERING_GAP_MS: number;\r\n\r\n  // Infrastructure\r\n  REDIS: {\r\n    ENABLED: boolean;\r\n    URL: string;\r\n    TTL: number;\r\n  };\r\n  NEO4J: {\r\n    ENABLED: boolean;\r\n    URI: string;\r\n    USER: string;\r\n    PASS: string;\r\n  };\r\n\r\n  // Features\r\n  FEATURES: {\r\n    CONTEXT_STORAGE: boolean;\r\n    MEMORY_RECALL: boolean;\r\n    CODA: boolean;\r\n    ARCHIVIST: boolean;\r\n    WEAVER: boolean;\r\n    MARKOVIAN: boolean;\r\n  };\r\n\r\n  // Models\r\n  MODELS: {\r\n    EMBEDDING_DIM: number;\r\n    MAIN: {\r\n      PATH: string;\r\n      CTX_SIZE: number;\r\n      GPU_LAYERS: number;\r\n    };\r\n\r\n    ORCHESTRATOR: {\r\n      PATH: string;\r\n      CTX_SIZE: number;\r\n      GPU_LAYERS: number;\r\n    };\r\n    VISION: {\r\n      PATH: string;\r\n      PROJECTOR: string;\r\n      CTX_SIZE: number;\r\n      GPU_LAYERS: number;\r\n    };\r\n  };\r\n}\r\n\r\n// Default configuration\r\nconst DEFAULT_CONFIG: Config = {\r\n  // Core\r\n  PORT: parseInt(process.env['PORT'] || \"3000\"),\r\n  HOST: process.env['HOST'] || \"0.0.0.0\",\r\n  API_KEY: process.env['API_KEY'] || \"ece-secret-key\",\r\n  LOG_LEVEL: process.env['LOG_LEVEL'] || \"INFO\",\r\n  OVERLAY_PORT: parseInt(process.env['OVERLAY_PORT'] || \"3001\"),\r\n\r\n  // Tuning\r\n  DEFAULT_SEARCH_CHAR_LIMIT: 524288,\r\n  DREAM_INTERVAL_MS: 3600000, // 60 minutes\r\n  SIMILARITY_THRESHOLD: parseFloat(process.env['SIMILARITY_THRESHOLD'] || \"0.8\"),\r\n  TOKEN_LIMIT: 1000000,\r\n  DREAMER_BATCH_SIZE: parseInt(process.env['DREAMER_BATCH_SIZE'] || \"5\"),\r\n\r\n  VECTOR_INGEST_BATCH: parseInt(process.env['VECTOR_INGEST_BATCH'] || \"50\"),\r\n\r\n  // Extrapolated Settings\r\n  WATCHER_DEBOUNCE_MS: parseInt(process.env['WATCHER_DEBOUNCE_MS'] || \"2000\"),\r\n  CONTEXT_RELEVANCE_WEIGHT: parseFloat(process.env['CONTEXT_RELEVANCE_WEIGHT'] || \"0.7\"),\r\n  CONTEXT_RECENCY_WEIGHT: parseFloat(process.env['CONTEXT_RECENCY_WEIGHT'] || \"0.3\"),\r\n  DREAMER_CLUSTERING_GAP_MS: parseInt(process.env['DREAMER_CLUSTERING_GAP_MS'] || \"900000\"), // 15 mins\r\n\r\n  // Infrastructure\r\n  REDIS: {\r\n    ENABLED: process.env['REDIS_ENABLED'] === 'true',\r\n    URL: process.env['REDIS_URL'] || \"redis://localhost:6379\",\r\n    TTL: parseInt(process.env['REDIS_TTL'] || \"3600\")\r\n  },\r\n  NEO4J: {\r\n    ENABLED: process.env['NEO4J_ENABLED'] === 'true',\r\n    URI: process.env['NEO4J_URI'] || \"bolt://localhost:7687\",\r\n    USER: process.env['NEO4J_USER'] || \"neo4j\",\r\n    PASS: process.env['NEO4J_PASSWORD'] || \"password\"\r\n  },\r\n\r\n  // Features\r\n  FEATURES: {\r\n    CONTEXT_STORAGE: process.env['FEATURE_CONTEXT_STORAGE'] === 'true',\r\n    MEMORY_RECALL: process.env['FEATURE_MEMORY_RECALL'] === 'true',\r\n    CODA: process.env['FEATURE_CODA_ENABLED'] === 'true',\r\n    ARCHIVIST: process.env['FEATURE_ARCHIVIST_ENABLED'] === 'true',\r\n    WEAVER: process.env['FEATURE_WEAVER_ENABLED'] === 'true',\r\n    MARKOVIAN: process.env['MARKOVIAN_ENABLED'] === 'true'\r\n  },\r\n\r\n  // Models\r\n  MODELS: {\r\n    EMBEDDING_DIM: parseInt(process.env['LLM_EMBEDDING_DIM'] || \"768\"),\r\n    MAIN: {\r\n      PATH: process.env['LLM_MODEL_PATH'] || \"gemma-3-4b-it-abliterated-v2.i1-Q4_K_S.gguf\",\r\n      CTX_SIZE: parseInt(process.env['LLM_CTX_SIZE'] || \"4096\"),\r\n      GPU_LAYERS: (() => {\r\n        console.log(\"DEBUG: Loading Config. Env LLM_GPU_LAYERS:\", process.env['LLM_GPU_LAYERS']);\r\n        console.log(\"DEBUG: .env Path:\", path.join(__dirname, '..', '..', '..', '.env'));\r\n        return parseInt(process.env['LLM_GPU_LAYERS'] || \"33\");\r\n      })()\r\n    },\r\n\r\n    ORCHESTRATOR: {\r\n      PATH: process.env['ORCHESTRATOR_MODEL_PATH'] || \"Qwen3-4B-Function-Calling-Pro.gguf\",\r\n      CTX_SIZE: parseInt(process.env['ORCHESTRATOR_CTX_SIZE'] || \"8192\"),\r\n      GPU_LAYERS: parseInt(process.env['ORCHESTRATOR_GPU_LAYERS'] || \"0\")\r\n    },\r\n    VISION: {\r\n      PATH: process.env['VISION_MODEL_PATH'] || \"\",  // MUST BE SET IN .ENV\r\n      PROJECTOR: process.env['VISION_PROJECTOR_PATH'] || \"\", // MUST BE SET IN .ENV\r\n      CTX_SIZE: 2048,\r\n      GPU_LAYERS: parseInt(process.env['LLM_GPU_LAYERS'] || \"33\")\r\n    }\r\n  }\r\n};\r\n\r\n// Configuration loader\r\nfunction loadConfig(): Config {\r\n  // Determine config file path\r\n  // Priority: user_settings.json > sovereign.yaml > .env > Defaults\r\n\r\n  let loadedConfig = { ...DEFAULT_CONFIG };\r\n\r\n  // 1. Try Loading user_settings.json (Highest Priority for User Overrides)\r\n  const userSettingsPath = path.join(__dirname, '..', '..', 'user_settings.json');\r\n  if (fs.existsSync(userSettingsPath)) {\r\n    try {\r\n      const userSettings = JSON.parse(fs.readFileSync(userSettingsPath, 'utf8'));\r\n      console.log(`[Config] Loaded settings from ${userSettingsPath}`);\r\n\r\n      if (userSettings.llm) {\r\n        if (userSettings.llm.chat_model) loadedConfig.MODELS.MAIN.PATH = userSettings.llm.chat_model;\r\n        if (userSettings.llm.gpu_layers !== undefined) loadedConfig.MODELS.MAIN.GPU_LAYERS = userSettings.llm.gpu_layers;\r\n        if (userSettings.llm.ctx_size !== undefined) loadedConfig.MODELS.MAIN.CTX_SIZE = userSettings.llm.ctx_size;\r\n\r\n        // Also update Orchestrator if task_model is set\r\n        if (userSettings.llm.task_model) loadedConfig.MODELS.ORCHESTRATOR.PATH = userSettings.llm.task_model;\r\n      }\r\n\r\n      if (userSettings.dreamer) {\r\n        if (userSettings.dreamer.batch_size) loadedConfig.DREAMER_BATCH_SIZE = userSettings.dreamer.batch_size;\r\n      }\r\n\r\n    } catch (e) {\r\n      console.error(`[Config] Failed to parse user_settings.json:`, e);\r\n    }\r\n  }\r\n\r\n  // 2. Try Loading sovereign.yaml (Legacy/System Config)\r\n  const configPath = process.env['SOVEREIGN_CONFIG_PATH'] ||\r\n    path.join(__dirname, '..', '..', 'sovereign.yaml') ||\r\n    path.join(__dirname, '..', 'config', 'default.yaml');\r\n\r\n  if (fs.existsSync(configPath)) {\r\n    try {\r\n      const configFile = fs.readFileSync(configPath, 'utf8');\r\n      const parsedConfig = yaml.load(configFile) as Partial<Config>;\r\n      // We merge yaml ON TOP of defaults, but user_settings should ideally win.\r\n      // However, since we already loaded user_settings into loadedConfig, \r\n      // simple spread would overwrite IF sovereign.yaml has keys.\r\n      // But typically sovereign.yaml is missing.\r\n      // To be safe: Merge YAML first, then re-apply user_settings (or just trust user didn't make both).\r\n      loadedConfig = { ...loadedConfig, ...parsedConfig };\r\n\r\n      // Re-apply user_settings to be sure (Simplified logic above was slightly out of order for strict priority)\r\n      // Re-reading user settings is cheap enough or we could have done it second.\r\n      // Let's stick to the flow: Defaults -> YAML -> User Settings.\r\n      // ... (Redoing correct order below)\r\n    } catch (error) {\r\n      console.warn(`Failed to load config from ${configPath}:`, error);\r\n    }\r\n  }\r\n\r\n  // CORRECT ORDER RE-APPLICATION:\r\n  if (fs.existsSync(userSettingsPath)) {\r\n    try {\r\n      const userSettings = JSON.parse(fs.readFileSync(userSettingsPath, 'utf8'));\r\n      if (userSettings.llm) {\r\n        if (userSettings.llm.chat_model) loadedConfig.MODELS.MAIN.PATH = userSettings.llm.chat_model;\r\n        if (userSettings.llm.gpu_layers !== undefined) loadedConfig.MODELS.MAIN.GPU_LAYERS = userSettings.llm.gpu_layers;\r\n        if (userSettings.llm.ctx_size !== undefined) loadedConfig.MODELS.MAIN.CTX_SIZE = userSettings.llm.ctx_size;\r\n        if (userSettings.llm.task_model) loadedConfig.MODELS.ORCHESTRATOR.PATH = userSettings.llm.task_model;\r\n      }\r\n    } catch (e) { }\r\n  }\r\n\r\n  return loadedConfig;\r\n}\r\n\r\n// Export configuration\r\nexport const config = loadConfig();\r\n\r\nexport default config;"
    tokens: 2967
    size: 8573
  - path: engine\src\config\paths.ts
    content: "/**\r\n * Path Configuration for Sovereign Context Engine\r\n * \r\n * Defines all the important paths used by the system.\r\n */\r\n\r\nimport * as path from 'path';\r\nimport * as os from 'os';\r\n\r\n\r\nimport { fileURLToPath } from 'url';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n// Define base paths\r\nexport const PROJECT_ROOT = path.resolve(process.env['PROJECT_ROOT'] || path.join(__dirname, '..', '..'));\r\nexport const CONTEXT_DIR = path.resolve(process.env['CONTEXT_DIR'] || path.join(PROJECT_ROOT, 'context'));\r\nexport const MODELS_DIR = path.resolve(process.env['MODELS_DIR'] || path.join(PROJECT_ROOT, '..', '..', 'models'));\r\nexport const DIST_DIR = path.resolve(process.env['DIST_DIR'] || path.join(PROJECT_ROOT, 'dist'));\r\nexport const BASE_PATH = PROJECT_ROOT;\r\nexport const LOGS_DIR = path.join(PROJECT_ROOT, 'logs');\r\nexport const NOTEBOOK_DIR = path.resolve(process.env['NOTEBOOK_DIR'] || path.join(PROJECT_ROOT, '..', '..', 'notebook'));\r\n\r\n// Define specific paths\r\nconst PATHS = {\r\n  PROJECT_ROOT,\r\n  CONTEXT_DIR,\r\n  MODELS_DIR,\r\n  DIST_DIR,\r\n  BACKUPS_DIR: path.join(PROJECT_ROOT, 'backups'),\r\n  LOGS_DIR: path.join(PROJECT_ROOT, 'logs'),\r\n  CONFIG_FILE: path.join(PROJECT_ROOT, 'sovereign.yaml'),\r\n  USER_SETTINGS: path.join(PROJECT_ROOT, 'user_settings.json'),\r\n  DATABASE_FILE: path.join(CONTEXT_DIR, 'context.db'),\r\n  INBOX_DIR: path.join(CONTEXT_DIR, 'inbox'),\r\n  LIBRARIES_DIR: path.join(CONTEXT_DIR, 'libraries'),\r\n  MIRRORS_DIR: path.join(CONTEXT_DIR, 'mirrors'),\r\n  SESSIONS_DIR: path.join(CONTEXT_DIR, 'sessions'),\r\n  TEMP_DIR: path.join(os.tmpdir(), 'sovereign-context-engine'),\r\n  ENGINE_BIN: path.join(PROJECT_ROOT, 'engine', 'bin'),\r\n  ENGINE_SRC: path.join(PROJECT_ROOT, 'engine', 'src'),\r\n  ENGINE_DIST: path.join(PROJECT_ROOT, 'engine', 'dist'),\r\n  DESKTOP_OVERLAY_SRC: path.join(PROJECT_ROOT, 'desktop-overlay', 'src'),\r\n  DESKTOP_OVERLAY_DIST: path.join(PROJECT_ROOT, 'desktop-overlay', 'dist'),\r\n};\r\n\r\nexport default PATHS;"
    tokens: 715
    size: 2008
  - path: engine\src\core\batch.ts
    content: "/**\r\n * Batch Processing Utility\r\n * \r\n * Provides a standardized way to process large arrays of items in chunks.\r\n * Useful for LLM operations, Database writes, and heavy processing loops.\r\n */\r\n\r\nexport interface BatchOptions {\r\n    batchSize: number;\r\n    delayMs?: number; // Optional delay between batches to let system breathe\r\n}\r\n\r\n/**\r\n * Process an array of items in batches.\r\n * \r\n * @param items Array of items to process\r\n * @param processor Async function to process a single batch\r\n * @param options Configuration options\r\n */\r\nexport async function processInBatches<T, R>(\r\n    items: T[],\r\n    processor: (batch: T[], batchIndex: number, startItemIndex: number) => Promise<R>,\r\n    options: BatchOptions\r\n): Promise<R[]> {\r\n    const { batchSize, delayMs } = options;\r\n    const results: R[] = [];\r\n    const totalBatches = Math.ceil(items.length / batchSize);\r\n\r\n    for (let i = 0; i < items.length; i += batchSize) {\r\n        const batch = items.slice(i, i + batchSize);\r\n        const batchIndex = Math.floor(i / batchSize);\r\n\r\n        try {\r\n            const result = await processor(batch, batchIndex, i);\r\n            results.push(result);\r\n        } catch (error) {\r\n            console.error(`[Batch] Error in batch ${batchIndex + 1}/${totalBatches}:`, error);\r\n            // We continue processing other batches? \r\n            // Depends on specific service needs, but generally safer to throw or let caller handle try/catch inside processor.\r\n            throw error;\r\n        }\r\n\r\n        if (delayMs && i + batchSize < items.length) {\r\n            await new Promise(resolve => setTimeout(resolve, delayMs));\r\n        }\r\n    }\r\n\r\n    return results;\r\n}\r\n"
    tokens: 608
    size: 1684
  - path: engine\src\core\db.ts
    content: "/**\r\n * Database Module for Sovereign Context Engine\r\n *\r\n * This module manages the CozoDB database connection and provides\r\n * database operations for the context engine.\r\n */\r\n\r\nimport { createRequire } from 'module';\r\nconst require = createRequire(import.meta.url);\r\nconst cozoNode = require('cozo-node');\r\nimport { config } from '../config/index.js';\r\n\r\nexport class Database {\r\n  private dbId: string | null = null;\r\n\r\n  constructor() {\r\n    // Database connection is now established in init()\r\n  }\r\n\r\n  /**\r\n   * Initialize the database with required schemas\r\n   */\r\n  async init() {\r\n    // 0. Initialize the database connection (moved from constructor to prevent import-time crashes)\r\n    if (this.dbId === null) {\r\n      try {\r\n        console.log('[DB] Attempting to open RocksDB backend: ./context.db');\r\n        this.dbId = cozoNode.open_db('rocksdb', './context.db', {});\r\n        console.log('[DB] Initialized with RocksDB backend: ./context.db');\r\n      } catch (e: any) {\r\n        if (e.message?.includes('lock file') || e.message?.includes('IO error')) {\r\n          console.error('\\n\\n[DB] CRITICAL ERROR: Database is LOCKED.');\r\n          console.error('[DB] This usually means another ECE process is running.');\r\n          console.error('[DB] Please stop all \"node\" processes and try again.\\n');\r\n          // We can optionally attempt to force-clear locks here, but it's risky if process is alive.\r\n          // For now, fail gracefully.\r\n          throw new Error('Database Locked: ' + e.message);\r\n        }\r\n        throw e;\r\n      }\r\n    }\r\n\r\n    // Create the memory table schema\r\n    // We check for existing columns to determine if migration is needed\r\n    try {\r\n      const result = await this.run('::columns memory');\r\n      const columns = result.rows.map((r: any) => r[0]);\r\n\r\n      // Check for Level 1 Atomizer fields\r\n      const hasSequence = columns.includes('sequence');\r\n      const hasEmbedding = columns.includes('embedding');\r\n      const hasSourceId = columns.includes('source_id');\r\n\r\n      if (!hasSequence || !hasEmbedding || !hasSourceId) {\r\n        console.log('Migrating memory schema: Adding Atomizer columns...');\r\n\r\n        // 1. Fetch old data into memory (Safe subset of columns)\r\n        // We only fallback to what we know existed in v2\r\n        const oldDataResult = await this.run(`\r\n          ?[id, timestamp, content, source, provenance] :=\r\n          *memory{id, timestamp, content, source, provenance}\r\n        `);\r\n\r\n        console.log(`[DB] Migrating ${oldDataResult.rows.length} rows...`);\r\n\r\n        // 2. Drop old indices and table\r\n        try {\r\n          console.log('[DB] Removing indices...');\r\n          try { await this.run('::remove memory:knn'); } catch (e) { }\r\n          try { await this.run('::remove memory:vec_idx'); } catch (e) { } // Legacy\r\n          try { await this.run('::remove memory:content_fts'); } catch (e) { }\r\n        } catch (e: any) {\r\n          console.log(`[DB] Index removal warning: ${e.message}`);\r\n        }\r\n\r\n        console.log('[DB] Removing old table...');\r\n        await this.run('::remove memory');\r\n\r\n        // 3. Create new table\r\n        await this.run(`\r\n          :create memory {\r\n            id: String\r\n            =>\r\n            timestamp: Float,\r\n            content: String,\r\n            source: String,\r\n            source_id: String,\r\n            sequence: Int,\r\n            type: String,\r\n            hash: String,\r\n            buckets: [String],\r\n            epochs: [String],\r\n            tags: [String],\r\n            provenance: String,\r\n            embedding: <F32; ${config.MODELS.EMBEDDING_DIM}>\r\n          }\r\n        `);\r\n\r\n        // 4. Re-insert data with defaults\r\n        if (oldDataResult.rows.length > 0) {\r\n          const crypto = await import('crypto'); // Dynamic import for hash generation\r\n\r\n          const newData = oldDataResult.rows.map((row: any) => {\r\n            // row: [id, timestamp, content, source, provenance]\r\n            const content = row[2] || \"\";\r\n            const hash = crypto.createHash('md5').update(content).digest('hex');\r\n\r\n            return [\r\n              row[0], // id\r\n              row[1] || Date.now(), // timestamp\r\n              content, // content\r\n              row[3] || \"unknown\", // source\r\n              row[3] || \"unknown\", // source_id (default to source path)\r\n              0,      // sequence\r\n              'fragment', // type (default)\r\n              hash, // hash (calculated)\r\n              [], // buckets\r\n              [], // tags\r\n              [], // epochs\r\n              row[4] || \"{}\", // provenance\r\n              new Array(config.MODELS.EMBEDDING_DIM).fill(0.0) // embedding (reset to zero to force re-embed)\r\n            ];\r\n          });\r\n\r\n          // Batch insert\r\n          const chunkSize = 100;\r\n          for (let i = 0; i < newData.length; i += chunkSize) {\r\n            const chunk = newData.slice(i, i + chunkSize);\r\n            await this.run(`\r\n               ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data\r\n               :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}\r\n             `, { data: chunk });\r\n          }\r\n        }\r\n        console.log('[DB] Migration complete.');\r\n      }\r\n    } catch (e: any) {\r\n      // Create fresh if not exists\r\n      if (e.message && (e.message.includes('RelNotFound') || e.message.includes('not found') || e.message.includes('Cannot find'))) {\r\n        console.log('[DB] Creating memory table from scratch...');\r\n        // Create Memory Table\r\n        try {\r\n          await this.run(`\r\n            :create memory {\r\n                id: String\r\n                =>\r\n                timestamp: Float,\r\n                content: String,\r\n                source: String,\r\n                source_id: String,\r\n                sequence: Int,\r\n                type: String,\r\n                hash: String,\r\n                buckets: [String],\r\n                epochs: [String],\r\n                tags: [String],\r\n                provenance: String,\r\n                embedding: <F32; ${config.MODELS.EMBEDDING_DIM}>\r\n            }\r\n        `);\r\n          console.log('Memory table initialized');\r\n\r\n          // REMOVED: Vector index is no longer used. Tag-Walker is the primary retrieval method.\r\n          // Explicitly remove it if it exists to save resources and prevent zero-vector errors.\r\n          try {\r\n            await this.run('::remove memory:knn');\r\n            console.log('[DB] Legacy vector index (memory:knn) removed.');\r\n          } catch (e) {\r\n            // Ignore if index doesn't exist\r\n          }\r\n\r\n        } catch (createError: any) {\r\n          console.error(`[DB] Failed to create memory table: ${createError.message}`);\r\n\r\n          // Check if table already exists (not an error technically, but we might want schema check)\r\n          if (!createError.message?.includes('Duplicate') && !createError.display?.includes('Duplicate')) {\r\n            throw createError;\r\n          }\r\n        }\r\n      } else {\r\n        console.log(`[DB] Schema check/migration failed: ${e.message}`);\r\n        if (e.message.includes('indices attached') || e.message.includes('Index lock')) {\r\n          console.log('[DB] Index lock detected. Automatically purging corrupted database...');\r\n\r\n          // Close existing connection\r\n          try { this.close(); } catch (c) { }\r\n\r\n          // Give OS time to release file locks (Windows is slow)\r\n          await new Promise(resolve => setTimeout(resolve, 1000));\r\n\r\n          const fs = await import('fs');\r\n          try {\r\n            // RocksDB creates a DIRECTORY, not a file. unlinkSync fails on dirs.\r\n            if (fs.existsSync('./context.db')) fs.rmSync('./context.db', { recursive: true, force: true });\r\n            if (fs.existsSync('./context.db-log')) fs.rmSync('./context.db-log', { force: true });\r\n            if (fs.existsSync('./context.db-lock')) fs.rmSync('./context.db-lock', { force: true });\r\n          } catch (err: any) {\r\n            console.error('[DB] Failed to auto-purge:', err.message);\r\n            console.error('[DB] Please MANUALLY delete the \"context.db\" folder and restart.');\r\n            process.exit(1); // Do not recurse if FS fails, just exit.\r\n          }\r\n\r\n          // Re-initialize fresh\r\n          console.log('[DB] Re-initializing fresh database...');\r\n          this.dbId = cozoNode.open_db('rocksdb', './context.db', {});\r\n          await this.init(); // Recursive retry\r\n          return;\r\n        }\r\n        throw e;\r\n      }\r\n    }\r\n\r\n    // Create Source Table (Container)\r\n    try {\r\n      await this.run(`\r\n        :create source {\r\n           path: String,\r\n           hash: String,\r\n           total_atoms: Int,\r\n           last_ingest: Float\r\n        }\r\n      `);\r\n    } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }\r\n\r\n    // Create Summary Node Table (Level 2/3: Episodes/Epochs)\r\n    try {\r\n      await this.run(`\r\n        :create summary_node {\r\n           id: String,\r\n           type: String,\r\n           content: String,\r\n           span_start: Float,\r\n           span_end: Float,\r\n           embedding: <F32; 384>\r\n        }\r\n      `);\r\n    } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }\r\n\r\n    // Create Parent_Of Edge Table (Hierarchy)\r\n    try {\r\n      await this.run(`\r\n        :create parent_of {\r\n           parent_id: String,\r\n           child_id: String,\r\n           weight: Float\r\n        }\r\n      `);\r\n    } catch (e: any) { if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e; }\r\n\r\n    // Create Engram table (Lexical Sidecar)\r\n    try {\r\n      await this.run(`\r\n        :create engrams {\r\n          key: String,\r\n          value: String\r\n        }\r\n      `);\r\n    } catch (e: any) {\r\n      if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate')) throw e;\r\n    }\r\n\r\n    // Create FTS index for content\r\n    try {\r\n      await this.run(`\r\n        ::fts create memory:content_fts {\r\n          extractor: content,\r\n          tokenizer: Simple,\r\n          filters: [Lowercase]\r\n        }\r\n      `);\r\n    } catch (e: any) {\r\n      if (!e.message?.includes('conflict') && !e.message?.includes('Duplicate') && !e.message?.includes('already exists')) throw e;\r\n    }\r\n\r\n    console.log('Database initialized successfully');\r\n  }\r\n\r\n  /**\r\n   * Close the database connection\r\n   */\r\n  async close() {\r\n    // Close the database connection\r\n    if (this.dbId) {\r\n      cozoNode.close_db(this.dbId);\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Run a query against the database\r\n   */\r\n  async run(query: string, params?: any) {\r\n    const { config } = await import('../config/index.js');\r\n    if (config.LOG_LEVEL === 'DEBUG') {\r\n      if (query.includes(':put') || query.includes(':insert')) {\r\n        console.log(`[DB] Executing Write: ${query.substring(0, 50)}... Params keys: ${params ? Object.keys(params) : 'none'}`);\r\n        if (params && params.data) console.log(`[DB] Data rows: ${params.data.length}`);\r\n      }\r\n    }\r\n\r\n    try {\r\n      if (this.dbId === null) {\r\n        throw new Error('Database not initialized');\r\n      }\r\n      const result = cozoNode.query_db(this.dbId, query, params || {});\r\n      return result;\r\n    } catch (e: any) {\r\n      console.error(`[DB] Query Failed: ${e.message}`);\r\n      console.error(`[DB] Query: ${query}`);\r\n      throw e;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Run a FTS search query\r\n   */\r\n  async search(query: string) {\r\n    if (this.dbId === null) {\r\n      throw new Error('Database not initialized');\r\n    }\r\n    return cozoNode.query_db(this.dbId, query, {});\r\n  }\r\n}\r\n\r\n// Export a singleton instance\r\nexport const db = new Database();"
    tokens: 4188
    size: 11871
  - path: engine\src\core\inference\ChatWorker.ts
    content: "\r\nimport { parentPort, workerData } from 'worker_threads';\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel } from 'node-llama-cpp';\r\nimport os from 'os';\r\n\r\n// Worker state\r\nlet llama: any = null;\r\nlet model: LlamaModel | null = null;\r\nlet context: LlamaContext | null = null;\r\nlet session: LlamaChatSession | null = null;\r\nlet currentSequence: any = null;\r\n\r\nasync function init() {\r\n    if (llama) return;\r\n    try {\r\n        // Use workerData to force CPU, or fallback to global env\r\n        const forceCpu = workerData?.forceCpu || process.env['LLM_GPU_LAYERS'] === '0';\r\n\r\n        if (forceCpu) {\r\n            console.log(\"[Worker] Force CPU mode detected. Disabling GPU backends.\");\r\n            llama = await getLlama({\r\n                gpu: { type: 'auto', exclude: ['cuda', 'vulkan', 'metal'] }\r\n            });\r\n        } else {\r\n            console.log(\"[Worker] Initializing Llama with hardware acceleration support.\");\r\n            llama = await getLlama();\r\n        }\r\n        parentPort?.postMessage({ type: 'ready' });\r\n    } catch (error: any) {\r\n        console.error(\"[Worker] Initialization Error:\", error);\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n}\r\n\r\nparentPort?.on('message', async (message) => {\r\n    try {\r\n        switch (message.type) {\r\n            case 'loadModel':\r\n                await handleLoadModel(message.data);\r\n                break;\r\n            case 'chat':\r\n                await handleChat(message.data);\r\n                break;\r\n            case 'dispose':\r\n                await handleDispose();\r\n                break;\r\n        }\r\n    } catch (error: any) {\r\n        console.error(\"[Worker] Message Handling Error:\", error);\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n});\r\n\r\nasync function handleLoadModel(data: { modelPath: string, options: any }) {\r\n    if (!llama) await init();\r\n\r\n    if (session) { session.dispose(); session = null; }\r\n    if (currentSequence) { currentSequence.dispose(); currentSequence = null; }\r\n    if (context) { await context.dispose(); context = null; }\r\n    if (model) { await model.dispose(); model = null; }\r\n\r\n    try {\r\n        console.log(`[Worker] Loading model: ${data.modelPath} (gpuLayers: ${data.options.gpuLayers || 0})`);\r\n        model = await llama.loadModel({\r\n            modelPath: data.modelPath,\r\n            gpuLayers: data.options.gpuLayers || 0\r\n        });\r\n\r\n        const ctxSize = data.options.ctxSize || 4096;\r\n        const threads = Math.max(1, Math.floor(os.cpus().length / 2));\r\n        console.log(`[Worker] Creating context: ${ctxSize} tokens, ${threads} threads`);\r\n\r\n        context = await model!.createContext({\r\n            contextSize: ctxSize,\r\n            batchSize: 128, // Smaller batch for smoother CPU pre-fill\r\n            sequences: 4,   // Bump to 4 to handle high concurrency (e.g. Discovery + Infection + Search)\r\n            threads\r\n        });\r\n\r\n        // Pre-allocate minimal sequences to avoid runtime allocation lag\r\n        currentSequence = context.getSequence();\r\n        session = new LlamaChatSession({\r\n            contextSequence: currentSequence,\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n\r\n        parentPort?.postMessage({ type: 'modelLoaded', data: { modelPath: data.modelPath } });\r\n    } catch (error: any) {\r\n        throw new Error(`Failed to load Chat Model: ${error.message}`);\r\n    }\r\n}\r\n\r\nasync function handleChat(data: { prompt: string, options: any }) {\r\n    if (!context) throw new Error(\"Context not initialized\");\r\n\r\n    if (data.options.systemPrompt || !session) {\r\n        if (session) session.dispose();\r\n        if (currentSequence) currentSequence.dispose();\r\n\r\n        currentSequence = context.getSequence();\r\n        session = new LlamaChatSession({\r\n            contextSequence: currentSequence,\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n    }\r\n\r\n    console.log(`[Worker] Chat Request Received. Pre-filling prompt (${data.prompt.length} chars)...`);\r\n    let tokensReceived = 0;\r\n\r\n    try {\r\n        const response = await session.prompt(data.prompt, {\r\n            temperature: 0.7, // Higher temp for reasoning\r\n            maxTokens: data.options.maxTokens || 2048,\r\n            onTextChunk: (chunk: string) => {\r\n                if (tokensReceived === 0) {\r\n                    console.log(`[Worker] First token generated! Pre-fill took ${(Date.now() - startTime) / 1000}s`);\r\n                }\r\n                tokensReceived += chunk.length; // Approximate, as chunk is text\r\n                parentPort?.postMessage({ type: 'token', token: chunk });\r\n            }\r\n        });\r\n\r\n        console.log(`[Worker] Chat Completed. Response: ${response.length} chars.`);\r\n        parentPort?.postMessage({ type: 'chatResponse', data: response });\r\n    } catch (error: any) {\r\n        console.error(`[Worker] Inference Error:`, error);\r\n        throw error;\r\n    }\r\n}\r\n\r\nconst startTime = Date.now(); // Used for pre-fill timing\r\n\r\nasync function handleDispose() {\r\n    if (session) { session.dispose(); session = null; }\r\n    if (currentSequence) { currentSequence.dispose(); currentSequence = null; }\r\n    if (context) { await context.dispose(); context = null; }\r\n    if (model) await model.dispose();\r\n    parentPort?.postMessage({ type: 'disposed' });\r\n}\r\n\r\ninit();\r\n"
    tokens: 1882
    size: 5460
  - path: engine\src\core\inference\context_manager.ts
    content: "import { config } from '../../config/index.js';\r\n\r\nexport interface ContextAtom {\r\n    id: string;\r\n    content: string;\r\n    source: string;\r\n    timestamp: number;\r\n    score: number; // Relevance Score\r\n}\r\n\r\nexport interface ContextResult {\r\n    prompt: string;\r\n    stats: {\r\n        tokenCount: number;\r\n        charCount: number;\r\n        filledPercent: number;\r\n        atomCount: number;\r\n    };\r\n}\r\n\r\n/**\r\n * Rolling Context Slicer (Feature 8)\r\n * \r\n * Implements \"Middle-Out\" Context Budgeting.\r\n * Prioritizes atoms based on a mix of Relevance (Vector Similarity) and Recency.\r\n * \r\n * Strategy:\r\n * 1. Rank Candidates: Score = (Relevance * 0.7) + (RecencyNorm * 0.3).\r\n * 2. Select: Fill budget with highest ranked atoms.\r\n * 3. Smart Slice: If an atom fits partially, slice around the keyword match (windowing).\r\n * 4. Order: Sort selected atoms Chronologically (or by Sequence) for linear readability.\r\n */\r\nexport function composeRollingContext(\r\n    query: string,\r\n    results: ContextAtom[],\r\n    tokenBudget: number = 4096\r\n): ContextResult {\r\n    // Constants\r\n    const CHARS_PER_TOKEN = 4; // Rough estimate\r\n\r\n    // Safety Buffer: Target 95% of budget to account for multibyte chars / math errors\r\n    const SAFE_BUDGET = Math.floor(tokenBudget * 0.95);\r\n    const charBudget = SAFE_BUDGET * CHARS_PER_TOKEN;\r\n\r\n    // 1. Dynamic Recency Analysis\r\n    // Check for temporal signals in query\r\n    const temporalSignals = [\"recent\", \"latest\", \"new\", \"today\", \"now\", \"current\", \"last\"];\r\n    const hasTemporalSignal = temporalSignals.some(signal => query.toLowerCase().includes(signal));\r\n\r\n    // Adjust weights based on intent\r\n    // Default: Relevance 70%, Recency 30%\r\n    // Temporal: Relevance 40%, Recency 60%\r\n    const RELEVANCE_WEIGHT = hasTemporalSignal ? 0.4 : config.CONTEXT_RELEVANCE_WEIGHT;\r\n    const RECENCY_WEIGHT = hasTemporalSignal ? 0.6 : config.CONTEXT_RECENCY_WEIGHT;\r\n\r\n    // 2. Normalize Recency & Score\r\n    const now = Date.now();\r\n    const oneMonth = 30 * 24 * 60 * 60 * 1000;\r\n\r\n    const candidates = results.map(atom => {\r\n        const age = Math.max(0, now - atom.timestamp);\r\n        // Recency Score: 1.0 = Brand new, 0.0 = >1 Month old (clamped)\r\n        const recencyScore = Math.max(0, 1.0 - (age / oneMonth));\r\n\r\n        // Final Mixed Score\r\n        const mixedScore = (atom.score * RELEVANCE_WEIGHT) + (recencyScore * RECENCY_WEIGHT);\r\n\r\n        return { ...atom, mixedScore, recencyScore };\r\n    });\r\n\r\n    // 3. Sort by Mixed Score (Descending)\r\n    candidates.sort((a, b) => b.mixedScore - a.mixedScore);\r\n\r\n    // 4. Selection (Fill Budget)\r\n    const selectedAtoms: typeof candidates = [];\r\n    let currentChars = 0;\r\n\r\n    for (const atom of candidates) {\r\n        if (currentChars >= charBudget) break;\r\n\r\n        const atomLen = atom.content.length;\r\n\r\n        if (currentChars + atomLen <= charBudget) {\r\n            selectedAtoms.push(atom);\r\n            currentChars += atomLen;\r\n        } else {\r\n            // Partial Fill with Smart Slicing\r\n            const remaining = charBudget - currentChars;\r\n            if (remaining > 200) {\r\n                // Slice to nearest punctuation to keep thought intact\r\n                // Look for . ! ? or \\n within the last 50 chars of the budget\r\n\r\n                // Finds last punctuation before the hard limit\r\n                const safeContent = atom.content.substring(0, remaining);\r\n\r\n                // Polyfill for finding last punctuation\r\n                const lastDot = safeContent.lastIndexOf('.');\r\n                const lastBang = safeContent.lastIndexOf('!');\r\n                const lastQ = safeContent.lastIndexOf('?');\r\n                const lastNew = safeContent.lastIndexOf('\\n');\r\n\r\n                const bestCut = Math.max(lastDot, lastBang, lastQ, lastNew);\r\n\r\n                if (bestCut > (remaining * 0.5)) {\r\n                    // If punctuation is reasonably far in, use it\r\n                    const slicedContent = atom.content.substring(0, bestCut + 1) + \" [Truncated]\";\r\n                    selectedAtoms.push({ ...atom, content: slicedContent });\r\n                    currentChars += slicedContent.length;\r\n                } else {\r\n                    // Fallback to hard cut if no punctuation found nearby\r\n                    const slicedContent = atom.content.substring(0, remaining) + \"...\";\r\n                    selectedAtoms.push({ ...atom, content: slicedContent });\r\n                    currentChars += slicedContent.length;\r\n                }\r\n            }\r\n            break; // Filled\r\n        }\r\n    }\r\n\r\n    // 5. Final Sort (Chronological / Flow)\r\n    // Preservation of narrative flow is key.\r\n    selectedAtoms.sort((a, b) => a.timestamp - b.timestamp);\r\n\r\n    // 6. Assemble\r\n    const contextString = selectedAtoms\r\n        .map(a => `[Source: ${a.source}] (${new Date(a.timestamp).toISOString()})\\n${a.content}`)\r\n        .join('\\n\\n');\r\n\r\n    return {\r\n        prompt: contextString,\r\n        stats: {\r\n            tokenCount: Math.ceil(currentChars / CHARS_PER_TOKEN),\r\n            charCount: currentChars,\r\n            filledPercent: Math.min(100, (currentChars / charBudget) * 100),\r\n            atomCount: selectedAtoms.length\r\n        }\r\n    };\r\n}\r\n"
    tokens: 1814
    size: 5230
  - path: engine\src\core\inference\llamaLoaderWorker.ts
    content: "\r\nimport { parentPort } from 'worker_threads';\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel, LlamaEmbeddingContext } from 'node-llama-cpp';\r\n\r\n// Worker state\r\nlet llama: any = null;\r\nlet model: LlamaModel | null = null;\r\nlet context: LlamaContext | null = null;\r\nlet session: LlamaChatSession | null = null;\r\nlet embeddingContext: LlamaEmbeddingContext | null = null;\r\nlet currentSequence: any = null;\r\n\r\nasync function init() {\r\n    if (llama) return;\r\n    try {\r\n        const systemForceCpu = process.env['LLM_GPU_LAYERS'] === '0';\r\n\r\n        if (systemForceCpu) {\r\n            console.log(\"[Worker] Global CPU-only mode detected. Disabling GPU backends.\");\r\n            llama = await getLlama({\r\n                gpu: { type: 'auto', exclude: ['cuda', 'vulkan', 'metal'] }\r\n            });\r\n        } else {\r\n            console.log(\"[Worker] Initializing Llama with hardware acceleration support.\");\r\n            llama = await getLlama();\r\n        }\r\n        parentPort?.postMessage({ type: 'ready' });\r\n    } catch (error: any) {\r\n        console.error(\"[Worker] Initialization Error:\", error);\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n}\r\n\r\n// Handle messages from main thread\r\nparentPort?.on('message', async (message) => {\r\n    try {\r\n        switch (message.type) {\r\n            case 'loadModel':\r\n                await handleLoadModel(message.data);\r\n                break;\r\n            case 'chat':\r\n                await handleChat(message.data);\r\n                break;\r\n            case 'getEmbedding':\r\n                await handleGetEmbedding(message.data);\r\n                break;\r\n            case 'getEmbeddings':\r\n                await handleGetEmbeddings(message.data);\r\n                break;\r\n            case 'dispose':\r\n                await handleDispose();\r\n                break;\r\n        }\r\n    } catch (error: any) {\r\n        console.error(\"[Worker] Message Handling Error:\", error);\r\n        parentPort?.postMessage({ type: 'error', error: error.message });\r\n    }\r\n});\r\n\r\nasync function handleLoadModel(data: { modelPath: string, options: any }) {\r\n    if (!llama) await init();\r\n\r\n    // Cleanup existing\r\n    if (session) { session.dispose(); session = null; }\r\n    if (currentSequence) { currentSequence.dispose(); currentSequence = null; }\r\n    if (context) { await context.dispose(); context = null; }\r\n    if (embeddingContext) { await embeddingContext.dispose(); embeddingContext = null; }\r\n    if (model) { await model.dispose(); model = null; }\r\n\r\n    try {\r\n        console.log(`[Worker] Loading model: ${data.modelPath} (gpuLayers: ${data.options.gpuLayers || 0})`);\r\n        model = await llama.loadModel({\r\n            modelPath: data.modelPath,\r\n            gpuLayers: data.options.gpuLayers || 0\r\n        });\r\n\r\n        const ctxSize = data.options.ctxSize || 4096;\r\n        console.log(`[Worker] Creating context: ${ctxSize} tokens`);\r\n        context = await model!.createContext({\r\n            contextSize: ctxSize,\r\n            batchSize: Math.min(ctxSize, 512),\r\n            sequences: 4\r\n        });\r\n\r\n        // Initialize dedicated embedding context\r\n        embeddingContext = await model!.createEmbeddingContext({\r\n            contextSize: 2048,\r\n            batchSize: 512\r\n        });\r\n\r\n        currentSequence = context.getSequence();\r\n        session = new LlamaChatSession({\r\n            contextSequence: currentSequence,\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n\r\n        parentPort?.postMessage({ type: 'modelLoaded', data: { modelPath: data.modelPath } });\r\n    } catch (error: any) {\r\n        throw new Error(`Failed to load model: ${error.message}`);\r\n    }\r\n}\r\n\r\nasync function handleChat(data: { prompt: string, options: any }) {\r\n    if (!context) throw new Error(\"Context not initialized\");\r\n\r\n    if (data.options.systemPrompt || !session) {\r\n        if (session) session.dispose();\r\n        if (currentSequence) currentSequence.dispose();\r\n\r\n        currentSequence = context.getSequence();\r\n        session = new LlamaChatSession({\r\n            contextSequence: currentSequence,\r\n            systemPrompt: data.options.systemPrompt || \"You are a helpful assistant.\"\r\n        });\r\n    }\r\n\r\n    console.log(`[Worker] Chat Request: ${data.prompt.length} chars. Generating response...`);\r\n    let tokensReceived = 0;\r\n\r\n    const response = await session.prompt(data.prompt, {\r\n        temperature: data.options.temperature || 0.7,\r\n        maxTokens: data.options.maxTokens || 1024,\r\n        onToken: () => {\r\n            tokensReceived++;\r\n            if (tokensReceived % 20 === 0) {\r\n                console.log(`[Worker] Activity Heartbeat: Generated ${tokensReceived} tokens...`);\r\n            }\r\n        }\r\n    });\r\n\r\n    console.log(`[Worker] Chat Completed. Response: ${response.length} chars.`);\r\n    parentPort?.postMessage({ type: 'chatResponse', data: response });\r\n}\r\n\r\nasync function handleGetEmbedding(data: { text: string }) {\r\n    if (!embeddingContext) throw new Error(\"Embedding Context not initialized\");\r\n    try {\r\n        const embedding = await embeddingContext.getEmbeddingFor(data.text);\r\n        parentPort?.postMessage({ type: 'embeddingResponse', data: Array.from(embedding.vector) });\r\n    } catch (e: any) {\r\n        throw new Error(`Embedding Generation Failed: ${e.message}`);\r\n    }\r\n}\r\n\r\nasync function handleGetEmbeddings(data: { texts: string[] }) {\r\n    if (!embeddingContext) throw new Error(\"Embedding Context not initialized\");\r\n    try {\r\n        const embeddings: number[][] = [];\r\n        for (const text of data.texts) {\r\n            if (typeof text !== 'string') {\r\n                embeddings.push([]);\r\n                continue;\r\n            }\r\n            const embedding = await embeddingContext.getEmbeddingFor(text);\r\n            embeddings.push(Array.from(embedding.vector));\r\n        }\r\n        parentPort?.postMessage({ type: 'embeddingsGenerated', data: embeddings });\r\n    } catch (e: any) {\r\n        throw new Error(`Batch Embedding Generation Failed: ${e.message}`);\r\n    }\r\n}\r\n\r\nasync function handleDispose() {\r\n    if (session) { session.dispose(); session = null; }\r\n    if (currentSequence) { currentSequence.dispose(); currentSequence = null; }\r\n    if (context) { await context.dispose(); context = null; }\r\n    if (embeddingContext) { await embeddingContext.dispose(); embeddingContext = null; }\r\n    if (model) await model.dispose();\r\n    parentPort?.postMessage({ type: 'disposed' });\r\n}\r\n\r\ninit();\r\n"
    tokens: 2216
    size: 6561
  - path: engine\src\hydrate.js
    content: "const fs = require('fs');\r\nconst path = require('path');\r\nconst yaml = require('js-yaml');\r\nconst crypto = require('crypto');\r\nconst { CozoDb } = require('cozo-node');\r\n\r\n// Helper to calculate hash\r\nfunction getHash(content) {\r\n    return crypto.createHash('md5').update(content || '').digest('hex');\r\n}\r\n\r\nasync function hydrate(db, filePath) {\r\n    console.log(` Hydrating Schema 2.0 from: ${filePath}`);\r\n    \r\n    try {\r\n        // 1. Force Re-Create Schema with new columns\r\n        // We drop the old table to ensure clean migration if needed, but :create if not exists is safer\r\n        // To force an upgrade, we rely on the user deleting context.db manually or we just run the create command\r\n        // Since we are changing columns, we must ensure the schema matches.\r\n        \r\n        const schema = ':create memory {id: String => timestamp: Int, content: String, source: String, type: String, hash: String, bucket: String}';\r\n        try {\r\n            await db.run(schema);\r\n        } catch (e) {\r\n            if (!e.message.includes('already exists') && !e.message.includes('conflicts with an existing one')) throw e;\r\n        }\r\n        \r\n        // FTS Update\r\n        try {\r\n            await db.run(`::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]}`);\r\n        } catch (e) {\r\n            if (!e.message.includes('already exists')) console.error('FTS Error:', e.message);\r\n        }\r\n\r\n        // 2. Load Data\r\n        const fileContent = fs.readFileSync(filePath, 'utf8');\r\n        const records = yaml.load(fileContent);\r\n        \r\n        if (!Array.isArray(records)) throw new Error(\"Invalid snapshot format\");\r\n\r\n        console.log(`Found ${records.length} memories. Upgrading...`);\r\n\r\n        const BATCH_SIZE = 100;\r\n        let processed = 0;\r\n\r\n        while (processed < records.length) {\r\n            const batch = records.slice(processed, processed + BATCH_SIZE);\r\n            \r\n            // Map legacy records to new format\r\n            const values = batch.map(r => [\r\n                r.id, \r\n                parseInt(r.timestamp), \r\n                r.content, \r\n                r.source, \r\n                r.type,\r\n                r.hash || getHash(r.content),      // Backfill hash\r\n                r.bucket || 'core'                 // Backfill bucket\r\n            ]);\r\n            \r\n            const q = `\r\n                ?[id, timestamp, content, source, type, hash, bucket] <- $values\r\n                :put memory {id, timestamp, content, source, type, hash, bucket}\r\n            `;\r\n            \r\n            await db.run(q, { values });\r\n            processed += batch.length;\r\n            process.stdout.write(`\\rProgress: ${processed}/${records.length}`);\r\n        }\r\n        console.log(\"\\n Hydration & Upgrade Complete.\");\r\n\r\n    } catch (e) {\r\n        console.error(\"\\n Hydration Failed:\", e.message);\r\n    }\r\n}\r\n\r\nmodule.exports = { hydrate };\r\n\r\nif (require.main === module) {\r\n    const targetFile = process.argv[2];\r\n    const dbPath = path.join(__dirname, '..', 'context.db');\r\n    const db = new CozoDb('rocksdb', dbPath);\r\n    if (!targetFile) { console.log(\"Usage: node src/hydrate.js <snapshot.yaml>\"); process.exit(1); }\r\n    hydrate(db, targetFile);\r\n}\r\n"
    tokens: 1169
    size: 3272
  - path: engine\src\index.js
    content: "const express = require('express');\r\nconst cors = require('cors');\r\nconst bodyParser = require('body-parser');\r\nconst { CozoDb } = require('cozo-node');\r\nconst chokidar = require('chokidar');\r\nconst fs = require('fs');\r\nconst path = require('path');\r\nconst yaml = require('js-yaml');\r\nconst crypto = require('crypto');\r\nconst { createReadStream } = require('fs');\r\nconst { join } = require('path');\r\nconst { hydrate } = require('./hydrate');\r\n\r\n// Initialize CozoDB with RocksDB backend\r\nconst db = new CozoDb('rocksdb', path.join(__dirname, '..', 'context.db'));\r\n\r\n// Set up Express app\r\nconst app = express();\r\nconst PORT = 3000;\r\n\r\n// Serve static files from interface directory\r\napp.use(express.static(join(__dirname, '..', '..', 'interface')));\r\n\r\n// Middleware\r\napp.use(cors());\r\napp.use(bodyParser.json({ limit: '50mb' }));\r\napp.use(bodyParser.urlencoded({ extended: true }));\r\n\r\n// Initialize database schema\r\nasync function initializeDb() {\r\n  try {\r\n    // Check if the memory relation already exists\r\n    const checkQuery = '::relations';\r\n    const relations = await db.run(checkQuery);\r\n\r\n    // Only create the memory table if it doesn't already exist\r\n    if (!relations.rows.some(row => row[0] === 'memory')) {\r\n        const schemaQuery = ':create memory {id: String => timestamp: Int, content: String, source: String, type: String, hash: String, bucket: String}';\r\n        await db.run(schemaQuery);\r\n        console.log('Database schema initialized');\r\n    } else {\r\n        console.log('Database schema already exists');\r\n    }\r\n\r\n    // Try to create FTS index (optional, may not be supported in all builds)\r\n    try {\r\n      // Create FTS index for content field - using correct CozoDB syntax\r\n      // We use a simpler syntax and handle existing index gracefully\r\n      const ftsQuery = `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]}`;\r\n      await db.run(ftsQuery);\r\n      console.log('FTS index created');\r\n    } catch (e) {\r\n      // FTS might not be supported in this build or index might already exist\r\n      if (e.message && e.message.includes('already exists')) {\r\n        console.log('FTS index already exists');\r\n      } else {\r\n        console.log('FTS creation failed (optional feature):', e.message);\r\n      }\r\n    }\r\n    console.log('Database initialization complete');\r\n  } catch (error) {\r\n    console.error('Error initializing database:', error);\r\n    throw error;\r\n  }\r\n}\r\n\r\n// Automatically hydrate from the latest snapshot in the backups folder\r\nasync function autoHydrate() {\r\n  const backupsDir = path.join(__dirname, '..', '..', 'backups');\r\n  if (!fs.existsSync(backupsDir)) {\r\n    console.log('No backups directory found at ' + backupsDir + ', skipping auto-hydration.');\r\n    return;\r\n  }\r\n\r\n  try {\r\n    // Check if the database already has data\r\n    const countQuery = '?[count(id)] := *memory{id}';\r\n    const countResult = await db.run(countQuery);\r\n    const count = countResult.rows[0][0];\r\n\r\n    if (count > 0) {\r\n      console.log(` Database already contains ${count} memories. Skipping auto-hydration.`);\r\n      console.log(` To force re-hydration, delete the 'engine/context.db' folder or clear the 'memory' relation.`);\r\n      return;\r\n    }\r\n\r\n    const files = fs.readdirSync(backupsDir)\r\n      .filter(f => f.endsWith('.yaml') || f.endsWith('.yml'))\r\n      .map(f => ({\r\n        name: f,\r\n        path: path.join(backupsDir, f),\r\n        mtime: fs.statSync(path.join(backupsDir, f)).mtime\r\n      }))\r\n      .sort((a, b) => b.mtime - a.mtime);\r\n\r\n    if (files.length > 0) {\r\n      const latest = files[0];\r\n      console.log(` Auto-hydration: Found ${files.length} snapshots. Picking latest: ${latest.name}`);\r\n      await hydrate(db, latest.path);\r\n      console.log(` Auto-hydration complete.`);\r\n    } else {\r\n      console.log('No snapshots found in backups directory.');\r\n    }\r\n  } catch (error) {\r\n    console.error('Auto-hydration failed:', error);\r\n  }\r\n}\r\n\r\n// POST /v1/ingest endpoint\r\napp.post('/v1/ingest', async (req, res) => {\r\n  try {\r\n    const { content, filename, source, type = 'text', bucket = 'core' } = req.body;\r\n    \r\n    if (!content) return res.status(400).json({ error: 'Content required' });\r\n    \r\n    // 1. Calculate Hash\r\n    const hash = crypto.createHash('md5').update(content).digest('hex');\r\n\r\n    // 2. Check for Duplicates (Deduplication)\r\n    const checkQuery = `?[id] := *memory{id, hash: $hash, bucket: $bucket}`;\r\n    const checkResult = await db.run(checkQuery, { hash, bucket });\r\n\r\n    if (checkResult.ok && checkResult.rows.length > 0) {\r\n        return res.json({ \r\n            status: 'skipped', \r\n            id: checkResult.rows[0][0], \r\n            message: 'Duplicate content detected. Skipped.' \r\n        });\r\n    }\r\n    \r\n    // 3. Insert New Memory\r\n    const id = `doc_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;\r\n    const timestamp = Date.now();\r\n    \r\n    const query = `?[id, timestamp, content, source, type, hash, bucket] <- $data :insert memory {id, timestamp, content, source, type, hash, bucket}`;\r\n    const params = {\r\n      data: [[ id, timestamp, content, source || filename || 'unknown', type, hash, bucket ]]\r\n    };\r\n    \r\n    await db.run(query, params);\r\n    \r\n    res.json({ status: 'success', id, message: 'Ingested.' });\r\n  } catch (error) {\r\n    console.error('Ingest error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// POST /v1/query endpoint\r\napp.post('/v1/query', async (req, res) => {\r\n  try {\r\n    const { query, params = {} } = req.body;\r\n\r\n    if (!query) {\r\n      return res.status(400).json({ error: 'Query is required' });\r\n    }\r\n\r\n    const result = await db.run(query, params);\r\n\r\n    res.json(result);\r\n  } catch (error) {\r\n    console.error('Query error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// Basic search function as fallback when FTS fails\r\nasync function basicSearch(query, max_chars = 5000, buckets) {\r\n  try {\r\n    // Simple search implementation - retrieve all memory entries\r\n    const useBuckets = Array.isArray(buckets) && buckets.length > 0;\r\n    const searchQuery = useBuckets \r\n      ? `?[id, timestamp, content, source, type, bucket] := *memory{id, timestamp, content, source, type, bucket}, is_in(bucket, $b)`\r\n      : `?[id, timestamp, content, source, type, bucket] := *memory{id, timestamp, content, source, type, bucket}`;\r\n    \r\n    const result = await db.run(searchQuery, useBuckets ? { b: buckets } : {});\r\n\r\n    let context = '';\r\n    let charCount = 0;\r\n\r\n    if (result.rows) {\r\n      // Filter rows that contain the query term (case insensitive)\r\n      const filteredRows = result.rows.filter(row => {\r\n        const [id, timestamp, content, source, type, b] = row;\r\n        return content.toLowerCase().includes(query.toLowerCase()) ||\r\n               source.toLowerCase().includes(query.toLowerCase());\r\n      });\r\n\r\n      // Sort by relevance (rows with query in content first, then in source)\r\n      filteredRows.sort((a, b) => {\r\n        const [a_id, a_timestamp, a_content, a_source, a_type, a_b] = a;\r\n        const [b_id, b_timestamp, b_content, b_source, b_type, b_b] = b;\r\n\r\n        const aContentMatch = a_content.toLowerCase().includes(query.toLowerCase());\r\n        const bContentMatch = b_content.toLowerCase().includes(query.toLowerCase());\r\n\r\n        // Prioritize content matches over source matches\r\n        if (aContentMatch && !bContentMatch) return -1;\r\n        if (!aContentMatch && bContentMatch) return 1;\r\n        return 0;\r\n      });\r\n\r\n      for (const row of filteredRows) {\r\n        const [id, timestamp, content, source, type, b] = row;\r\n        const entryText = `### Source: ${source}\\n${content}\\n\\n`;\r\n        if (charCount + entryText.length > max_chars) {\r\n          // Add partial content if we're near the limit\r\n          const remainingChars = max_chars - charCount;\r\n          context += entryText.substring(0, remainingChars);\r\n          break;\r\n        }\r\n        context += entryText;\r\n        charCount += entryText.length;\r\n      }\r\n    }\r\n\r\n    return { context: context || 'No results found.' };\r\n  } catch (error) {\r\n    console.error('Basic search error:', error);\r\n    return { context: 'Search failed' };\r\n  }\r\n}\r\n\r\n// POST /v1/memory/search endpoint (for context.html)\r\napp.post('/v1/memory/search', async (req, res) => {\r\n  try {\r\n    const { query, max_chars = 5000, bucket, buckets, deep = false } = req.body;\r\n    if (!query) return res.status(400).json({ error: 'Query required' });\r\n\r\n    // Support both single 'bucket' and array 'buckets'\r\n    const targetBuckets = buckets || (bucket ? [bucket] : null);\r\n\r\n    // 1. FTS Search (Get Candidates)\r\n    // If buckets are provided, filter by them. Otherwise search all.\r\n    const useBuckets = Array.isArray(targetBuckets) && targetBuckets.length > 0;\r\n    \r\n    // Scale k based on max_chars to ensure we have enough candidates to fill the requested context\r\n    // Roughly 1 candidate per 500 chars, with a minimum of 30 (or 200 for deep)\r\n    const baseK = deep ? 200 : 30;\r\n    const k = Math.max(baseK, Math.ceil(max_chars / 500));\r\n\r\n    const ftsQuery = useBuckets \r\n      ? `?[id, score] := ~memory:content_fts{id | query: $q, k: ${k}, bind_score: s}, *memory{id, bucket: b}, is_in(b, $b), score = s`\r\n      : `?[id, score] := ~memory:content_fts{id | query: $q, k: ${k}, bind_score: s}, score = s`;\r\n\r\n    const ftsParams = useBuckets ? { q: query, b: targetBuckets } : { q: query };\r\n    let ftsResult;\r\n    \r\n    try {\r\n        ftsResult = await db.run(ftsQuery, ftsParams);\r\n    } catch (e) {\r\n        // Fallback if FTS fails\r\n        console.error('FTS Error, falling back to basic:', e.message);\r\n        return res.json(await basicSearch(query, max_chars, targetBuckets));\r\n    }\r\n\r\n    if (ftsResult.rows.length === 0) {\r\n        return res.json(await basicSearch(query, max_chars, targetBuckets));\r\n    }\r\n\r\n    // 2. Fetch Content for Candidates\r\n    const ids = ftsResult.rows.map(row => row[0]);\r\n    const scores = Object.fromEntries(ftsResult.rows);\r\n    \r\n    const contentQuery = `\r\n      ?[id, content, source] := \r\n        *memory{id, content, source},\r\n        is_in(id, $ids)\r\n    `;\r\n    \r\n    const contentResult = await db.run(contentQuery, { ids });\r\n    \r\n    // 3. Elastic Window Processing\r\n    let allHits = [];\r\n    const searchRegex = new RegExp(query.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'), 'gi');\r\n\r\n    for (const row of contentResult.rows) {\r\n        const [id, content, source] = row;\r\n        let match;\r\n        searchRegex.lastIndex = 0; \r\n        while ((match = searchRegex.exec(content)) !== null) {\r\n            allHits.push({\r\n                id, source, content, \r\n                start: match.index,\r\n                end: match.index + match[0].length,\r\n                score: scores[id]\r\n            });\r\n        }\r\n    }\r\n\r\n    if (allHits.length === 0) {\r\n        return res.json(await basicSearch(query, max_chars));\r\n    }\r\n\r\n    // 4. Budgeting (Optimized)\r\n    // Increase Min Window to 300 chars for better context\r\n    const rawWindowSize = Math.floor(max_chars / allHits.length);\r\n    const windowSize = Math.min(Math.max(rawWindowSize, 300), 1500); \r\n    const padding = Math.floor(windowSize / 2);\r\n\r\n    // 5. Grouping & Merging\r\n    const docsMap = {};\r\n\r\n    for (const hit of allHits) {\r\n        if (!docsMap[hit.id]) {\r\n            docsMap[hit.id] = { \r\n                source: hit.source, \r\n                score: hit.score, \r\n                ranges: [], \r\n                content: hit.content \r\n            };\r\n        }\r\n        \r\n        const start = Math.max(0, hit.start - padding);\r\n        const end = Math.min(hit.content.length, hit.end + padding);\r\n        docsMap[hit.id].ranges.push({ start, end });\r\n    }\r\n\r\n    // 6. Build Final Context (Grouped)\r\n    let finalContext = \"\";\r\n    let totalCharsUsed = 0;\r\n    \r\n    // Sort files by relevance score\r\n    const sortedDocs = Object.values(docsMap).sort((a, b) => b.score - a.score);\r\n\r\n    for (const doc of sortedDocs) {\r\n        if (totalCharsUsed >= max_chars) break;\r\n\r\n        // Sort ranges and merge overlaps within this file\r\n        doc.ranges.sort((a, b) => a.start - b.start);\r\n        const merged = [];\r\n        if (doc.ranges.length > 0) {\r\n            let current = doc.ranges[0];\r\n            for (let i = 1; i < doc.ranges.length; i++) {\r\n                if (doc.ranges[i].start <= current.end + 50) { // Merge if close (50 chars)\r\n                    current.end = Math.max(current.end, doc.ranges[i].end);\r\n                } else {\r\n                    merged.push(current);\r\n                    current = doc.ranges[i];\r\n                }\r\n            }\r\n            merged.push(current);\r\n        }\r\n\r\n        // Header (Printed Once)\r\n        const header = `### Source: ${doc.source} (Score: ${Math.round(doc.score)})\\n`;\r\n        if (totalCharsUsed + header.length > max_chars) break;\r\n        finalContext += header;\r\n        totalCharsUsed += header.length;\r\n\r\n        // Snippets\r\n        for (const range of merged) {\r\n            const snippet = doc.content.substring(range.start, range.end).replace(/\\n/g, ' ');\r\n            const entry = `...${snippet}...\\n\\n`;\r\n            \r\n            if (totalCharsUsed + entry.length > max_chars) break;\r\n            finalContext += entry;\r\n            totalCharsUsed += entry.length;\r\n        }\r\n        \r\n        finalContext += \"---\\n\"; // Separator between files\r\n    }\r\n\r\n    res.json({ context: finalContext });\r\n\r\n  } catch (error) {\r\n    console.error('Search error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// POST /v1/system/spawn_shell endpoint (for index.html)\r\napp.post('/v1/system/spawn_shell', async (req, res) => {\r\n  try {\r\n    // For now, just return success - spawning a shell is complex and platform-dependent\r\n    // In a real implementation, this would spawn a PowerShell terminal\r\n    res.json({ success: true, message: \"Shell spawned successfully\" });\r\n  } catch (error) {\r\n    console.error('Spawn shell error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// GET /v1/backup endpoint\r\napp.get('/v1/backup', async (req, res) => {\r\n  try {\r\n    console.log(\"[Backup] Starting full database export...\");\r\n\r\n    // 1. Query EVERYTHING from the memory relation\r\n    // We explicitly select columns to ensure order\r\n    const query = `?[id, timestamp, content, source, type, hash, bucket] := *memory{id, timestamp, content, source, type, hash, bucket}`;\r\n    const result = await db.run(query);\r\n\r\n    // 2. Format as a clean List of Objects\r\n    const records = result.rows.map(row => ({\r\n      id: row[0],\r\n      timestamp: row[1],\r\n      content: row[2],\r\n      source: row[3],\r\n      type: row[4],\r\n      hash: row[5],\r\n      bucket: row[6]\r\n    }));\r\n\r\n    // 3. Convert to YAML (Block style for readability)\r\n    const yamlStr = yaml.dump(records, {\r\n      lineWidth: -1,        // Don't wrap long lines\r\n      noRefs: true,         // No aliases\r\n      quotingType: '\"',     // Force quotes for safety\r\n      forceQuotes: false\r\n    });\r\n\r\n    // 4. Save to local backups directory for safety\r\n    const filename = `cozo_memory_snapshot_${new Date().toISOString().replace(/[:.]/g, '-')}.yaml`;\r\n    const backupPath = path.join(__dirname, '../../backups', filename);\r\n    \r\n    try {\r\n      fs.writeFileSync(backupPath, yamlStr);\r\n      console.log(`[Backup] Local copy saved to ${backupPath}`);\r\n    } catch (fsErr) {\r\n      console.error('[Backup] Failed to save local copy:', fsErr.message);\r\n    }\r\n\r\n    // 5. Send as Downloadable File\r\n    res.setHeader('Content-Type', 'text/yaml');\r\n    res.setHeader('Content-Disposition', `attachment; filename=\"${filename}\"`);\r\n    res.send(yamlStr);\r\n\r\n    console.log(`[Backup] Exported ${records.length} memories to ${filename}`);\r\n\r\n  } catch (error) {\r\n    console.error('[Backup] Error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// GET /v1/buckets endpoint\r\napp.get('/v1/buckets', async (req, res) => {\r\n  try {\r\n    const query = '?[bucket] := *memory{bucket}';\r\n    const result = await db.run(query);\r\n    let buckets = [...new Set(result.rows.map(row => row[0]))].sort();\r\n    if (buckets.length === 0) buckets = ['core'];\r\n    res.json(buckets);\r\n  } catch (error) {\r\n    console.error('Buckets error:', error);\r\n    res.status(500).json({ error: error.message });\r\n  }\r\n});\r\n\r\n// GET /health endpoint\r\napp.get('/health', (req, res) => {\r\n  res.json({\r\n    status: 'Sovereign',\r\n    timestamp: new Date().toISOString(),\r\n    uptime: process.uptime()\r\n  });\r\n});\r\n\r\n// Set up file watcher for context directory\r\nfunction setupFileWatcher() {\r\n  const contextDir = path.join(__dirname, '..', '..', 'context');\r\n\r\n  // Ensure context directory exists\r\n  if (!fs.existsSync(contextDir)) {\r\n    fs.mkdirSync(contextDir, { recursive: true });\r\n  }\r\n  \r\n  const watcher = chokidar.watch(contextDir, {\r\n    ignored: [\r\n      /(^|[\\/\\\\])\\../, // ignore dotfiles\r\n      /cozo_memory_snapshot_.*\\.yaml$/\r\n    ],\r\n    persistent: true,\r\n    ignoreInitial: false, // Ingest existing files on startup\r\n    awaitWriteFinish: {\r\n      stabilityThreshold: 2000,\r\n      pollInterval: 100\r\n    }\r\n  });\r\n\r\n  watcher\r\n    .on('add', filePath => handleFileChange(filePath))\r\n    .on('change', filePath => handleFileChange(filePath))\r\n    .on('error', error => console.error('Watcher error:', error));\r\n    \r\n  console.log('File watcher initialized for context directory');\r\n}\r\n\r\nasync function handleFileChange(filePath) {\r\n  // Skip backup files\r\n  if (filePath.includes('cozo_memory_snapshot_')) return;\r\n\r\n  try {\r\n    const stats = fs.statSync(filePath);\r\n    if (stats.isDirectory()) return;\r\n    \r\n    if (stats.size > 10 * 1024 * 1024) { // Skip files > 10MB\r\n      console.log(`Skipping large file: ${filePath} (${stats.size} bytes)`);\r\n      return;\r\n    }\r\n\r\n    const ext = path.extname(filePath).toLowerCase();\r\n    const textExtensions = ['.txt', '.md', '.json', '.yaml', '.yml', '.js', '.ts', '.py', '.html', '.css', '.bat', '.ps1', '.sh'];\r\n    if (!textExtensions.includes(ext) && ext !== '') {\r\n      return;\r\n    }\r\n\r\n    console.log(`Processing: ${filePath}`);\r\n    const content = fs.readFileSync(filePath, 'utf8');\r\n    const hash = crypto.createHash('md5').update(content).digest('hex');\r\n    \r\n    const relPath = path.relative(\r\n      path.join(__dirname, '..', '..', 'context'),\r\n      filePath\r\n    );\r\n    \r\n    // Auto-Bucket Logic: Top-level folder name = Bucket\r\n    const pathParts = relPath.split(path.sep);\r\n    const bucket = pathParts.length > 1 ? pathParts[0] : 'core';\r\n\r\n    // Deduplication Check\r\n    const checkQuery = `?[id] := *memory{id, hash: $hash, bucket: $bucket}`;\r\n    const check = await db.run(checkQuery, { hash, bucket });\r\n    \r\n    if (check.ok && check.rows.length > 0) {\r\n        // Content exists. Optionally update the path/source if it moved, but for now skip.\r\n        // console.log(`Skipping duplicate: ${filePath}`);\r\n        return; \r\n    }\r\n\r\n    // Use a stable ID based on the relative path to allow updates\r\n    const id = `file_${Buffer.from(relPath).toString('base64').replace(/=/g, '')}`;\r\n    \r\n    // Using :put to update if ID matches (file edit) but hash changed\r\n    const query = `?[id, timestamp, content, source, type, hash, bucket] <- $data :put memory {id, timestamp, content, source, type, hash, bucket}`;\r\n    const params = {\r\n      data: [[ id, Date.now(), content, relPath, ext || 'text', hash, bucket ]]\r\n    };\r\n    \r\n    await db.run(query, params);\r\n    console.log(`Ingested: ${relPath}`);\r\n  } catch (error) {\r\n    console.error(`Error processing file ${filePath}:`, error);\r\n  }\r\n}\r\n\r\n// Initialize and start server\r\nasync function startServer() {\r\n  try {\r\n    await initializeDb();\r\n    \r\n    // Small delay to ensure DB is ready\r\n    await new Promise(resolve => setTimeout(resolve, 1000));\r\n    \r\n    await autoHydrate();\r\n    setupFileWatcher();\r\n    \r\n    app.listen(PORT, () => {\r\n      console.log(`Sovereign Context Engine listening on port ${PORT}`);\r\n      console.log(`Health check: http://localhost:${PORT}/health`);\r\n    });\r\n  } catch (error) {\r\n    console.error('Failed to start server:', error);\r\n    process.exit(1);\r\n  }\r\n}\r\n\r\n// Handle graceful shutdown\r\nprocess.on('SIGINT', async () => {\r\n  console.log('Shutting down gracefully...');\r\n  try {\r\n    await db.close();\r\n  } catch (e) {\r\n    console.error('Error closing database:', e);\r\n  }\r\n  process.exit(0);\r\n});\r\n\r\n// Start the server\r\nstartServer();\r\n\r\nmodule.exports = { db, app };"
    tokens: 7454
    size: 20660
  - path: engine\src\index.ts
    content: "/**\r\n * Sovereign Context Engine - Main Entry Point\r\n * \r\n * This is the primary entry point for the TypeScript-based Context Engine.\r\n * It orchestrates all the core services including database management,\r\n * context ingestion, search functionality, and API services.\r\n */\r\n\r\nimport 'dotenv/config';\r\n\r\n\r\nimport express, { Request, Response } from 'express';\r\nimport cors from 'cors';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\n\r\n// Import core services\r\nimport { db } from './core/db.js';\r\nimport { setupRoutes } from './routes/api.js';\r\n\r\n// For __dirname equivalent in ES modules\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\nconst app = express();\r\nconst PORT = parseInt(process.env['PORT'] || '3000', 10);\r\n\r\n// Middleware\r\napp.use(cors());\r\napp.use(express.json({ limit: '50mb' }));\r\napp.use(express.urlencoded({ extended: true }));\r\n\r\n// API Routes\r\nsetupRoutes(app);\r\n\r\n// Serve static files from the dist directory\r\napp.use('/static', express.static(path.join(__dirname, '../dist')));\r\n\r\n// Health check endpoint\r\napp.get('/health', (_req: Request, res: Response) => {\r\n  res.status(200).json({\r\n    status: 'Sovereign',\r\n    timestamp: new Date().toISOString(),\r\n    uptime: process.uptime(),\r\n    version: '1.0.0'\r\n  });\r\n});\r\n\r\n// Root endpoint\r\n// Serve Static Frontend\r\nconst FRONTEND_DIST = path.join(__dirname, '../../frontend/dist');\r\napp.use(express.static(FRONTEND_DIST));\r\n\r\n// Fallback for SPA routing\r\napp.get('*', (_req, res) => {\r\n  // Check if it's an API call first to avoid swallowing 404s for API\r\n  if (_req.path.startsWith('/v1') || _req.path.startsWith('/health')) {\r\n    res.status(404).json({ error: 'Not Found' });\r\n    return;\r\n  }\r\n  res.sendFile(path.join(FRONTEND_DIST, 'index.html'));\r\n});\r\n\r\n// Initialize the database and start the server\r\nasync function startServer() {\r\n  try {\r\n    console.log('Initializing Sovereign Context Engine...');\r\n\r\n    // Initialize the database\r\n    await db.init();\r\n    console.log('Database initialized successfully');\r\n\r\n    // Start the server immediately (Standard: Localhost Load First)\r\n    app.listen(PORT, () => {\r\n      console.log(`Sovereign Context Engine running on port ${PORT}`);\r\n      console.log(`Health check available at http://localhost:${PORT}/health`);\r\n    });\r\n\r\n    // Auto-Restore logic\r\n    try {\r\n      const { listBackups, restoreBackup } = await import('./services/backup/backup.js');\r\n      const backups = await listBackups();\r\n      if (backups.length > 0) {\r\n        const latest = backups[0];\r\n        console.log(`[Startup] Found backup: ${latest}. Attempting restore...`);\r\n        await restoreBackup(latest);\r\n        console.log(`[Startup] Restore complete.`);\r\n      } else {\r\n        console.log(`[Startup] No backups found. Starting fresh.`);\r\n      }\r\n    } catch (e: any) {\r\n      console.error(`[Startup] Restore failed: ${e.message}. Continuing...`);\r\n    }\r\n\r\n    // Start Watchdog\r\n    // Start Watchdog\r\n    const { startWatchdog } = await import('./services/ingest/watchdog.js');\r\n    startWatchdog();\r\n\r\n    // Start Dreamer Service (Temporal Clustering)\r\n    const { dream } = await import('./services/dreamer/dreamer.js');\r\n    const { config } = await import('./config/index.js');\r\n\r\n    console.log(`[Startup] Initializing Dreamer (Interval: ${config.DREAM_INTERVAL_MS}ms)...`);\r\n\r\n    // Trigger immediately on startup (Standard 072)\r\n    try {\r\n      console.log('[Startup] Triggering immediate Dreamer cycle...');\r\n      await dream();\r\n    } catch (e: any) {\r\n      console.error(`[Startup] Immediate Dreamer cycle failed: ${e.message}`);\r\n    }\r\n\r\n    setInterval(async () => {\r\n      try {\r\n        const result = await dream();\r\n        if (result.status !== 'skipped' && result.analyzed && result.analyzed > 0) {\r\n          console.log(`[Dreamer] Cycle Complete: Analyzed ${result.analyzed}, Updated ${result.updated}`);\r\n        }\r\n      } catch (e: any) {\r\n        console.error(`[Dreamer] Cycle Failed: ${e.message}`);\r\n      }\r\n    }, config.DREAM_INTERVAL_MS);\r\n\r\n    // Start the server (Moved to top)\r\n  } catch (error) {\r\n    console.error('Failed to start the Sovereign Context Engine:', error);\r\n    process.exit(1);\r\n  }\r\n}\r\n\r\n// Handle graceful shutdown\r\nprocess.on('SIGINT', async () => {\r\n  console.log('\\nShutting down gracefully...');\r\n  try {\r\n    await db.close();\r\n    console.log('Database connection closed.');\r\n    process.exit(0);\r\n  } catch (error) {\r\n    console.error('Error during shutdown:', error);\r\n    process.exit(1);\r\n  }\r\n});\r\n\r\n// Start the server\r\nstartServer();\r\n\r\nexport { app };"
    tokens: 1673
    size: 4633
  - path: engine\src\migrate_history.js
    content: "const fs = require('fs');\r\nconst path = require('path');\r\nconst glob = require('glob');\r\nconst yaml = require('js-yaml');\r\n\r\n// Migration script to consolidate legacy session files\r\nasync function migrateHistory() {\r\n  console.log('Starting legacy session migration...');\r\n\r\n  // Find all session files\r\n  const sessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions', 'raws');\r\n  const pattern = path.join(sessionsDir, 'sessions_part_*.json');\r\n\r\n  // Use glob to find all matching files\r\n  const sessionFiles = glob.sync(pattern);\r\n\r\n  if (sessionFiles.length === 0) {\r\n    console.log('No session files found in the expected location.');\r\n    // Try alternative path\r\n    const altSessionsDir = path.join(__dirname, '..', '..', 'context', 'Coding-Notes', 'Notebook', 'history', 'important-context', 'sessions');\r\n    const altPattern = path.join(altSessionsDir, 'sessions_part_*.json');\r\n    const altSessionFiles = glob.sync(altPattern);\r\n\r\n    if (altSessionFiles.length === 0) {\r\n      console.log('No session files found in alternative location either.');\r\n      return;\r\n    }\r\n\r\n    console.log(`Found ${altSessionFiles.length} session files in alternative location.`);\r\n    processSessionFiles(altSessionFiles);\r\n    return;\r\n  }\r\n\r\n  console.log(`Found ${sessionFiles.length} session files`);\r\n  processSessionFiles(sessionFiles);\r\n}\r\n\r\nfunction processSessionFiles(sessionFiles) {\r\n  // Sort files numerically (part_1, part_2, ..., part_10, etc.)\r\n  sessionFiles.sort((a, b) => {\r\n    const matchA = a.match(/part_(\\d+)/);\r\n    const matchB = b.match(/part_(\\d+)/);\r\n\r\n    if (matchA && matchB) {\r\n      return parseInt(matchA[1]) - parseInt(matchB[1]);\r\n    }\r\n    return a.localeCompare(b);\r\n  });\r\n\r\n  let allSessions = [];\r\n\r\n  for (const file of sessionFiles) {\r\n    console.log(`Processing: ${path.basename(file)}`);\r\n    try {\r\n      const content = fs.readFileSync(file, 'utf8');\r\n\r\n      // Try to extract valid JSON from potentially corrupted files\r\n      let data = extractValidJson(content);\r\n\r\n      if (!data) {\r\n        console.error(`Could not extract valid JSON from ${file}`);\r\n        continue;\r\n      }\r\n\r\n      // Handle both list and object formats\r\n      if (Array.isArray(data)) {\r\n        allSessions = allSessions.concat(data);\r\n      } else if (typeof data === 'object') {\r\n        allSessions.push(data);\r\n      } else {\r\n        console.log(`Unexpected data format in ${file}, skipping...`);\r\n      }\r\n    } catch (error) {\r\n      console.error(`Error reading ${file}:`, error.message);\r\n    }\r\n  }\r\n\r\n  console.log(`Merged ${allSessions.length} total sessions`);\r\n\r\n  // Save to YAML file\r\n  const outputDir = path.join(__dirname, '..', '..', 'context');\r\n  const outputFile = path.join(outputDir, 'full_history.yaml');\r\n\r\n  // Custom YAML representer for multiline strings\r\n  yaml.representer = {\r\n    ...yaml.representer,\r\n    string: (data) => {\r\n      if (data.includes('\\n')) {\r\n        return new yaml.types.Str(data, { style: '|' });\r\n      }\r\n      return data;\r\n    }\r\n  };\r\n\r\n  try {\r\n    const yamlContent = yaml.dump(allSessions, {\r\n      lineWidth: -1,\r\n      noRefs: true,\r\n      skipInvalid: true\r\n    });\r\n\r\n    fs.writeFileSync(outputFile, yamlContent, 'utf8');\r\n    console.log(`YAML file created: ${outputFile}`);\r\n\r\n    // Also save as JSON for compatibility\r\n    const jsonOutputFile = path.join(outputDir, 'full_history.json');\r\n    fs.writeFileSync(jsonOutputFile, JSON.stringify(allSessions, null, 2), 'utf8');\r\n    console.log(`JSON file created: ${jsonOutputFile}`);\r\n\r\n  } catch (error) {\r\n    console.error('Error saving YAML file:', error.message);\r\n    return;\r\n  }\r\n\r\n  console.log('Migration completed successfully!');\r\n}\r\n\r\n// Function to extract valid JSON from potentially corrupted files\r\nfunction extractValidJson(content) {\r\n  try {\r\n    // First, try to parse as regular JSON\r\n    return JSON.parse(content);\r\n  } catch (e) {\r\n    // If that fails, clean the content and try again\r\n    try {\r\n      // Remove null bytes and other control characters that often corrupt JSON\r\n      let cleanContent = content.replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]/g, '');\r\n\r\n      // Try to parse the cleaned content\r\n      return JSON.parse(cleanContent);\r\n    } catch (e2) {\r\n      // If still failing, try to extract JSON array from the content\r\n      try {\r\n        // Find the main JSON array by looking for opening [ and closing ]\r\n        const startIdx = cleanContent.indexOf('[');\r\n        const endIdx = cleanContent.lastIndexOf(']');\r\n\r\n        if (startIdx !== -1 && endIdx !== -1 && startIdx < endIdx) {\r\n          const arrayContent = cleanContent.substring(startIdx, endIdx + 1);\r\n\r\n          // Try to parse the extracted array\r\n          return JSON.parse(arrayContent);\r\n        }\r\n      } catch (e3) {\r\n        // If all attempts fail, return null\r\n        return null;\r\n      }\r\n    }\r\n  }\r\n\r\n  return null;\r\n}\r\n\r\n// Run migration if this file is executed directly\r\nif (require.main === module) {\r\n  migrateHistory().catch(console.error);\r\n}\r\n\r\nmodule.exports = { migrateHistory };"
    tokens: 1835
    size: 5145
  - path: engine\src\routes\api.ts
    content: "/**\r\n * API Routes for Sovereign Context Engine\r\n * \r\n * Standardized API Interface implementing UniversalRAG architecture.\r\n */\r\n\r\nimport { Application, Request, Response } from 'express';\r\nimport * as crypto from 'crypto';\r\nimport { db } from '../core/db.js';\r\n\r\n// Import services and types\r\nimport { executeSearch, iterativeSearch } from '../services/search/search.js';\r\nimport { dream } from '../services/dreamer/dreamer.js';\r\nimport { getState, clearState } from '../services/scribe/scribe.js';\r\nimport { listModels, loadModel, runStreamingChat } from '../services/llm/provider.js';\r\nimport { createBackup, listBackups, restoreBackup } from '../services/backup/backup.js';\r\nimport { summarizeContext } from '../services/llm/reader.js';\r\nimport { fetchAndProcess, searchWeb } from '../services/research/researcher.js';\r\nimport { SearchRequest } from '../types/api.js';\r\n\r\nexport function setupRoutes(app: Application) {\r\n  // Ingestion endpoint\r\n  app.post('/v1/ingest', async (req: Request, res: Response) => {\r\n    try {\r\n      const { content, source, type, bucket, buckets = [], tags = [] } = req.body;\r\n\r\n      if (!content) {\r\n        res.status(400).json({ error: 'Content is required' });\r\n        return;\r\n      }\r\n\r\n      // Handle legacy single-bucket param\r\n      const allBuckets = bucket ? [...buckets, bucket] : buckets;\r\n\r\n      // Generate a unique ID for the memory\r\n      const id = `mem_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\r\n      const timestamp = Date.now();\r\n      const hash = crypto.createHash('sha256').update(content).digest('hex');\r\n\r\n      // Insert into the database\r\n      console.log(`[API] Ingesting memory: ${id} (Source: ${source || 'unknown'})`);\r\n      // Schema: id => timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding\r\n      await db.run(\r\n        `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data \r\n         :insert memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n        {\r\n          data: [[\r\n            id,\r\n            timestamp,\r\n            content,\r\n            source || 'unknown',\r\n            source || 'unknown',\r\n            0,\r\n            type || 'text',\r\n            hash,\r\n            allBuckets,\r\n            [], // epochs (aligned with schema)\r\n            tags,\r\n            'external',\r\n            new Array(384).fill(0.0)\r\n          ]]\r\n        }\r\n      );\r\n\r\n      // Verification (Standard 059: Read-After-Write)\r\n      // We check for the specific ID we just inserted.\r\n      const verify = await db.run(`?[id] := *memory{id}, id = $id`, { id });\r\n      const count = verify.rows ? verify.rows.length : 0;\r\n\r\n      console.log(`[API] VERIFY ID ${id}: Found ${count}`);\r\n\r\n      if (count === 0) {\r\n        throw new Error(`Ingestion Verification Failed: ID ${id} not found after write.`);\r\n      }\r\n\r\n      try {\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n        const logPath = path.join(process.cwd(), 'debug_force.log');\r\n        fs.appendFileSync(logPath, `[${new Date().toISOString()}] Ingest Success: ${id} | Count: ${count}\\n`);\r\n        console.log(`[API] Logged to ${logPath}`);\r\n      } catch (e) {\r\n        console.error('[API] Log Write Failed', e);\r\n      }\r\n\r\n      res.status(200).json({\r\n        status: 'success',\r\n        id,\r\n        message: 'Content ingested successfully'\r\n      });\r\n    } catch (error: any) {\r\n      console.error('Ingestion error:', error);\r\n      res.status(500).json({ error: 'Failed to ingest content', details: error.message });\r\n    }\r\n  });\r\n\r\n  // POST Quarantine Atom (Standard 073)\r\n  app.post('/v1/atoms/:id/quarantine', async (req: Request, res: Response) => {\r\n    try {\r\n      const { id } = req.params;\r\n      if (!id) {\r\n        res.status(400).json({ error: 'Atom ID is required' });\r\n        return;\r\n      }\r\n\r\n      console.log(`[API] Quarantining Atom: ${id}`);\r\n\r\n      // Update Provenance + Add Tag\r\n      // We use a transaction-like update: Read -> Modify -> Write\r\n      // 1. Get current record\r\n      const check = await db.run(`?[tags] := *memory{id, tags}, id = $id`, { id });\r\n      if (!check.rows || check.rows.length === 0) {\r\n        res.status(404).json({ error: 'Atom not found' });\r\n        return;\r\n      }\r\n\r\n      const currentTags = check.rows[0][0] as string[];\r\n      const newTags = [...new Set([...currentTags, '#manually_quarantined'])];\r\n\r\n      // 2. Update Record (CozoDB :put overwrites existing key)\r\n      // We only update provenance and tags. Other fields remain same?\r\n      // No, :put replaces the whole tuple. We must read ALL fields first.\r\n\r\n      const fullRecord = await db.run(`?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] := *memory{id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}, id = $id`, { id });\r\n\r\n      if (!fullRecord.rows || fullRecord.rows.length === 0) {\r\n        res.status(500).json({ error: 'Read-Modify-Write failed' });\r\n        return;\r\n      }\r\n\r\n      const row = fullRecord.rows[0];\r\n      // row indices: 0:id, 1:ts, 2:content, 3:source, 4:sid, 5:seq, 6:type, 7:hash, 8:buckets, 9:epochs, 10:tags, 11:provenance, 12:embedding\r\n\r\n      const updatedRow = [...row];\r\n      updatedRow[10] = newTags;      // Update Tags\r\n      updatedRow[11] = 'quarantine'; // Update Provenance\r\n\r\n      await db.run(\r\n        `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data \r\n         :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}`,\r\n        { data: [updatedRow] }\r\n      );\r\n\r\n      res.status(200).json({ status: 'success', message: `Atom ${id} quarantined.` });\r\n\r\n    } catch (e: any) {\r\n      console.error(`[API] Quarantine Failed: ${e.message}`);\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // POST Search endpoint (Standard UniversalRAG + Iterative Logic)\r\n  app.post('/v1/memory/search', async (req: Request, res: Response) => {\r\n    try {\r\n      const body = req.body as SearchRequest;\r\n      if (!body.query) {\r\n        res.status(400).json({ error: 'Query is required' });\r\n        return;\r\n      }\r\n\r\n      // Handle legacy params\r\n      const bucketParam = (req.body as any).bucket;\r\n      const buckets = body.buckets || [];\r\n      const allBuckets = bucketParam ? [...buckets, bucketParam] : buckets;\r\n      const budget = (req.body as any).token_budget ? (req.body as any).token_budget * 4 : (body.max_chars || 20000);\r\n\r\n      // Use Iterative Back-off Search Strategy\r\n      const result = await iterativeSearch(\r\n        body.query,\r\n        allBuckets,\r\n        budget\r\n      );\r\n\r\n      // Construct standard response\r\n      console.log(`[API] Search \"${body.query}\" -> Found ${result.results.length} results (Attempt ${result.attempt})`);\r\n\r\n      res.status(200).json({\r\n        status: 'success',\r\n        context: result.context,\r\n        results: result.results,\r\n        attempt: result.attempt, // Report which strategy worked\r\n        metadata: {\r\n          engram_hits: 0,\r\n          vector_latency: 0,\r\n          provenance_boost_active: true,\r\n          ...(result.metadata || {})\r\n        }\r\n      });\r\n    } catch (error: any) {\r\n      console.error('Search error:', error);\r\n      res.status(500).json({ error: error.message });\r\n    }\r\n  });\r\n\r\n  // GET Search (Legacy support) - redirect to use POST effectively\r\n  app.get('/v1/memory/search', async (_req: Request, res: Response) => {\r\n    res.status(400).json({ error: \"Use POST /v1/memory/search for complex queries.\" });\r\n  });\r\n\r\n  // Get all buckets\r\n  app.get('/v1/buckets', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await db.run('?[bucket] := *memory{buckets}, bucket in buckets');\r\n      const buckets = result.rows ? [...new Set(result.rows.map((row: any) => row[0]))].sort() : [];\r\n      res.status(200).json(buckets);\r\n    } catch (error) {\r\n      console.error('Bucket retrieval error:', error);\r\n      res.status(500).json({ error: 'Failed to retrieve buckets' });\r\n    }\r\n  });\r\n\r\n  // Get all tags\r\n  app.get('/v1/tags', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await db.run('?[tag] := *memory{tags}, tag in tags');\r\n      const tags = result.rows ? [...new Set(result.rows.map((row: any) => row[0]))].sort() : [];\r\n      res.status(200).json(tags);\r\n    } catch (error) {\r\n      console.error('Tag retrieval error:', error);\r\n      res.status(500).json({ error: 'Failed to retrieve tags' });\r\n    }\r\n  });\r\n\r\n  // Backup Endpoints\r\n  // POST /v1/backup - Create a new backup\r\n  app.post('/v1/backup', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await createBackup();\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      console.error(\"Backup Failed\", e);\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // GET /v1/backups - List available backups\r\n  app.get('/v1/backups', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await listBackups();\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // POST /v1/backup/restore - Restore a specific backup\r\n  app.post('/v1/backup/restore', async (req: Request, res: Response) => {\r\n    try {\r\n      const { filename } = req.body;\r\n      if (!filename) {\r\n        res.status(400).json({ error: \"Filename required\" });\r\n        return;\r\n      }\r\n      const result = await restoreBackup(filename);\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      console.error(\"Restore Failed\", e);\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // GET /v1/backup (Legacy Dump) - Kept for compatibility or download\r\n  // Modifying to use createBackup logic but stream result?\r\n  // Current createBackup writes to disk.\r\n  // Let's redirect to disk file download if needed, or keep previous logic.\r\n  // The user wanted \"Save to server\".\r\n  // Let's keep the GET for downloading the LATEST backup or generating one on fly?\r\n  // Let's make GET just return text of latest? Or generate on fly?\r\n  // Let's generate on fly like before for \"Dump\".\r\n  app.get('/v1/backup', async (_req: Request, res: Response) => {\r\n    // Return ID of new backup? Or stream content?\r\n    // Legacy behavior was stream content.\r\n    try {\r\n      const result = await createBackup();\r\n      const path = await import('path');\r\n      // const fs = await import('fs'); // Unused\r\n      const fpath = path.join(process.cwd(), 'backups', result.filename);\r\n      res.download(fpath);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // Trigger Dream Endpoint\r\n  app.post('/v1/dream', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await dream();\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // Research Plugin Endpoint\r\n  app.post('/v1/research/scrape', async (req: Request, res: Response) => {\r\n    try {\r\n      const { url, category } = req.body;\r\n      if (!url) {\r\n        res.status(400).json({ error: 'URL required' });\r\n        return;\r\n      }\r\n\r\n      const result = await fetchAndProcess(url, category || 'article');\r\n      if (result.success) {\r\n        res.status(200).json(result);\r\n      } else {\r\n        res.status(500).json(result);\r\n      }\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // Web Search Endpoint\r\n  app.get('/v1/research/web-search', async (req: Request, res: Response) => {\r\n    try {\r\n      const q = req.query['q'] as string;\r\n      if (!q) {\r\n        res.status(400).json({ error: 'Query required' });\r\n        return;\r\n      }\r\n\r\n      const results = await searchWeb(q);\r\n      res.status(200).json(results);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n\r\n\r\n  // LLM: List Models\r\n  app.get('/v1/models', async (req: Request, res: Response) => {\r\n    try {\r\n      const dir = req.query['dir'] as string | undefined;\r\n      const models = await listModels(dir);\r\n      res.status(200).json(models);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // LLM: Load Model\r\n  app.post('/v1/inference/load', async (req: Request, res: Response) => {\r\n    try {\r\n      const { model, options, dir } = req.body; // dir optional, used to construct absolute path if model is just filename?\r\n      if (!model) {\r\n        res.status(400).json({ error: \"Model name required\" });\r\n        return;\r\n      }\r\n\r\n      // If dir provided and model is not absolute, join them\r\n      const path = await import('path');\r\n      let modelPath = model;\r\n      if (dir && !path.isAbsolute(model)) {\r\n        modelPath = path.join(dir, model);\r\n      }\r\n\r\n      const result = await loadModel(modelPath, options);\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // LLM: Chat Completions (Real Streaming + Reasoning Loop)\r\n  app.post('/v1/chat/completions', async (req: Request, res: Response) => {\r\n    try {\r\n      const { messages, temperature = 0.7, max_tokens = 2048 } = req.body;\r\n\r\n      // 1. Setup SSE\r\n      res.setHeader('Content-Type', 'text/event-stream');\r\n      res.setHeader('Cache-Control', 'no-cache');\r\n      res.setHeader('Connection', 'keep-alive');\r\n      res.flushHeaders();\r\n\r\n      // 2. Inject System Prompt\r\n      let systemContent = `You are an intelligent assistant connected to a Sovereign Context Engine.\r\nYou have access to a semantic database.\r\nTo search for information, output a search query wrapped in tags like this: <search budget=\"5000\">your query here</search>.\r\nThe 'budget' attribute (optional, default 5000) controls the max characters of context to retrieve. Use higher budget (e.g. 10000) for broad research, lower (e.g. 2000) for specific facts.\r\nStop generating after outputting the tag.\r\nWhen you receive the search results, answer the user's question using that information.`;\r\n\r\n      // --- LOGIC LOOP START ---\r\n      // Deterministic \"Code-First\" Search before the model thinks\r\n      const userMsg = messages[messages.length - 1];\r\n      if (userMsg.role === 'user') {\r\n        // Stream \"Thinking\" / Logic events\r\n        res.write(`data: ${JSON.stringify({ type: 'tool', status: 'searching', query: userMsg.content, budget: 'Auto' })}\\n\\n`);\r\n\r\n        // 1. Iterative Search (Back-off)\r\n        const searchRes = await iterativeSearch(userMsg.content, [], 20000); // High budget for retrieval\r\n\r\n        if (searchRes.results.length > 0) {\r\n          const foundMsg = `Found ${searchRes.results.length} memories (Attempt ${searchRes.attempt}). Reading...`;\r\n          res.write(`data: ${JSON.stringify({ type: 'tool_result', content: foundMsg, full_context: '[Reading Context...]' })}\\n\\n`);\r\n\r\n          // 2. Summary (Reader Service)\r\n          const summary = await summarizeContext(searchRes.results, userMsg.content);\r\n\r\n          // Inject Summary\r\n          systemContent += `\\n\\nCONTEXT SUMMARY:\\n${summary}\\n\\n(This context was automatically retrieved and summarized. Use it to answer the user.)`;\r\n\r\n          // Stream Summary to UI (Visible to User)\r\n          res.write(`data: ${JSON.stringify({ type: 'tool_result', content: 'Context Summarized', full_context: summary })}\\n\\n`);\r\n        } else {\r\n          // 0 Results\r\n          res.write(`data: ${JSON.stringify({ type: 'tool_result', content: 'No memories found', full_context: 'No relevant memories found in database.' })}\\n\\n`);\r\n        }\r\n      }\r\n      // --- LOGIC LOOP END ---\r\n\r\n      const toolSystemMsg = {\r\n        role: 'system',\r\n        content: systemContent\r\n      };\r\n\r\n      const effectiveMessages = [toolSystemMsg, ...messages];\r\n      const MAX_TURNS = 5;\r\n      let turn = 0;\r\n\r\n      // 3. Reasoning Loop\r\n      while (turn < MAX_TURNS) {\r\n        turn++;\r\n        console.log(`\\n[API]  Turn ${turn} (Thought Loop)`);\r\n\r\n        let bufferedResponse = \"\";\r\n        let fullPrompt = effectiveMessages.map((msg: any) => {\r\n          const role = msg.role || 'user';\r\n          const content = typeof msg.content === 'string' ? msg.content : JSON.stringify(msg.content || '');\r\n          if (role === 'system') return `<|system|>\\n${content}`;\r\n          if (role === 'user') return `<|user|>\\n${content}`;\r\n          if (role === 'assistant') return `<|assistant|>\\n${content}`;\r\n          return `${role.charAt(0).toUpperCase() + role.slice(1)}: ${content}`;\r\n        }).join('\\n\\n');\r\n        fullPrompt += '\\n\\n<|assistant|>\\n';\r\n\r\n        try {\r\n          await runStreamingChat(\r\n            fullPrompt,\r\n            (token: string) => {\r\n              // Stream to client\r\n              const chunk = {\r\n                id: `chatcmpl-${Date.now()}`,\r\n                object: \"chat.completion.chunk\",\r\n                created: Math.floor(Date.now() / 1000),\r\n                model: \"ece-core\",\r\n                choices: [{ index: 0, delta: { content: token }, finish_reason: null }]\r\n              };\r\n              res.write(`data: ${JSON.stringify(chunk)}\\n\\n`);\r\n              bufferedResponse += token;\r\n            },\r\n            \"You are a helpful assistant.\", // System prompt handled in fullPrompt structure for Qwen\r\n            { temperature, maxTokens: max_tokens }\r\n          );\r\n\r\n          console.log(`[API] Turn ${turn} Complete. Output Length: ${bufferedResponse.length}`);\r\n\r\n          // 4. Check for Tools (Regex updated for Budget)\r\n          const searchMatch = bufferedResponse.match(/<search(?: budget=\"(\\d+)\")?>(.*?)<\\/search>/s);\r\n          if (searchMatch) {\r\n            const budget = searchMatch[1] ? parseInt(searchMatch[1]) : 5000;\r\n            const query = searchMatch[2].trim();\r\n            console.log(`[API]  Tool Call: Search(\"${query}\") | Budget: ${budget}`);\r\n\r\n            // Notify Client of Tool Usage (Simulating a \"Thought\" or \"Tool\" event)\r\n            res.write(`data: ${JSON.stringify({ type: 'tool', status: 'searching', query, budget })}\\n\\n`);\r\n\r\n            // Execute Search (Internal Function)\r\n            const searchResult = await executeSearch(query, undefined, undefined, budget, false, 'all');\r\n\r\n            let toolOutput = \"\";\r\n            let toolDisplay = \"\"; // Concise version for UI\r\n\r\n            if (searchResult.results.length > 0) {\r\n              toolDisplay = `Found ${searchResult.results.length} memories (Total ${searchResult.context.length} chars).`;\r\n              toolOutput = `[Found ${searchResult.results.length} memories]:\\n` +\r\n                searchResult.results.map(r => `- (${new Date(r.timestamp).toISOString()}) ${r.content.substring(0, 150)}...`).join('\\n');\r\n            } else {\r\n              toolDisplay = `No memories found.`;\r\n              toolOutput = `[No memories found for \"${query}\"]`;\r\n            }\r\n\r\n            // Stream Result to Client\r\n            res.write(`data: ${JSON.stringify({ type: 'tool_result', content: toolDisplay, full_context: toolOutput })}\\n\\n`);\r\n\r\n            // Append to context\r\n            effectiveMessages.push({ role: 'assistant', content: bufferedResponse });\r\n            effectiveMessages.push({ role: 'system', content: `TOOL OUTPUT: ${toolOutput}\\nNow answer.` });\r\n\r\n            // Continue Loop\r\n            continue;\r\n\r\n          } else {\r\n            // No tool call -> Done\r\n            break;\r\n          }\r\n\r\n        } catch (streamingError: any) {\r\n          console.error(\"[API] Streaming Error:\", streamingError);\r\n          res.write(`data: ${JSON.stringify({ error: streamingError.message })}\\n\\n`);\r\n          // Force break loop\r\n          break;\r\n        }\r\n      }\r\n\r\n      // 5. Finish\r\n      res.write('data: [DONE]\\n\\n');\r\n      res.end();\r\n\r\n    } catch (e: any) {\r\n      console.error(\"Chat API Error\", e);\r\n      if (!res.headersSent) res.status(500).json({ error: e.message });\r\n      else res.end();\r\n    }\r\n  });\r\n\r\n\r\n  // Scribe State Endpoints\r\n  // Get State\r\n  // Note: We need to import getState, clearState from services.\r\n  // I will add the import at the top first, then this block.\r\n  // Actually, I can use \"import(...)\" if I don't want to mess up top level imports, but better to update top level.\r\n  // Let's assume I updated imports.\r\n\r\n  app.get('/v1/scribe/state', async (_req: Request, res: Response) => {\r\n    try {\r\n      const state = await getState();\r\n      res.status(200).json({ state });\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  app.delete('/v1/scribe/state', async (_req: Request, res: Response) => {\r\n    try {\r\n      const result = await clearState();\r\n      res.status(200).json(result);\r\n    } catch (e: any) {\r\n      res.status(500).json({ error: e.message });\r\n    }\r\n  });\r\n\r\n  // System config endpoint\r\n  app.get('/v1/system/config', (_req: Request, res: Response) => {\r\n    res.status(200).json({\r\n      status: 'success',\r\n      config: {\r\n        version: '1.0.0',\r\n        engine: 'Sovereign Context Engine',\r\n        timestamp: new Date().toISOString()\r\n      }\r\n    });\r\n  });\r\n}"
    tokens: 7775
    size: 21536
  - path: engine\src\services\backup\backup.ts
    content: "\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as readline from 'readline';\r\nimport { db } from '../../core/db.js';\r\n\r\nconst BACKUP_DIR = path.join(process.cwd(), 'backups');\r\n\r\nif (!fs.existsSync(BACKUP_DIR)) {\r\n    fs.mkdirSync(BACKUP_DIR);\r\n}\r\n\r\nexport interface BackupStats {\r\n    memory_count: number;\r\n    source_count: number;\r\n    engram_count: number;\r\n    timestamp: string;\r\n}\r\n\r\n\r\nexport async function createBackup(): Promise<{ filename: string; stats: BackupStats }> {\r\n    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');\r\n    const filename = `backup_${timestamp}.json`;\r\n    const filePath = path.join(BACKUP_DIR, filename);\r\n\r\n    console.log(`[Backup] Starting streaming backup to ${filename}...`);\r\n\r\n    const stream = fs.createWriteStream(filePath, { encoding: 'utf8' });\r\n\r\n    // Helper to write to stream and wait for drain if needed\r\n    // Helper to write to stream and wait for drain if needed\r\n    const write = (data: string): Promise<void> => {\r\n        return new Promise((resolve) => {\r\n            if (!stream.write(data)) {\r\n                stream.once('drain', resolve);\r\n            } else {\r\n                resolve();\r\n            }\r\n        });\r\n    };\r\n\r\n    let memoryCount = 0;\r\n    let sourceCount = 0;\r\n    let engramCount = 0;\r\n\r\n    try {\r\n        await write('{\\n  \"timestamp\": \"' + new Date().toISOString() + '\",\\n');\r\n\r\n        // 1. Stream Memory\r\n        await write('  \"memory\": [\\n');\r\n        let memoryLastId = '';\r\n        let firstMemory = true;\r\n\r\n        while (true) {\r\n            const query = `\r\n                ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] := \r\n                *memory{id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding},\r\n                id > $lastId,\r\n                :order id\r\n                :limit 500\r\n            `;\r\n            const result = await db.run(query, { lastId: memoryLastId });\r\n\r\n            if (!result.rows || result.rows.length === 0) break;\r\n\r\n            for (const row of result.rows) {\r\n                if (!firstMemory) await write(',\\n');\r\n                await write('    ' + JSON.stringify(row));\r\n                firstMemory = false;\r\n                memoryLastId = row[0] as string; // id is first column\r\n                memoryCount++;\r\n            }\r\n        }\r\n        await write('\\n  ],\\n');\r\n\r\n        // 2. Stream Source\r\n        await write('  \"source\": [\\n');\r\n        const sourceResult = await db.run('?[path, hash, total_atoms, last_ingest] := *source{path, hash, total_atoms, last_ingest}');\r\n        if (sourceResult.rows) {\r\n            for (let i = 0; i < sourceResult.rows.length; i++) {\r\n                if (i > 0) await write(',\\n');\r\n                await write('    ' + JSON.stringify(sourceResult.rows[i]));\r\n                sourceCount++;\r\n            }\r\n        }\r\n        await write('\\n  ],\\n');\r\n\r\n        // 3. Stream Engrams\r\n        await write('  \"engrams\": [\\n');\r\n        const engramResult = await db.run('?[key, value] := *engrams{key, value}');\r\n        if (engramResult.rows) {\r\n            for (let i = 0; i < engramResult.rows.length; i++) {\r\n                if (i > 0) await write(',\\n');\r\n                await write('    ' + JSON.stringify(engramResult.rows[i]));\r\n                engramCount++;\r\n            }\r\n        }\r\n        await write('\\n  ]\\n}');\r\n\r\n    } catch (e: any) {\r\n        console.error('[Backup] Streaming failed:', e);\r\n        stream.end();\r\n        throw e;\r\n    }\r\n\r\n    return new Promise((resolve, reject) => {\r\n        stream.end(() => {\r\n            const stats: BackupStats = {\r\n                memory_count: memoryCount,\r\n                source_count: sourceCount,\r\n                engram_count: engramCount,\r\n                timestamp: timestamp\r\n            };\r\n            console.log(`[Backup] Completed. Stats:`, stats);\r\n            resolve({ filename, stats });\r\n        });\r\n        stream.on('error', reject);\r\n    });\r\n}\r\n\r\nexport async function listBackups(): Promise<string[]> {\r\n    if (!fs.existsSync(BACKUP_DIR)) return [];\r\n    const files = await fs.promises.readdir(BACKUP_DIR);\r\n    return files.filter(f => f.endsWith('.json')).sort().reverse(); // Newest first\r\n}\r\n\r\nexport async function restoreBackup(filename: string): Promise<BackupStats> {\r\n    const filePath = path.join(BACKUP_DIR, filename);\r\n    if (!fs.existsSync(filePath)) {\r\n        throw new Error(`Backup file not found: ${filename}`);\r\n    }\r\n\r\n    console.log(`[Backup] Restoring from ${filename} (Streaming Mode)...`);\r\n\r\n    const fileStream = fs.createReadStream(filePath);\r\n    const rl = readline.createInterface({\r\n        input: fileStream,\r\n        crlfDelay: Infinity\r\n    });\r\n\r\n    let currentSection: 'none' | 'memory' | 'source' | 'engrams' = 'none';\r\n    let batch: any[] = [];\r\n    const BATCH_SIZE = 100;\r\n\r\n    // Stats tracking\r\n    let stats: BackupStats = {\r\n        memory_count: 0,\r\n        source_count: 0,\r\n        engram_count: 0,\r\n        timestamp: new Date().toISOString()\r\n    };\r\n\r\n    const flushBatch = async () => {\r\n        if (batch.length === 0) return;\r\n\r\n        if (currentSection === 'memory') {\r\n            await db.run(\r\n                `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding] <- $data\r\n                 :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, tags, epochs, provenance, embedding}`,\r\n                { data: batch }\r\n            );\r\n            stats.memory_count += batch.length;\r\n        } else if (currentSection === 'source') {\r\n            await db.run(\r\n                `?[path, hash, total_atoms, last_ingest] <- $data :put source {path, hash, total_atoms, last_ingest}`,\r\n                { data: batch }\r\n            );\r\n            stats.source_count += batch.length;\r\n        } else if (currentSection === 'engrams') {\r\n            await db.run(\r\n                `?[key, value] <- $data :put engrams {key, value}`,\r\n                { data: batch }\r\n            );\r\n            stats.engram_count += batch.length;\r\n        }\r\n\r\n        batch = [];\r\n    };\r\n\r\n    for await (const line of rl) {\r\n        const trimmed = line.trim();\r\n\r\n        // Detect Section Start\r\n        if (trimmed.startsWith('\"memory\": [')) {\r\n            currentSection = 'memory';\r\n            continue;\r\n        } else if (trimmed.startsWith('\"source\": [')) {\r\n            currentSection = 'source';\r\n            continue;\r\n        } else if (trimmed.startsWith('\"engrams\": [')) {\r\n            currentSection = 'engrams';\r\n            continue;\r\n        }\r\n\r\n        // Detect Section End\r\n        if (trimmed.startsWith('],')) {\r\n            await flushBatch();\r\n            currentSection = 'none';\r\n            continue;\r\n        }\r\n\r\n        // Process Data Lines\r\n        if (currentSection !== 'none') {\r\n            // Remove trailing comma if present\r\n            const jsonStr = trimmed.endsWith(',') ? trimmed.slice(0, -1) : trimmed;\r\n            try {\r\n                // Only parse object-like lines\r\n                if (jsonStr.startsWith('{') || jsonStr.startsWith('[')) {\r\n                    const item = JSON.parse(jsonStr);\r\n                    batch.push(item);\r\n\r\n                    if (batch.length >= BATCH_SIZE) {\r\n                        await flushBatch();\r\n                    }\r\n                }\r\n            } catch (e) {\r\n                // Ignore parsing errors for non-data lines\r\n            }\r\n        }\r\n    }\r\n\r\n    // Final flush if any leftovers (though `],` should catch it)\r\n    await flushBatch();\r\n\r\n    console.log(`[Backup] Restore Completed. Stats:`, stats);\r\n    return stats;\r\n}\r\n"
    tokens: 2661
    size: 7819
  - path: engine\src\services\dreamer\dreamer.ts
    content: "/**\r\n * Dreamer Service - Markovian Memory Organization with Epochal Historian\r\n *\r\n * Implements:\r\n * 1. Markovian reasoning for background memory organization\r\n * 2. Deterministic Temporal Tagging for grounding memories in time\r\n * 3. Epochal Historian for identifying Epochs, Episodes, and Entities\r\n */\r\n\r\nimport { db } from '../../core/db.js';\r\nimport wink from 'wink-nlp';\r\nimport model from 'wink-eng-lite-web-model';\r\n\r\n// Initialize Wink-NLP (Low-Memory Model)\r\nconst nlp = wink(model);\r\n\r\n// AsyncLock implementation for preventing concurrent dream cycles\r\nclass AsyncLock {\r\n  private locked = false;\r\n  private waiting: Array<(releaser: () => void) => void> = [];\r\n\r\n  async acquire(): Promise<() => void> {\r\n    if (!this.locked) {\r\n      this.locked = true;\r\n      return this.release.bind(this);\r\n    }\r\n\r\n    return new Promise<() => void>((resolve) => {\r\n      this.waiting.push(resolve);\r\n    });\r\n  }\r\n\r\n  private release(): void {\r\n    if (this.waiting.length > 0) {\r\n      const next = this.waiting.shift();\r\n      if (next) next(this.release.bind(this));\r\n    } else {\r\n      this.locked = false;\r\n    }\r\n  }\r\n\r\n  get isLocked(): boolean {\r\n    return this.locked;\r\n  }\r\n}\r\n\r\nconst dreamLock = new AsyncLock();\r\n\r\n// Temporal constants\r\nconst SEASONS: { [key: number]: string } = {\r\n  0: 'Winter', 1: 'Winter', 2: 'Spring',\r\n  3: 'Spring', 4: 'Spring', 5: 'Summer',\r\n  6: 'Summer', 7: 'Summer', 8: 'Autumn',\r\n  9: 'Autumn', 10: 'Autumn', 11: 'Winter'\r\n};\r\n\r\nconst QUARTERS: { [key: number]: string } = {\r\n  0: 'Q1', 1: 'Q1', 2: 'Q1',\r\n  3: 'Q2', 4: 'Q2', 5: 'Q2',\r\n  6: 'Q3', 7: 'Q3', 8: 'Q3',\r\n  9: 'Q4', 10: 'Q4', 11: 'Q4'\r\n};\r\n\r\nconst DAYS = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'];\r\nconst MONTHS = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'];\r\n\r\n/**\r\n * Generates deterministic temporal tags based on the timestamp\r\n */\r\nfunction generateTemporalTags(timestamp: number): string[] {\r\n  if (!timestamp) return [];\r\n\r\n  const date = new Date(timestamp);\r\n  if (isNaN(date.getTime())) return [];\r\n\r\n  const tags = new Set<string>();\r\n  const monthIndex = date.getMonth();\r\n\r\n  // Core Date Units\r\n  tags.add(date.getFullYear().toString());\r\n  tags.add(MONTHS[monthIndex]);\r\n  tags.add(DAYS[date.getDay()]);\r\n\r\n  // Broad Temporal Buckets\r\n  tags.add(SEASONS[monthIndex]);\r\n  tags.add(QUARTERS[monthIndex]);\r\n\r\n  // Time of Day\r\n  const hour = date.getHours();\r\n  if (hour >= 5 && hour < 12) tags.add('Morning');\r\n  else if (hour >= 12 && hour < 17) tags.add('Afternoon');\r\n  else if (hour >= 17 && hour < 21) tags.add('Evening');\r\n  else tags.add('Night');\r\n\r\n  return Array.from(tags);\r\n}\r\n\r\n/**\r\n * Performs background memory organization using Markovian reasoning\r\n * Identifies Epochs, Episodes, and Entities as part of the Epochal Historian\r\n */\r\nexport async function dream(): Promise<{ status: string; analyzed?: number; updated?: number; message?: string }> {\r\n  // Check if a dream cycle is already running\r\n  if (dreamLock.isLocked) {\r\n    return {\r\n      status: 'skipped',\r\n      message: 'Previous dream cycle still running'\r\n    };\r\n  }\r\n\r\n  const release = await dreamLock.acquire();\r\n\r\n  try {\r\n    console.log(' Dreamer: Starting self-organization cycle...');\r\n\r\n    // 1. Get all memories that might benefit from re-categorization\r\n    const allMemoriesQuery = '?[id, content, buckets, timestamp] := *memory{id, content, buckets, timestamp}';\r\n    const allMemoriesResult = await db.run(allMemoriesQuery);\r\n\r\n    if (!allMemoriesResult.rows || allMemoriesResult.rows.length === 0) {\r\n      return { status: 'success', analyzed: 0, message: 'No memories to analyze' };\r\n    }\r\n\r\n    // Filter memories that need attention\r\n    const memoriesToAnalyze = allMemoriesResult.rows.filter((row: any[]) => {\r\n      const [_, __, buckets, timestamp] = row;\r\n\r\n      // Always include memories with no buckets\r\n      if (!buckets || buckets.length === 0) return true;\r\n\r\n      // Include memories with generic buckets\r\n      const genericBuckets = ['core', 'misc', 'general', 'other', 'unknown'];\r\n      const hasOnlyGenericBuckets = buckets.every((bucket: string) => genericBuckets.includes(bucket));\r\n      if (hasOnlyGenericBuckets) return true;\r\n\r\n      // Include memories that lack temporal tags\r\n      const year = new Date(timestamp).getFullYear().toString();\r\n      if (!buckets.includes(year)) return true;\r\n\r\n      return false;\r\n    });\r\n\r\n    console.log(` Dreamer: Found ${memoriesToAnalyze.length} memories to analyze.`);\r\n\r\n    let updatedCount = 0;\r\n\r\n    // Process in batches using Shared Module\r\n    const { processInBatches } = await import('../../core/batch.js');\r\n    const { config } = await import('../../config/index.js');\r\n    const batchSize = config.DREAMER_BATCH_SIZE || 5;\r\n\r\n    const totalBatches = Math.ceil(memoriesToAnalyze.length / batchSize);\r\n    await processInBatches(memoriesToAnalyze, async (batch: any[], batchIndex: number) => {\r\n      const batchStartTime = Date.now();\r\n      if ((batchIndex + 1) % 5 === 0 || batchIndex === 0 || batchIndex === totalBatches - 1) {\r\n        console.log(`[Dreamer] Processing batch ${batchIndex + 1}/${totalBatches} (${batch.length} memories)...`);\r\n      }\r\n\r\n      // 1. Prepare updates mapping in memory\r\n      const updatesMap = new Map<string, string[]>();\r\n      for (const row of batch) {\r\n        const [id, _content, currentBuckets, timestamp] = row;\r\n        const temporalTags = generateTemporalTags(timestamp);\r\n\r\n        let newSemanticTags: string[] = [];\r\n        const meaningfulBuckets = (currentBuckets || []).filter((b: string) =>\r\n          !['core', 'pending'].includes(b) && !/^\\d{4}$/.test(b)\r\n        );\r\n\r\n        if (meaningfulBuckets.length < 2) {\r\n          newSemanticTags = ['semantic_tag_placeholder'];\r\n        }\r\n\r\n        const combinedBuckets = [...new Set([...(currentBuckets || []), ...newSemanticTags, ...temporalTags])];\r\n        let finalBuckets = [...combinedBuckets];\r\n        if (combinedBuckets.length > 1) {\r\n          const specificBuckets = combinedBuckets.filter((b: string) =>\r\n            !['core', 'pending', 'misc', 'general', 'other', 'unknown', 'inbox'].includes(b)\r\n          );\r\n          if (specificBuckets.length > 0) {\r\n            finalBuckets = specificBuckets;\r\n          }\r\n        }\r\n        updatesMap.set(id, finalBuckets);\r\n      }\r\n\r\n      // 2. Bulk fetch full data for the batch\r\n      const flatIds = batch.map(r => r[0]);\r\n      const fetchQuery = `\r\n        ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] :=\r\n        id in $flatIds,\r\n        *memory{id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}\r\n      `;\r\n      const fullDataResult = await db.run(fetchQuery, { flatIds });\r\n\r\n      if (fullDataResult.rows && fullDataResult.rows.length > 0) {\r\n        const finalUpdateData = fullDataResult.rows.map((row: any) => {\r\n          const id = row[0];\r\n          const newBuckets = updatesMap.get(id);\r\n          const updatedRow = [...row];\r\n          updatedRow[8] = newBuckets; // index 8 is buckets\r\n          return updatedRow;\r\n        });\r\n\r\n        // 3. Bulk Update\r\n        await db.run(`\r\n          ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data\r\n          :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}\r\n        `, { data: finalUpdateData });\r\n\r\n        updatedCount += finalUpdateData.length;\r\n      }\r\n\r\n      const batchDuration = Date.now() - batchStartTime;\r\n      const rate = Math.round((batch.length / (batchDuration / 1000)) * 10) / 10;\r\n      if (batchDuration > 500) {\r\n        console.log(`[Dreamer] Batch ${batchIndex + 1}/${totalBatches} completed in ${batchDuration}ms (${rate} items/sec)`);\r\n      }\r\n    }, { batchSize });\r\n\r\n    // PHASE 21: Tag Infection Protocol (Standard 068)\r\n    try {\r\n      console.log(' Dreamer: Running Tag Infection cycle...');\r\n      const { runDiscovery } = await import('../tags/discovery.js');\r\n      const { runInfectionLoop } = await import('../tags/infector.js');\r\n\r\n      // 1. Teacher learns from a sample\r\n      await runDiscovery(30);\r\n      // 2. Student infects the entire graph\r\n      await runInfectionLoop();\r\n    } catch (infectionError: any) {\r\n      console.error(' Dreamer: Tag Infection failed:', infectionError.message);\r\n    }\r\n\r\n    // NEW: The Abstraction Pyramid - Cluster and Summarize into Episodes/Epochs\r\n    await clusterAndSummarize();\r\n\r\n    // MIRROR PROTOCOL: Export to Notebook\r\n    try {\r\n      console.log(' Dreamer: Triggering Mirror Protocol...');\r\n      // Dynamic import to handle JS file and potential circular deps\r\n      const { createMirror } = await import('../mirror/mirror.js');\r\n      await createMirror();\r\n    } catch (mirrorError: any) {\r\n      console.error(' Dreamer: Mirror Protocol failed:', mirrorError.message);\r\n    }\r\n\r\n    return {\r\n      status: 'success',\r\n      analyzed: memoriesToAnalyze.length,\r\n      updated: updatedCount\r\n    };\r\n  } catch (error) {\r\n    console.error(' Dreamer Fatal Error:', error);\r\n    throw error;\r\n  } finally {\r\n    release();\r\n  }\r\n}\r\n\r\n/**\r\n * The Abstraction Pyramid: Clusters Atoms into Episodes and Epochs\r\n * Uses Iterative Summarization to prevent Context Window overflow.\r\n */\r\nasync function clusterAndSummarize(): Promise<void> {\r\n  try {\r\n    console.log(' Dreamer: Running Abstraction Pyramid analysis...');\r\n\r\n    // 1. Find Unbound Atoms (Level 1 Nodes without a Parent)\r\n    const { config } = await import('../../config/index.js');\r\n    const limit = (config.DREAMER_BATCH_SIZE || 5) * 4; // Fetch 4x batch size for clustering context\r\n\r\n    // We look for memories that are NOT a child in 'parent_of'\r\n    // Cozo: `?[id] := *memory{id}, not *parent_of{child_id: id}`\r\n    // We look for memories that are NOT a child in 'parent_of'\r\n    // Cozo: `?[id] := *memory{id}, not *parent_of{child_id: id}`\r\n    const unboundQuery = `\r\n            ?[id, timestamp, content, tags] := *memory{id, timestamp, content, tags},\r\n            not *parent_of{child_id: id},\r\n            :order timestamp\r\n    :limit ${limit}\r\n    `;\r\n    const result = await db.run(unboundQuery);\r\n\r\n    if (!result.rows || result.rows.length === 0) {\r\n      console.log(' Dreamer: No unbound atoms found.');\r\n      return;\r\n    }\r\n\r\n    const atoms = result.rows.map((r: any[]) => ({\r\n      id: r[0],\r\n      timestamp: r[1],\r\n      content: r[2],\r\n      tags: r[3] || [] // Fetch tags\r\n    }));\r\n    console.log(` Dreamer: Found ${atoms.length} unbound atoms.Clustering...`);\r\n\r\n    // 2. Temporal Clustering (Gap > 15 minutes = New Cluster)\r\n    const clusters: any[][] = [];\r\n    let currentCluster: any[] = [];\r\n    let lastTime = atoms[0].timestamp;\r\n\r\n    for (const atom of atoms) {\r\n      if (atom.timestamp - lastTime > config.DREAMER_CLUSTERING_GAP_MS) {\r\n        if (currentCluster.length > 0) clusters.push(currentCluster);\r\n        currentCluster = [];\r\n      }\r\n      currentCluster.push(atom);\r\n      lastTime = atom.timestamp;\r\n    }\r\n    if (currentCluster.length > 0) clusters.push(currentCluster);\r\n\r\n    // 3. Process Clusters -> Episodes (Level 2)\r\n    console.log(` Dreamer: Processing ${clusters.length} temporal clusters...`);\r\n    let clusterIndex = 0;\r\n    for (const cluster of clusters) {\r\n      clusterIndex++;\r\n      if (cluster.length < 3) continue; // Skip tiny clusters for now, wait for more context?\r\n      // Or just summarize them if they are old enough?\r\n      // For now, let's process clusters of size >= 3.\r\n\r\n      console.log(` Dreamer: Summarizing cluster of ${cluster.length} atoms...`);\r\n\r\n\r\n      // 3. Process Clusters -> Episodes (Level 2)\r\n      console.log(` Dreamer: Summarizing cluster of ${cluster.length} atoms (Deterministic Episode)...`);\r\n\r\n      // DETERMINISTIC EPISODE GENERATION (Standard 072)\r\n      // Instead of LLM, we use pure metadata extraction for the Episode Node.\r\n      // This is O(1) and instant.\r\n\r\n      // A. Extract Date Range\r\n      const startTime = cluster[0].timestamp;\r\n      const endTime = cluster[cluster.length - 1].timestamp;\r\n\r\n      // B. Concatenate content for Wink Analysis\r\n      const fullText = cluster.map((a: any) => a.content).join('\\n');\r\n\r\n      // C. Extract Entities & Keywords using Wink\r\n      const doc = nlp.readDoc(fullText);\r\n\r\n      // Get top entities (if any)\r\n      // Filter: Must be > 2 chars, exclude numbers\r\n      const entities = doc.entities().out(nlp.its.value, nlp.as.freqTable)\r\n        .filter((e: any) => e[0].length > 2 && !/^\\d+$/.test(e[0]))\r\n        .slice(0, 10)\r\n        .map((e: any) => e[0]);\r\n\r\n      // D. Aggregate Tags (The \"Sovereign\" Context)\r\n      // Instead of weak NLP nouns, we harvest the high-quality tags from the atoms themselves.\r\n      const tagCounts = new Map<string, number>();\r\n      cluster.forEach((atom: any) => {\r\n        atom.tags.forEach((tag: string) => {\r\n          tagCounts.set(tag, (tagCounts.get(tag) || 0) + 1);\r\n        });\r\n      });\r\n\r\n      // Sort tags by frequency\r\n      const topics = Array.from(tagCounts.entries())\r\n        .sort((a, b) => b[1] - a[1])\r\n        .slice(0, 8)\r\n        .map(entry => entry[0]);\r\n\r\n      // Fallback: If no tags, try basic NLP nouns\r\n      if (topics.length === 0) {\r\n        const nouns = doc.tokens()\r\n          .filter((t: any) => t.out(nlp.its.pos) === 'NOUN' && !t.out((nlp.its as any).stopWord))\r\n          .out(nlp.its.normal, nlp.as.freqTable)\r\n          .filter((t: any) => t[0].length > 3 && !/^[0-9]+$/.test(t[0]))\r\n          .slice(0, 5)\r\n          .map((t: any) => t[0]);\r\n        topics.push(...nouns);\r\n      }\r\n\r\n      // D. Construct Metadata-Rich Content\r\n      const episodeContent = `\r\nEPISODE HEADER\r\nRange: ${new Date(startTime).toISOString()} - ${new Date(endTime).toISOString()}\r\nTopics: ${topics.join(', ')}\r\nEntities: ${entities.join(', ')}\r\nAtom Count: ${cluster.length}\r\n      `.trim();\r\n\r\n      // Create Episode Node (Level 2)\r\n      const crypto = await import('crypto');\r\n      const summaryHash = crypto.createHash('sha256').update(episodeContent).digest('hex');\r\n      const episodeId = `ep_${summaryHash.substring(0, 16)}`;\r\n\r\n      // Insert Summary Node\r\n      await db.run(\r\n        `?[id, type, content, span_start, span_end, embedding] <- [[$id, $type, $content, $start, $end, $emb]]\r\n      :put summary_node { id, type, content, span_start, span_end, embedding }`,\r\n        {\r\n          id: episodeId,\r\n          type: 'episode',\r\n          content: episodeContent,\r\n          start: startTime,\r\n          end: endTime,\r\n          emb: new Array(384).fill(0.0) // Placeholder\r\n        }\r\n      );\r\n\r\n      // Link Atoms to Episode (Parent_Of)\r\n      const edges = cluster.map(atom => [episodeId, atom.id, 1.0]);\r\n      await db.run(\r\n        `?[parent_id, child_id, weight] <- $edges :put parent_of { parent_id, child_id, weight }`,\r\n        { edges }\r\n      );\r\n\r\n      console.log(` Dreamer: Created Episode ${episodeId} (Topics: ${topics.join(', ')})`);\r\n    }\r\n\r\n  } catch (e: any) {\r\n    console.error(' Dreamer: Error in Abstraction Pyramid:', e.message);\r\n  }\r\n}"
    tokens: 5498
    size: 15395
  - path: engine\src\services\inference\inference.ts
    content: "/**\r\n * Inference Service for Sovereign Context Engine\r\n * \r\n * Handles all LLM inference operations including model loading,\r\n * chat sessions, and token streaming.\r\n */\r\n\r\n// import { db } from '../../core/db'; // Unused import\r\nimport config from '../../config/index';\r\n// import { fileURLToPath } from 'url'; // Unused\r\n\r\n\r\n// For __dirname equivalent in ES modules\r\n// const __filename = fileURLToPath(import.meta.url); // Unused\r\n// const __dirname = path.dirname(__filename); // This variable is not used anywhere else in the file.\r\n\r\n// Define interfaces\r\ninterface InferenceOptions {\r\n  model?: string;\r\n  contextSize?: number;\r\n  gpuLayers?: number;\r\n  temperature?: number;\r\n  maxTokens?: number;\r\n}\r\n\r\ninterface ChatRequest {\r\n  messages: Array<{ role: string; content: string }>;\r\n  model?: string;\r\n  options?: InferenceOptions;\r\n}\r\n\r\n// Placeholder for the actual Llama provider implementation\r\nclass LlamaProvider {\r\n  async loadModel(modelPath: string, _options: InferenceOptions): Promise<any> {\r\n    // In a real implementation, this would load the actual model\r\n    console.log(`Loading model from: ${modelPath}`);\r\n    return { model: modelPath, loaded: true };\r\n  }\r\n\r\n  async createSession(model: any, contextSize: number): Promise<any> {\r\n    // In a real implementation, this would create a chat session\r\n    return { model, contextSize, sessionId: Math.random().toString(36).substr(2, 9) };\r\n  }\r\n\r\n  async chatCompletion(_session: any, _messages: any[], _options: InferenceOptions): Promise<any> {\r\n    // In a real implementation, this would run the actual inference\r\n    return {\r\n      choices: [{\r\n        message: {\r\n          role: 'assistant',\r\n          content: 'This is a simulated response from the LLM.'\r\n        }\r\n      }]\r\n    };\r\n  }\r\n}\r\n\r\nconst llamaProvider = new LlamaProvider();\r\n\r\n/**\r\n * Initialize the inference engine with the specified model\r\n */\r\nexport async function initializeInference(modelPath?: string, options: InferenceOptions = {}): Promise<{ success: boolean; message: string; model?: any }> {\r\n  try {\r\n    // const modelToLoad = modelPath || config.MODELS.MAIN.PATH; // Unused\r\n    const inferenceOptions = {\r\n      contextSize: options.contextSize || config.MODELS.MAIN.CTX_SIZE,\r\n      gpuLayers: options.gpuLayers || config.MODELS.MAIN.GPU_LAYERS,\r\n      temperature: options.temperature || 0.7,\r\n      maxTokens: options.maxTokens || 1024\r\n    };\r\n\r\n    const modelPathString = modelPath || 'default-model';\r\n    const model = await llamaProvider.loadModel(modelPathString, inferenceOptions);\r\n\r\n    return {\r\n      success: true,\r\n      message: 'Inference engine initialized successfully',\r\n      model\r\n    };\r\n  } catch (error: any) {\r\n    return {\r\n      success: false,\r\n      message: `Failed to initialize inference engine: ${error.message}`\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Run a chat completion with the loaded model\r\n */\r\nexport async function runChatCompletion(request: ChatRequest): Promise<{ success: boolean; response?: any; error?: string }> {\r\n  try {\r\n    // In a real implementation, we would use the actual loaded model\r\n    // For now, we'll simulate the response\r\n\r\n    const response = await llamaProvider.chatCompletion(\r\n      { /* placeholder for actual model */ },\r\n      request.messages,\r\n      request.options || {}\r\n    );\r\n\r\n    return {\r\n      success: true,\r\n      response: response.choices[0].message\r\n    };\r\n  } catch (error: any) {\r\n    return {\r\n      success: false,\r\n      error: error.message\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Run a simple text completion\r\n */\r\nexport async function runCompletion(prompt: string, options: InferenceOptions = {}): Promise<{ success: boolean; response?: string; error?: string }> {\r\n  try {\r\n    // Simulate a completion request\r\n    const messages = [{ role: 'user', content: prompt }];\r\n    const request: ChatRequest = { messages, options };\r\n\r\n    const result = await runChatCompletion(request);\r\n\r\n    if (result.success && result.response) {\r\n      return {\r\n        success: true,\r\n        response: result.response.content as string\r\n      };\r\n    } else {\r\n      return {\r\n        success: false,\r\n        error: result.error || 'Unknown error occurred'\r\n      };\r\n    }\r\n  } catch (error: any) {\r\n    return {\r\n      success: false,\r\n      error: error.message\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Get the current status of the inference engine\r\n */\r\nexport function getInferenceStatus(): { loaded: boolean; model?: string; error?: string } {\r\n  // In a real implementation, this would check the actual model status\r\n  return {\r\n    loaded: true, // Assuming it's loaded for this simulation\r\n    model: config.MODELS.MAIN.PATH,\r\n    error: undefined\r\n  };\r\n}"
    tokens: 1649
    size: 4690
  - path: engine\src\services\ingest\atomizer.ts
    content: "/**\r\n * Markovian Atomizer\r\n * \r\n * Splits text content into \"Thought Atoms\" based on semantic density and natural boundaries.\r\n * Implements the \"Markovian Chunking\" strategy:\r\n * 1. Primary Split: Logical Blocks (Double Newline).\r\n * 2. Secondary Split: Length Constraint (>1000 chars) with Sentence Overlap.\r\n */\r\n\r\nexport function atomizeContent(text: string, strategy: 'code' | 'prose' | 'blob' = 'prose'): string[] {\r\n    // Strategy: Code - Split by top-level blocks (indentation-based)\r\n    if (strategy === 'code') {\r\n        const lines = text.split('\\n');\r\n        const atoms: string[] = [];\r\n        let currentChunk = '';\r\n\r\n        // Helper to push and reset\r\n        const pushChunk = () => {\r\n            if (currentChunk.trim().length > 0) {\r\n                atoms.push(currentChunk.trim());\r\n                currentChunk = '';\r\n            }\r\n        };\r\n\r\n        for (const line of lines) {\r\n            // Check for top-level definitions (no indentation or specific keywords)\r\n            // Regex checks for: Starts with non-whitespace, AND isn't a closing brace only\r\n            const isTopLevel = /^[^\\s]/.test(line) && !/^[\\}\\] \\t]*$/.test(line);\r\n\r\n            // If it's a new top-level block AND we have a substantial chunk, split.\r\n            // But don't split if the current chunk is small (< 500 chars) to keep related imports/vars together.\r\n            if (isTopLevel && currentChunk.length > 500) {\r\n                pushChunk();\r\n            }\r\n\r\n            // Hard limit safety valve (2000 chars)\r\n            if ((currentChunk + line).length > 2000) {\r\n                pushChunk();\r\n            }\r\n\r\n            currentChunk += line + '\\n';\r\n        }\r\n        pushChunk();\r\n        return enforceMaxSize(atoms, 6000, 200);\r\n    }\r\n\r\n    if (strategy === 'blob') {\r\n        // Just hard split every 1500 chars with overlap to be extremely safe for dense/binary text\r\n        return enforceMaxSize([text], 1500, 100);\r\n    }\r\n\r\n    // 1. Primary Split: Logical Blocks (Paragraphs)\r\n    // This preserves the \"Thought\" unit.\r\n    const rawBlocks = text.split(/\\n\\s*\\n/);\r\n\r\n    const atoms: string[] = [];\r\n\r\n    for (const block of rawBlocks) {\r\n        if (block.trim().length === 0) continue;\r\n\r\n        // 2. Secondary Split: Length Constraint (800 chars)\r\n        // If a paragraph is massive, we chop it by sentence.\r\n        if (block.length > 800) {\r\n            // Split by sentence endings (. ! ? ) followed by space or end of string\r\n            const sentences = block.match(/[^.!?]+[.!?]+(\\s+|$)|[^.!?]+$/g) || [block];\r\n\r\n            let currentChunk = \"\";\r\n\r\n            for (const sentence of sentences) {\r\n                if ((currentChunk + sentence).length > 800) {\r\n                    if (currentChunk.trim().length > 0) {\r\n                        atoms.push(currentChunk.trim());\r\n                    }\r\n\r\n                    // OVERLAP: Keep the last sentence as the start of the new chunk\r\n                    // This creates the \"Markov Link\"\r\n                    const sentenceParts = currentChunk.match(/[^.!?]+[.!?]+(\\s+|$)/g);\r\n                    let lastSentence = \"\";\r\n                    if (sentenceParts && sentenceParts.length > 0) {\r\n                        lastSentence = sentenceParts[sentenceParts.length - 1];\r\n                    }\r\n\r\n                    currentChunk = lastSentence + sentence;\r\n                } else {\r\n                    currentChunk += sentence;\r\n                }\r\n            }\r\n            if (currentChunk.trim().length > 0) {\r\n                atoms.push(currentChunk.trim());\r\n            }\r\n        } else {\r\n            // Small block = 1 Atom\r\n            atoms.push(block.trim());\r\n        }\r\n    }\r\n\r\n    // FINAL PASS: Strict Size Enforcement\r\n    // Ensure no atom exceeds the hard limit (6000 chars), splitting strictly if needed.\r\n    return enforceMaxSize(atoms, 6000, 200);\r\n}\r\n\r\n/**\r\n * Splits atoms that exceed the maxSize into smaller overlapping chunks.\r\n */\r\nfunction enforceMaxSize(atoms: string[], maxSize: number, overlap: number): string[] {\r\n    const result: string[] = [];\r\n    for (const atom of atoms) {\r\n        if (atom.length <= maxSize) {\r\n            result.push(atom);\r\n        } else {\r\n            // Hard split with overlap\r\n            let i = 0;\r\n            while (i < atom.length) {\r\n                const end = Math.min(i + maxSize, atom.length);\r\n                const chunk = atom.substring(i, end);\r\n                result.push(chunk);\r\n\r\n                // If we reached the end, stop\r\n                if (end >= atom.length) break;\r\n\r\n                // Move forward by maxSize - overlap (so we back up a bit for the next chunk)\r\n                i += (maxSize - overlap);\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n\r\n}\r\n"
    tokens: 1672
    size: 4784
  - path: engine\src\services\ingest\ingest.ts
    content: "/**\r\n * Ingest Service - Memory Ingestion with Provenance Tracking\r\n *\r\n * Implements the Data Provenance feature by adding a 'provenance' column\r\n * to distinguish between \"Sovereign\" (User-Created) and \"Ancillary\" (External) data.\r\n */\r\n\r\nimport { db } from '../../core/db.js';\r\nimport crypto from 'crypto';\r\nimport { config } from '../../config/index.js';\r\n\r\ninterface IngestOptions {\r\n  atomize?: boolean;\r\n}\r\n\r\n\r\n\r\n\r\n\r\n/**\r\n * Determines the provenance of content based on its source\r\n */\r\nfunction determineProvenance(source: string, type?: string): 'sovereign' | 'external' | 'system' {\r\n  const normalizedSource = source.replace(/\\\\/g, '/');\r\n\r\n  // 1. Explicit Trusted Inbox\r\n  if (normalizedSource.includes('internal-inbox/') || normalizedSource.includes('/sovereign/') || type === 'user') {\r\n    return 'sovereign';\r\n  }\r\n\r\n  // 2. Explicit External Inbox\r\n  if (normalizedSource.includes('external-inbox/') || normalizedSource.includes('web_scrape') || normalizedSource.includes('news_agent') || type === 'external') {\r\n    return 'external';\r\n  }\r\n\r\n  // 3. Main Inbox Fallback (Treat as Sovereign for manual user drops)\r\n  if (normalizedSource.includes('/inbox/')) {\r\n    return 'sovereign';\r\n  }\r\n\r\n  // Default to external for unknown sources\r\n  return 'external';\r\n}\r\n\r\n/**\r\n * Ingest content into the memory database with provenance tracking\r\n */\r\nexport async function ingestContent(\r\n  content: string,\r\n  source: string,\r\n  type: string = 'text',\r\n  buckets: string[] = ['core'],\r\n  tags: string[] = [],\r\n  _options: IngestOptions = {}\r\n): Promise<{ status: string; id?: string; message?: string }> {\r\n\r\n  if (!content) {\r\n    throw new Error('Content is required for ingestion');\r\n  }\r\n\r\n  // Auto-assign provenance based on source\r\n  const provenance = determineProvenance(source, type);\r\n\r\n  // Generate hash for content deduplication\r\n  const hash = crypto.createHash('md5').update(content).digest('hex');\r\n\r\n  // Check if content with same hash already exists\r\n  const existingQuery = `?[id] := *memory{id, hash}, hash = $hash`;\r\n  const existingResult = await db.run(existingQuery, { hash });\r\n\r\n  if (existingResult.rows && existingResult.rows.length > 0) {\r\n    return {\r\n      status: 'skipped',\r\n      id: existingResult.rows[0][0],\r\n      message: 'Content with same hash already exists'\r\n    };\r\n  }\r\n\r\n  // Generate unique ID\r\n  const id = `mem_${Date.now()}_${crypto.randomBytes(8).toString('hex').substring(0, 16)}`;\r\n  const timestamp = Date.now();\r\n  const tagsJson = tags; // Pass as array, Cozo Napi handles it\r\n  const bucketsArray = Array.isArray(buckets) ? buckets : [buckets];\r\n  const epochsJson: string[] = []; // Pass as array\r\n\r\n  // Insert the memory with provenance information\r\n  // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding\r\n  const insertQuery = `?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}`;\r\n\r\n  await db.run(insertQuery, {\r\n    data: [[id, timestamp, content, source, source, 0, type, hash, bucketsArray, epochsJson, tagsJson, provenance, new Array(config.MODELS.EMBEDDING_DIM).fill(0.1)]]\r\n  });\r\n\r\n  // Strict Read-After-Write Verification (Standard 059)\r\n  const verify = await db.run(`?[id] := *memory{id}, id = $id`, { id });\r\n  if (!verify.rows || verify.rows.length === 0) {\r\n    throw new Error(`Ingestion Verification Failed: ID ${id} not found after write.`);\r\n  }\r\n\r\n  return {\r\n    status: 'success',\r\n    id,\r\n    message: 'Content ingested successfully with provenance tracking'\r\n  };\r\n}\r\n\r\nexport interface IngestAtom {\r\n  id: string;\r\n  content: string;\r\n  sourceId: string;\r\n  sourcePath: string; // Preservation of original context\r\n  sequence: number;\r\n  timestamp: number;\r\n  provenance: 'sovereign' | 'external' | 'quarantine';\r\n  embedding?: number[];\r\n  hash?: string; // Explicit hash to avoid ID-based guessing\r\n  tags?: string[]; // <--- NEW FIELD\r\n}\r\n\r\n/**\r\n * Ingest pre-processed atoms\r\n */\r\nexport async function ingestAtoms(\r\n  atoms: IngestAtom[],\r\n  source: string,\r\n  buckets: string[] = ['core'],\r\n  tags: string[] = [] // Batch-level tags (e.g., \"inbox\")\r\n): Promise<number> {\r\n\r\n  if (atoms.length === 0) return 0;\r\n\r\n  const rows = atoms.map(atom => {\r\n    // MERGE: Combine Batch Tags + Atom-Specific Tags (Deduplicated)\r\n    const atomSpecificTags = atom.tags || [];\r\n    const finalTags = [...new Set([...tags, ...atomSpecificTags])];\r\n\r\n    // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding\r\n    return [\r\n      atom.id,\r\n      atom.timestamp,\r\n      atom.content,\r\n      source,\r\n      atom.sourceId,\r\n      atom.sequence,\r\n      'text', // Type\r\n      atom.hash || atom.id.replace('atom_', ''),\r\n      buckets,\r\n      [], // epochs\r\n      finalTags, // <--- Use the merged tags\r\n      atom.provenance,\r\n      (atom.embedding && atom.embedding.length === config.MODELS.EMBEDDING_DIM)\r\n        ? atom.embedding\r\n        : new Array(config.MODELS.EMBEDDING_DIM).fill(0.1) // Zero-stub if embeddings disabled\r\n    ];\r\n  });\r\n\r\n  // Chunked Insert\r\n  const chunkSize = 50;\r\n  let inserted = 0;\r\n  const totalBatches = Math.ceil(rows.length / chunkSize);\r\n\r\n  console.log(`[Ingest] Starting DB Write for ${rows.length} atoms (${totalBatches} batches)...`);\r\n\r\n  for (let i = 0; i < rows.length; i += chunkSize) {\r\n    const batchNum = Math.floor(i / chunkSize) + 1;\r\n    const chunk = rows.slice(i, i + chunkSize);\r\n    try {\r\n      await db.run(`\r\n        ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data\r\n        :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}\r\n      `, { data: chunk });\r\n\r\n      if (batchNum % 10 === 0 || batchNum === totalBatches) {\r\n        console.log(`[Ingest] Batch ${batchNum}/${totalBatches} written.`);\r\n      }\r\n    } catch (e: any) {\r\n      console.error(`[Ingest] Batch insert failed: ${e.message}`);\r\n      throw e;\r\n    }\r\n\r\n    // Standard 059: Verification\r\n    try {\r\n      const chunkIds = chunk.map(row => row[0]);\r\n      const chunkIdsStr = JSON.stringify(chunkIds);\r\n      const verifyQuery = `?[id] := *memory{id}, id in ${chunkIdsStr}`;\r\n      const verifyResult = await db.run(verifyQuery);\r\n      const count = verifyResult.rows ? verifyResult.rows.length : 0;\r\n\r\n      if (count !== chunk.length) {\r\n        throw new Error(`[Ingest] CRITICAL: Verification Failed! Inserted: ${chunk.length}, Verified: ${count}.`);\r\n      } else {\r\n        inserted += count;\r\n      }\r\n    } catch (verifyError: any) {\r\n      console.error(`[Ingest] Verification Query Failed: ${verifyError.message}`);\r\n      throw verifyError;\r\n    }\r\n  }\r\n\r\n  return inserted;\r\n}\r\n\r\n/**\r\n * Bulk import YAML content with provenance tracking\r\n */\r\nexport async function importYamlContent(yamlContent: any[]): Promise<{ imported: number; skipped: number; errors: number }> {\r\n  let imported = 0;\r\n  let skipped = 0;\r\n  let errors = 0;\r\n\r\n  for (const record of yamlContent) {\r\n    try {\r\n      if (!record.content) {\r\n        errors++;\r\n        continue;\r\n      }\r\n\r\n      const result = await ingestContent(\r\n        record.content,\r\n        record.source || 'yaml_import',\r\n        record.type || 'text',\r\n        record.buckets || ['imported'],\r\n        record.tags || []\r\n      );\r\n\r\n      if (result.status === 'success') {\r\n        imported++;\r\n      } else if (result.status === 'skipped') {\r\n        skipped++;\r\n      }\r\n    } catch (error) {\r\n      console.error('YAML import error for record:', record, error);\r\n      errors++;\r\n    }\r\n  }\r\n\r\n  return { imported, skipped, errors };\r\n}"
    tokens: 2813
    size: 7901
  - path: engine\src\services\ingest\refiner.ts
    content: "// engine/src/services/ingest/refiner.ts\r\n\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as crypto from 'crypto';\r\nimport { fileURLToPath } from 'url';\r\nimport { atomizeContent as rawAtomize } from './atomizer.js';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n/**\r\n * Atom Interface\r\n */\r\nexport interface Atom {\r\n    id: string;\r\n    content: string;\r\n    sourceId: string;\r\n    sourcePath: string;\r\n    sequence: number;\r\n    timestamp: number;\r\n    provenance: 'sovereign' | 'external' | 'quarantine';\r\n    embedding?: number[];\r\n    tags: string[]; // <--- NEW: Supports Project Root Extraction\r\n}\r\n\r\n// Variable to cache sovereign keywords\r\nlet cachedSovereignKeywords: string[] | null = null;\r\n\r\n/**\r\n * Helper: Load Sovereign Keywords from context/sovereign_tags.json\r\n */\r\nfunction loadSovereignKeywords(): string[] {\r\n    if (cachedSovereignKeywords) return cachedSovereignKeywords;\r\n\r\n    try {\r\n        const possiblePaths = [\r\n            path.join(process.cwd(), 'context', 'sovereign_tags.json'),\r\n            path.join(process.cwd(), '..', 'context', 'sovereign_tags.json'),\r\n            path.join(__dirname, '../../../../context/sovereign_tags.json')\r\n        ];\r\n\r\n        for (const p of possiblePaths) {\r\n            if (fs.existsSync(p)) {\r\n                const content = fs.readFileSync(p, 'utf-8');\r\n                const json = JSON.parse(content);\r\n                if (Array.isArray(json.keywords)) {\r\n                    console.log(`[Refiner] Loaded ${json.keywords.length} Sovereign Keywords from ${p}`);\r\n                    cachedSovereignKeywords = json.keywords;\r\n                    return cachedSovereignKeywords!;\r\n                }\r\n            }\r\n        }\r\n\r\n        console.warn(`[Refiner] sovereign_tags.json not found in expected paths.`);\r\n        cachedSovereignKeywords = [];\r\n        return [];\r\n    } catch (e) {\r\n        console.error(`[Refiner] Failed to load sovereign_tags.json:`, e);\r\n        return [];\r\n    }\r\n}\r\n\r\n/**\r\n * Helper: Scan content for keywords and return relevant tags\r\n */\r\nfunction scanForSovereignTags(content: string, keywords: string[]): string[] {\r\n    const foundTags: string[] = [];\r\n    const lowerContent = content.toLowerCase();\r\n\r\n    for (const keyword of keywords) {\r\n        // Simple case-insensitive check.\r\n        if (lowerContent.includes(keyword.toLowerCase())) {\r\n            foundTags.push(`#${keyword}`);\r\n        }\r\n    }\r\n    return foundTags;\r\n}\r\n\r\n/**\r\n * HELPER: The Key Assassin\r\n * Surgically removes JSON wrapper noise without breaking code brackets.\r\n */\r\nfunction cleanseJsonArtifacts(text: string, filePath: string): string {\r\n    let clean = text;\r\n    const stats = {\r\n        metaKeys: 0,\r\n        wrappers: 0,\r\n        escapes: 0\r\n    };\r\n\r\n    // 1. Recursive Un-escape (Run this FIRST to reveal hidden keys and fix code formatting)\r\n    let pass = 0;\r\n    while (clean.includes('\\\\') && pass < 3) {\r\n        pass++;\r\n        const beforeLen = clean.length;\r\n        clean = clean.replace(/\\\\\"/g, '\"');\r\n        clean = clean.replace(/\\\\n/g, '\\n');\r\n        clean = clean.replace(/\\\\t/g, '\\t');\r\n        if (clean.length < beforeLen) stats.escapes += (beforeLen - clean.length);\r\n    }\r\n\r\n    // 2. Code Block Protection (Masking)\r\n    const codeBlocks: string[] = [];\r\n    const PLACEHOLDER = '___CODE_BLOCK_PLACEHOLDER___';\r\n\r\n    clean = clean.replace(/```[\\s\\S]*?```/g, (match) => {\r\n        codeBlocks.push(match);\r\n        return `${PLACEHOLDER}${codeBlocks.length - 1}___`;\r\n    });\r\n\r\n    // Helper to count and replace\r\n    const purge = (pattern: RegExp, type: 'metaKeys' | 'wrappers' | 'escapes') => {\r\n        const matches = clean.match(pattern);\r\n        if (matches) {\r\n            stats[type] += matches.length;\r\n            clean = clean.replace(pattern, '');\r\n        }\r\n    };\r\n\r\n    // 3. Remove known metadata keys (Only from non-code text)\r\n    purge(/\"type\"\\s*:\\s*\"[^\"]*\",?/g, 'metaKeys');\r\n    purge(/\"timestamp\"\\s*:\\s*\"[^\"]*\",?/g, 'metaKeys');\r\n    purge(/\"source\"\\s*:\\s*\"[^\"]*\",?/g, 'metaKeys');\r\n\r\n    // 4. Remove the wrapper keys\r\n    purge(/\"response_content\"\\s*:\\s*/g, 'wrappers');\r\n    purge(/\"thinking_content\"\\s*:\\s*/g, 'wrappers');\r\n    purge(/\"content\"\\s*:\\s*/g, 'wrappers');\r\n\r\n    // 5. Structural cleanup\r\n    // Matches: }, {  OR  },{\r\n    clean = clean.replace(/\\}\\s*,\\s*\\{/g, '\\n\\n');\r\n\r\n    // 6. Clean up outer brackets\r\n    clean = clean.trim();\r\n    if (clean.startsWith('[') && clean.endsWith(']')) {\r\n        clean = clean.substring(1, clean.length - 1);\r\n    }\r\n\r\n    // 7. Restore Code Blocks (Unmasking)\r\n    clean = clean.replace(/___CODE_BLOCK_PLACEHOLDER___(\\d+)___/g, (match, index) => {\r\n        return codeBlocks[parseInt(index)] || match;\r\n    });\r\n\r\n    // 8. Slash Compressor (Context Hygiene)\r\n    // Collapses \"C:\\\\\\\\\\\\\\\\Users\" -> \"C:/Users\" to save tokens\r\n    const beforeSlash = clean.length;\r\n    clean = clean.replace(/\\\\{2,}/g, '/');\r\n    if (clean.length < beforeSlash) {\r\n        // console.log(`[Refiner] Slash Compressor saved ${beforeSlash - clean.length} chars.`);\r\n    }\r\n\r\n    if (stats.metaKeys > 0 || stats.wrappers > 0 || stats.escapes > 0) {\r\n        console.log(`[Refiner] Key Assassin Report for ${filePath}: Removed ${stats.metaKeys} Metadata Keys, ${stats.wrappers} Wrappers, and processed ${stats.escapes} escape chars.`);\r\n    }\r\n\r\n    return clean;\r\n}\r\n\r\n/**\r\n * NEW: Project Root Extractor\r\n * Automatically derives context tags from the file path.\r\n */\r\nfunction extractProjectTags(filePath: string): string[] {\r\n    const tags: string[] = [];\r\n    const normalized = filePath.replace(/\\\\/g, '/');\r\n    const parts = normalized.split('/');\r\n\r\n    // 1. Project Tag (Root Folder)\r\n    // Assumption: path is relative to notebook/inbox or project root\r\n    if (parts[0] === 'codebase' && parts[1]) {\r\n        tags.push(`#project:${parts[1]}`);\r\n    }\r\n    // Fallback: If we are in ECE_Core root\r\n    else if (process.cwd().includes('ECE_Core')) {\r\n        tags.push(`#project:ECE_Core`);\r\n    }\r\n\r\n    // 2. Structural Tags (src, specs, tests)\r\n    if (normalized.includes('/src/') || normalized.startsWith('src/')) tags.push('#src');\r\n    if (normalized.includes('/specs/') || normalized.startsWith('specs/')) tags.push('#specs');\r\n    if (normalized.includes('/tests/') || normalized.startsWith('tests/')) tags.push('#test');\r\n    if (normalized.includes('/docs/') || normalized.startsWith('docs/')) tags.push('#docs');\r\n\r\n    // 3. File Type Tags\r\n    if (normalized.endsWith('.ts') || normalized.endsWith('.js')) tags.push('#code');\r\n    if (normalized.endsWith('.md')) tags.push('#doc');\r\n\r\n    return tags;\r\n}\r\n\r\n/**\r\n * Refine Content\r\n */\r\nexport async function refineContent(rawBuffer: Buffer | string, filePath: string, options: { skipEmbeddings?: boolean } = {}): Promise<Atom[]> {\r\n    options.skipEmbeddings = true; // Tag-Walker Standard\r\n\r\n    let cleanText = '';\r\n\r\n    // --- PHASE 1: BASIC DECODING ---\r\n    if (Buffer.isBuffer(rawBuffer)) {\r\n        cleanText = rawBuffer.toString('utf8');\r\n    } else {\r\n        cleanText = rawBuffer;\r\n    }\r\n\r\n    // Remove BOM and Binary Trash\r\n    cleanText = cleanText.replace(/^\\uFEFF/, '').replace(/[\\u0000\\uFFFD]/g, '');\r\n    cleanText = cleanText.replace(/\\r\\n/g, '\\n');\r\n\r\n    // SURGEON V2: Aggressive Regex cleaning for \"Processing...\" spam\r\n    // Handles both newlines and mid-line concatenation (e.g., \"FileA... Processing 'FileB'...\")\r\n    const beforeSurgeon = cleanText.length;\r\n    cleanText = cleanText.replace(/(?:^|\\s|\\.{3}\\s*)Processing '[^']+'\\.{3}/g, '\\n');\r\n\r\n    // Clean up resulting empty lines\r\n    cleanText = cleanText.replace(/\\n{3,}/g, '\\n\\n');\r\n\r\n    // Logic for logging removal stats\r\n    if (cleanText.length < beforeSurgeon) {\r\n        // console.log(`[Refiner] Surgeon V2 removed ${beforeSurgeon - cleanText.length} chars of log spam.`);\r\n    }\r\n\r\n    let strategy: 'code' | 'prose' | 'blob' = 'prose';\r\n\r\n    // HEURISTIC FIX: Check for the specific schema keys\r\n    const isSourceCode = /\\.(ts|tsx|js|jsx|py|rs|go|java|cpp|h|c)$/.test(filePath);\r\n\r\n    const isJsonLog = !isSourceCode && (\r\n        cleanText.includes('\"response_content\":') ||\r\n        (cleanText.includes('\"type\":') && cleanText.includes('\"Coda')) ||\r\n        cleanText.includes('\"thinking_content\":')\r\n    );\r\n\r\n    if (isJsonLog || filePath.endsWith('.json')) {\r\n        console.log(`[Refiner] Detected JSON artifacts in ${filePath}. Attempting extraction...`);\r\n\r\n        try {\r\n            // STRATEGY A: Try Parsing (Perfect extraction)\r\n            let jsonText = cleanText;\r\n            if (!jsonText.trim().startsWith('[')) {\r\n                const arrayStart = jsonText.indexOf('[');\r\n                const arrayEnd = jsonText.lastIndexOf(']');\r\n                if (arrayStart !== -1 && arrayEnd !== -1) {\r\n                    jsonText = jsonText.substring(arrayStart, arrayEnd + 1);\r\n                }\r\n            }\r\n\r\n            const json = JSON.parse(jsonText);\r\n            const messages = Array.isArray(json) ? json : (json.messages || []);\r\n\r\n            if (Array.isArray(messages)) {\r\n                cleanText = messages.map((m: any) => {\r\n                    const role = (m.role || m.type || 'unknown').toUpperCase();\r\n                    // Prefer response_content, fallback to content\r\n                    const content = m.response_content || m.content || '';\r\n                    const ts = m.timestamp ? ` [${m.timestamp}]` : '';\r\n                    return `### ${role}${ts}\\n${content}`;\r\n                }).join('\\n\\n');\r\n\r\n                // DOUBLE-TAP: Run the Assassin on the extracted text to catch nested artifacts\r\n                cleanText = cleanseJsonArtifacts(cleanText, filePath);\r\n            } else {\r\n                console.warn(`[Refiner] JSON Structure Mismatch for ${filePath}. Running Key Assassin...`);\r\n                cleanText = cleanseJsonArtifacts(cleanText, filePath);\r\n            }\r\n        } catch (e) {\r\n            // STRATEGY B: The Key Assassin (Fallback)\r\n            console.warn(`[Refiner] JSON Parse failed for ${filePath}. Running Key Assassin...`);\r\n            cleanText = cleanseJsonArtifacts(cleanText, filePath);\r\n        }\r\n    }\r\n\r\n    // --- PHASE 3: STRATEGY SELECTION ---\r\n    const lineCount = cleanText.split('\\n').length;\r\n    const avgLineLength = cleanText.length / (lineCount || 1);\r\n\r\n    if (avgLineLength > 300 || (cleanText.length > 50000 && lineCount < 50)) {\r\n        strategy = 'blob';\r\n    } else if (/\\.(ts|js|py|rs|cpp|c|h|go|java)$/.test(filePath)) {\r\n        strategy = 'code';\r\n    }\r\n\r\n    // --- PHASE 4: ATOMIZATION ---\r\n    const rawAtoms = rawAtomize(cleanText, strategy);\r\n\r\n    // GENERATE FILE-LEVEL TAGS ONCE\r\n    const autoTags = extractProjectTags(filePath);\r\n\r\n    // LOAD KEYWORDS ONCE (Cached)\r\n    const keywords = loadSovereignKeywords();\r\n\r\n    const sourceId = crypto.createHash('md5').update(filePath).digest('hex');\r\n    const timestamp = Date.now();\r\n    const normalizedPath = filePath.replace(/\\\\/g, '/');\r\n\r\n    let provenance: 'sovereign' | 'external' = 'external';\r\n    if (normalizedPath.includes('/internal-inbox/') || normalizedPath.includes('sovereign/') || normalizedPath.includes('/inbox/')) {\r\n        provenance = 'sovereign';\r\n    } else if (normalizedPath.includes('/external-inbox/') || normalizedPath.includes('news_agent')) {\r\n        provenance = 'external';\r\n    }\r\n\r\n    return rawAtoms.map((content, index) => {\r\n        const idHash = crypto.createHash('sha256')\r\n            .update(sourceId + index.toString() + content)\r\n            .digest('hex')\r\n            .substring(0, 16);\r\n\r\n        // DYNAMIC SCAN: Check this specific atom's content for keywords\r\n        const contentTags = scanForSovereignTags(content, keywords);\r\n        let finalTags = [...new Set([...autoTags, ...contentTags])];\r\n\r\n        // QUARANTINE HEURISTICS\r\n        let atomProvenance: 'sovereign' | 'external' | 'quarantine' = provenance;\r\n\r\n        // 1. \"Processing...\" Log Spam Detection\r\n        // Surgeon V2 handled this upstream, so any surviving lines are likely intentional or deep inside code blocks.\r\n        // We leave them as sovereign.\r\n\r\n        // 2. Excessive File Path Lists (Generic)\r\n        // If > 50% of lines look like file paths\r\n        const lines = content.split('\\n');\r\n        const pathLines = lines.filter(l => l.includes('/') || l.includes('\\\\'));\r\n        if (lines.length > 10 && (pathLines.length / lines.length > 0.6)) {\r\n            // Slightly weaker check, so maybe just tag it for now unless it's obviously junk\r\n            // atomProvenance = 'quarantine'; \r\n            // finalTags.push('#potential_junk'); \r\n        }\r\n\r\n        return {\r\n            id: `atom_${idHash}`,\r\n            content: content,\r\n            sourceId: sourceId,\r\n            sourcePath: normalizedPath,\r\n            sequence: index,\r\n            timestamp: timestamp,\r\n            provenance: atomProvenance,\r\n            embedding: [],\r\n            tags: finalTags\r\n        };\r\n    });\r\n}\r\n\r\nexport async function enrichAtoms(atoms: Atom[]): Promise<Atom[]> {\r\n    return atoms;\r\n}\r\n"
    tokens: 4579
    size: 13164
  - path: engine\src\services\ingest\watchdog.ts
    content: "/**\r\n * Watchdog Service\r\n *\r\n * Scans the Notebook directory for changes and ingests new content.\r\n * Uses 'chokidar' for efficient file watching.\r\n */\r\n\r\nimport * as chokidar from 'chokidar';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as crypto from 'crypto';\r\nimport { db } from '../../core/db.js';\r\nimport { NOTEBOOK_DIR } from '../../config/paths.js';\r\nimport { ingestAtoms } from './ingest.js';\r\nimport { refineContent } from './refiner.js';\r\n\r\nlet watcher: chokidar.FSWatcher | null = null;\r\nconst IGNORE_PATTERNS = /(^|[\\/\\\\])\\../; // Ignore dotfiles\r\n\r\nexport async function startWatchdog() {\r\n    if (watcher) return;\r\n\r\n    if (!fs.existsSync(NOTEBOOK_DIR)) {\r\n        console.warn(`[Watchdog] Notebook directory not found: ${NOTEBOOK_DIR}. Skipping watch.`);\r\n        return;\r\n    }\r\n\r\n    const inbox = path.join(NOTEBOOK_DIR, 'inbox');\r\n    const externalInbox = path.join(NOTEBOOK_DIR, 'external-inbox');\r\n\r\n    console.log(`[Watchdog] Starting watch on: ${inbox} and ${externalInbox}`);\r\n\r\n    if (!fs.existsSync(inbox)) {\r\n        console.warn(`[Watchdog] Inbox directory not found: ${inbox}. Skipping watch.`);\r\n        return;\r\n    }\r\n\r\n    // Auto-create external inbox if missing\r\n    if (!fs.existsSync(externalInbox)) {\r\n        fs.mkdirSync(externalInbox, { recursive: true });\r\n    }\r\n\r\n    watcher = chokidar.watch([inbox, externalInbox], {\r\n        ignored: IGNORE_PATTERNS,\r\n        persistent: true,\r\n        ignoreInitial: false, // Force scan on start to ingest existing files\r\n        awaitWriteFinish: {\r\n            stabilityThreshold: 2000,\r\n            pollInterval: 100\r\n        }\r\n    });\r\n\r\n    watcher\r\n        .on('add', (path) => processFile(path, 'add'))\r\n        .on('change', (path) => processFile(path, 'change'));\r\n    // .on('unlink', (path) => deleteFile(path)); // Implement delete logic later\r\n}\r\n\r\nasync function processFile(filePath: string, event: string) {\r\n    if (!filePath.endsWith('.md') && !filePath.endsWith('.txt') && !filePath.endsWith('.yaml')) return;\r\n    if (filePath.includes('mirrored_brain')) return;\r\n\r\n    console.log(`[Watchdog] Detected ${event}: ${filePath}`);\r\n\r\n    try {\r\n        const buffer = await fs.promises.readFile(filePath);\r\n        if (buffer.length === 0) return;\r\n\r\n        // 1. Calculate File Hash (Raw for Change Detection)\r\n        const fileHash = crypto.createHash('sha256').update(buffer).digest('hex');\r\n        const relativePath = path.relative(NOTEBOOK_DIR, filePath);\r\n\r\n        // 2. Check Source Table\r\n        const sourceQuery = `?[path, hash] := *source{path, hash}, path = $path`;\r\n        const sourceResult = await db.run(sourceQuery, { path: relativePath });\r\n\r\n        let shouldIngest = true;\r\n        if (sourceResult.rows && sourceResult.rows.length > 0) {\r\n            const [_path, existingHash] = sourceResult.rows[0];\r\n            if (existingHash === fileHash) {\r\n                console.log(`[Watchdog] File unchanged (hash match): ${relativePath}`);\r\n                shouldIngest = false;\r\n            }\r\n        }\r\n\r\n        if (!shouldIngest) return;\r\n\r\n        console.log(`[Watchdog] Refinement Pipeline: ${relativePath}`);\r\n\r\n        // 3. Smart Refinement (Dry Run)\r\n        // Parse atoms WITHOUT generating embeddings first\r\n        const { enrichAtoms } = await import('./refiner.js');\r\n        const dryRunAtoms = await refineContent(buffer, relativePath, { skipEmbeddings: true });\r\n\r\n        const sourceId = crypto.createHash('md5').update(relativePath).digest('hex');\r\n\r\n        // 4. Fetch Existing Atoms from DB for this source\r\n        // We need ID and Hash to compare\r\n        const existingQuery = `?[id, hash] := *memory{id, source_id, hash}, source_id = $sid`;\r\n        const existingResult = await db.run(existingQuery, { sid: sourceId });\r\n\r\n        const existingMap = new Map<string, string>(); // ID -> Hash\r\n        if (existingResult.rows) {\r\n            existingResult.rows.forEach((r: any) => existingMap.set(r[0], r[1]));\r\n        }\r\n\r\n        // 5. Calculate Diff\r\n        // New Atoms: Present in dryRun but NOT in DB (by ID) OR Hash mismatch\r\n        // Deleted Atoms: Present in DB but NOT in dryRun (by ID)\r\n\r\n        const atomsToIngest: any[] = [];\r\n        const atomIdsToKeep = new Set<string>();\r\n\r\n        for (const atom of dryRunAtoms) {\r\n            atomIdsToKeep.add(atom.id);\r\n            const existingHash = existingMap.get(atom.id);\r\n\r\n            // If it's new (not in DB) or changed (hash mismatch), we need to ingest it\r\n            // Note: Atom ID includes hash in standard refiner, so usually ID change = content change.\r\n            // But if we change ID generation later, comparing hashes is safer.\r\n            if (!existingHash) {\r\n                atomsToIngest.push(atom);\r\n            } else if (existingHash !== atom.id.replace('atom_', '')) {\r\n                // Fallback check if hash isn't explicit\r\n                atomsToIngest.push(atom);\r\n            }\r\n        }\r\n\r\n        const idsToDelete: string[] = [];\r\n        for (const [id] of existingMap) {\r\n            if (!atomIdsToKeep.has(id)) {\r\n                idsToDelete.push(id);\r\n            }\r\n        }\r\n\r\n        console.log(`[Watchdog] Smart Diff for ${relativePath}: +${atomsToIngest.length} / -${idsToDelete.length} / =${atomIdsToKeep.size - atomsToIngest.length}`);\r\n\r\n        // 6. Execute Updates\r\n\r\n        // A. DELETE orphans\r\n        if (idsToDelete.length > 0) {\r\n            await db.run(`?[id] <- $ids :delete memory {id}`, { ids: idsToDelete.map(id => [id]) });\r\n        }\r\n\r\n        // B. ENRICH & INSERT new/changed\r\n        if (atomsToIngest.length > 0) {\r\n            // Now we pay the cost of embedding ONLY for the new stuff\r\n            const enrichedAtoms = await enrichAtoms(atomsToIngest);\r\n\r\n            // Improved Bucket Logic for Subfolders\r\n            const parts = relativePath.split(path.sep);\r\n            let bucket = 'notebook';\r\n\r\n            if (parts.length >= 2) {\r\n                // Check if it's inside 'inbox'\r\n                if (parts[0] === 'inbox') {\r\n                    // inbox/subfolder/file.md -> use 'subfolder'\r\n                    // inbox/file.md -> use 'inbox'\r\n                    bucket = parts.length > 2 ? parts[1] : 'inbox';\r\n                } else {\r\n                    // other_folder/file.md -> use 'other_folder'\r\n                    bucket = parts[0];\r\n                }\r\n            }\r\n\r\n            const bucketList = [bucket];\r\n\r\n            await ingestAtoms(enrichedAtoms, relativePath, bucketList, []);\r\n        }\r\n\r\n        // 7. Update Source Table - ONLY if we reached here without error\r\n        await db.run(\r\n            `?[path, hash, total_atoms, last_ingest] <- [[$path, $hash, $total, $last]] \r\n             :put source {path, hash, total_atoms, last_ingest}`,\r\n            {\r\n                path: relativePath,\r\n                hash: fileHash,\r\n                total: dryRunAtoms.length, // Total is now current valid count\r\n                last: Date.now()\r\n            }\r\n        );\r\n\r\n        if (atomsToIngest.length > 0 || idsToDelete.length > 0) {\r\n            console.log(`[Watchdog] Sync Complete: ${relativePath}`);\r\n\r\n            // Trigger Mirror Protocol for Near-Real-Time visibility\r\n            try {\r\n                const { createMirror } = await import('../mirror/mirror.js');\r\n                await createMirror();\r\n            } catch (mirrorError: any) {\r\n                console.error(`[Watchdog] Mirror Protocol trigger failed:`, mirrorError.message);\r\n            }\r\n        } else {\r\n            console.log(`[Watchdog] No atom changes detected (Metadata update only).`);\r\n        }\r\n\r\n    } catch (e: any) {\r\n        console.error(`[Watchdog] Error processing ${filePath}:`, e.message);\r\n    }\r\n}\r\n"
    tokens: 2716
    size: 7788
  - path: engine\src\services\llm\context.ts
    content: "\r\n// import type { LlamaChatSession } from 'node-llama-cpp'; // Unused\r\nimport { getModel, getContext, getCurrentCtxSize, runSideChannel } from './provider.js';\r\n\r\ninterface MockLlamaModel {\r\n    tokenize(text: string): { length: number; slice(start: number, end: number): any[] } & any[];\r\n    detokenize(tokens: any[]): string;\r\n}\r\n\r\n\r\n/**\r\n * Summarizes massive content by chunking it and processing through a side-channel session.\r\n * Prevents polluting the main chat history with raw data.\r\n */\r\nexport async function summarizeLargeContent(text: string, maxOutputTokens = 500): Promise<string> {\r\n    const model = getModel() as unknown as MockLlamaModel;\r\n    const context = getContext();\r\n\r\n    if (!text || !model || !context) return \"\";\r\n\r\n    // First, check if the text is too large and needs to be preprocessed\r\n    if (text.length > 5000) {\r\n        console.log(`[Summarizer] Content too large (${text.length} chars). Preprocessing...`);\r\n\r\n        // For very large texts, we'll use a more aggressive chunking strategy\r\n        const MAX_CHUNK_SIZE = 3000;\r\n        const chunks: string[] = [];\r\n\r\n        for (let i = 0; i < text.length; i += MAX_CHUNK_SIZE) {\r\n            chunks.push(text.substring(i, i + MAX_CHUNK_SIZE));\r\n        }\r\n\r\n        console.log(`[Summarizer] Split into ${chunks.length} chunks for processing...`);\r\n        const summaries: string[] = [];\r\n\r\n        for (const [i, chunk] of chunks.entries()) {\r\n            try {\r\n                console.log(`[Summarizer] Processing chunk ${i + 1}/${chunks.length} (${chunk.length} chars)...`);\r\n\r\n                const systemPrompt = \"You are a precise technical summarizer. Extract key facts, code snippets, and definitions. Be extremely concise.\";\r\n                const prompt = `Summarize this content in under ${Math.min(Math.floor(maxOutputTokens / chunks.length) + 20, 200)} words found below:\\n\\n${chunk}\\n\\nSummary:`;\r\n\r\n                const chunkSummary = (await runSideChannel(\r\n                    prompt,\r\n                    systemPrompt,\r\n                    { maxTokens: 300, temperature: 0.1 }\r\n                )) as string;\r\n\r\n                summaries.push(chunkSummary || `[SUMMARY UNAVAILABLE] Chunk ${i + 1} failed.`);\r\n            } catch (chunkError: any) {\r\n                console.warn(`[Summarizer] Failed to process chunk ${i + 1}:`, chunkError.message);\r\n                summaries.push(`[SUMMARY UNAVAILABLE] Failed to process chunk ${i + 1} due to context limitations.`);\r\n            }\r\n        }\r\n\r\n        // Now summarize the combined summaries if needed\r\n        const combinedSummaries = summaries.join(\"\\n\\n\");\r\n        if (combinedSummaries.length > 2000) {\r\n            console.log(`[Summarizer] Combined summaries still large (${combinedSummaries.length} chars), final summarization...`);\r\n            const finalSystem = \"You are a precise technical summarizer. Be extremely concise.\";\r\n            const finalPrompt = `Summarize these notes:\\n\\n${combinedSummaries}`;\r\n            const final = (await runSideChannel(finalPrompt, finalSystem, { maxTokens: Math.min(maxOutputTokens, 400), temperature: 0.1 })) as string;\r\n            return final || combinedSummaries;\r\n        }\r\n\r\n        return combinedSummaries;\r\n    } else {\r\n        // Original logic for smaller texts\r\n        const tokens = model.tokenize(text);\r\n        const totalTokens = tokens.length;\r\n\r\n        // Reserve space for prompt overhead + generation\r\n        const CONTEXT_WINDOW = getCurrentCtxSize();\r\n        const CHUNK_CAPACITY = Math.floor(CONTEXT_WINDOW * 0.4);\r\n\r\n        if (totalTokens <= CHUNK_CAPACITY) {\r\n            const systemPrompt = \"You are a precise technical summarizer. Extract key facts, code snippets, and definitions. Be extremely concise.\";\r\n            const prompt = `Summarize this content in under ${maxOutputTokens} words found below:\\n\\n${text}\\n\\nSummary:`;\r\n            const res = (await runSideChannel(prompt, systemPrompt, { maxTokens: maxOutputTokens, temperature: 0.1 })) as string;\r\n            return res || text.substring(0, maxOutputTokens * 4);\r\n        }\r\n\r\n        console.log(`[Summarizer] Content too large (${totalTokens} tokens). Chunking...`);\r\n        const chunks: string[] = [];\r\n        let offset = 0;\r\n        while (offset < totalTokens) {\r\n            const chunkTokens = tokens.slice(offset, offset + CHUNK_CAPACITY);\r\n            chunks.push(model.detokenize(chunkTokens));\r\n            offset += CHUNK_CAPACITY;\r\n        }\r\n\r\n        console.log(`[Summarizer] Processing ${chunks.length} chunks...`);\r\n        const summaries: string[] = [];\r\n\r\n        for (const [i, chunk] of chunks.entries()) {\r\n            const systemPrompt = \"You are a precise technical summarizer. Be extremely concise.\";\r\n            const prompt = `Summarize this chunk:\\n\\n${chunk}`;\r\n            const res = (await runSideChannel(prompt, systemPrompt, { maxTokens: 300, temperature: 0.1 })) as string;\r\n            summaries.push(res || `[Chunk ${i} Failed]`);\r\n        }\r\n\r\n        return summaries.join(\"\\n\\n\");\r\n    }\r\n}\r\n"
    tokens: 1784
    size: 5069
  - path: engine\src\services\llm\provider.ts
    content: "import { Worker } from 'worker_threads';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\nimport { MODELS_DIR } from '../../config/paths.js';\r\nimport config from '../../config/index.js';\r\n\r\n// Global State\r\nlet clientWorker: Worker | null = null;\r\nlet orchestratorWorker: Worker | null = null;\r\nlet currentChatModelName = \"\";\r\nlet currentOrchestratorModelName = \"\";\r\n\r\n// ESM __dirname fix\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n// Helper to resolve worker path dynamically based on environment (src vs dist)\r\nfunction resolveWorkerPath(relativePath: string) {\r\n  const isDev = __dirname.includes('src');\r\n  const ext = isDev ? '.ts' : '.js';\r\n  return path.resolve(__dirname, relativePath + ext);\r\n}\r\n\r\nconst CHAT_WORKER_PATH = resolveWorkerPath('../../core/inference/ChatWorker');\r\n\r\n\r\nexport interface LoadModelOptions {\r\n  ctxSize?: number;\r\n  batchSize?: number;\r\n  systemPrompt?: string;\r\n  gpuLayers?: number;\r\n}\r\n\r\n// Initialize workers based on configuration\r\nexport async function initWorker() {\r\n  // TAG-WALKER MODE (Lightweight)\r\n  // We strictly skip embedding workers to save RAM. \r\n  // All embedding calls return zero-stubs.\r\n\r\n  if (!clientWorker) {\r\n    console.log(`[Provider] Tag-Walker Mode Active. Spawning Chat Worker...`);\r\n    // Use Chat Worker for Main Chat (Standardized)\r\n    clientWorker = await spawnWorker(\"ChatWorker\", CHAT_WORKER_PATH, {\r\n      gpuLayers: config.MODELS.MAIN.GPU_LAYERS,\r\n      // Pass forceCpu if needed, but we rely on gpuLayers config\r\n      forceCpu: config.MODELS.MAIN.GPU_LAYERS === 0\r\n    });\r\n  }\r\n\r\n  // Spawn Orchestrator (Side Channel) Worker - CPU Optimized\r\n  if (!orchestratorWorker) {\r\n    orchestratorWorker = await spawnWorker(\"OrchestratorWorker\", CHAT_WORKER_PATH, {\r\n      gpuLayers: config.MODELS.ORCHESTRATOR.GPU_LAYERS,\r\n      forceCpu: config.MODELS.ORCHESTRATOR.GPU_LAYERS === 0\r\n    });\r\n  }\r\n\r\n  return clientWorker;\r\n}\r\n\r\nasync function spawnWorker(name: string, workerPath: string, workerData: any = {}): Promise<Worker> {\r\n  return new Promise((resolve, reject) => {\r\n    const w = new Worker(workerPath, { workerData });\r\n    w.on('message', (msg) => {\r\n      if (msg.type === 'ready') resolve(w);\r\n      if (msg.type === 'error') console.error(`[${name}] Error:`, msg.error);\r\n    });\r\n    w.on('error', (err) => {\r\n      console.error(`[${name}] Thread Error:`, err);\r\n      reject(err);\r\n    });\r\n    w.on('exit', (code) => {\r\n      if (code !== 0) console.error(`[${name}] Stopped with exit code ${code}`);\r\n    });\r\n  });\r\n}\r\n\r\n// Lock for initAutoLoad\r\nlet initPromise: Promise<void> | null = null;\r\n\r\n// Auto-loader for Engine Start\r\nexport async function initAutoLoad() {\r\n  if (initPromise) return initPromise;\r\n\r\n  initPromise = (async () => {\r\n    console.log(\"[Provider] Auto-loading configured models...\");\r\n    console.log(`[Provider] DEBUG: process.env['LLM_GPU_LAYERS'] = \"${process.env['LLM_GPU_LAYERS']}\"`);\r\n    console.log(`[Provider] DEBUG: process.env['LLM_MODEL_PATH'] = \"${process.env['LLM_MODEL_PATH']}\"`);\r\n    console.log(`[Provider] DEBUG: config.MODELS.MAIN.GPU_LAYERS = ${config.MODELS.MAIN.GPU_LAYERS}`);\r\n    console.log(`[Provider] DEBUG: config.MODELS.MAIN.PATH = \"${config.MODELS.MAIN.PATH}\"`);\r\n\r\n    try {\r\n      await initWorker();\r\n\r\n      // Load Chat Model\r\n      await loadModel(config.MODELS.MAIN.PATH, {\r\n        ctxSize: config.MODELS.MAIN.CTX_SIZE,\r\n        gpuLayers: config.MODELS.MAIN.GPU_LAYERS\r\n      }, 'chat');\r\n\r\n      // Load Orchestrator Model\r\n      await loadModel(config.MODELS.ORCHESTRATOR.PATH, {\r\n        ctxSize: config.MODELS.ORCHESTRATOR.CTX_SIZE,\r\n        gpuLayers: config.MODELS.ORCHESTRATOR.GPU_LAYERS\r\n      }, 'orchestrator');\r\n\r\n    } catch (e) {\r\n      console.error(\"[Provider] Auto-load failed:\", e);\r\n      // Reset promise on failure to allow retry\r\n      initPromise = null;\r\n      throw e;\r\n    }\r\n  })();\r\n\r\n  return initPromise;\r\n}\r\n\r\n// Model Loading Logic\r\nlet chatLoadingPromise: Promise<any> | null = null;\r\nlet orchLoadingPromise: Promise<any> | null = null;\r\n\r\nexport async function loadModel(modelPath: string, options: LoadModelOptions = {}, target: 'chat' | 'orchestrator' = 'chat') {\r\n  if (!clientWorker) await initWorker();\r\n\r\n  let targetWorker = clientWorker;\r\n  if (target === 'orchestrator') targetWorker = orchestratorWorker;\r\n\r\n  if (!targetWorker) throw new Error(\"Worker not initialized\");\r\n\r\n  // Check if already loaded\r\n  if (target === 'chat' && modelPath === currentChatModelName) return { status: \"ready\" };\r\n  if (target === 'orchestrator' && modelPath === currentOrchestratorModelName) return { status: \"ready\" };\r\n\r\n  // Prevent parallel loads for *same target*\r\n  if (target === 'chat' && chatLoadingPromise) return chatLoadingPromise;\r\n  if (target === 'orchestrator' && orchLoadingPromise) return orchLoadingPromise;\r\n\r\n  const loadTask = new Promise((resolve, reject) => {\r\n    const fullModelPath = path.isAbsolute(modelPath) ? modelPath : path.join(MODELS_DIR, modelPath);\r\n\r\n    const handler = (msg: any) => {\r\n      if (msg.type === 'modelLoaded') {\r\n        console.log(`[Provider] ${target} Model loaded: ${modelPath}`);\r\n        targetWorker!.off('message', handler);\r\n        if (target === 'chat') {\r\n          currentChatModelName = modelPath;\r\n          chatLoadingPromise = null;\r\n        } else {\r\n          currentOrchestratorModelName = modelPath;\r\n          orchLoadingPromise = null;\r\n        }\r\n        resolve({ status: \"success\" });\r\n      } else if (msg.type === 'error') {\r\n        targetWorker!.off('message', handler);\r\n        if (target === 'chat') chatLoadingPromise = null;\r\n        else orchLoadingPromise = null;\r\n        console.error(`[Provider] Worker ${target} Error:`, msg.error);\r\n        reject(new Error(msg.error));\r\n      }\r\n    };\r\n\r\n    targetWorker!.on('message', handler);\r\n    targetWorker!.postMessage({\r\n      type: 'loadModel',\r\n      data: { modelPath: fullModelPath, options }\r\n    });\r\n  });\r\n\r\n  if (target === 'chat') chatLoadingPromise = loadTask;\r\n  else orchLoadingPromise = loadTask;\r\n\r\n  return loadTask;\r\n}\r\n\r\n// ... Inference ...\r\n\r\nexport async function runInference(prompt: string, data: any) {\r\n  if (!clientWorker || !currentChatModelName) throw new Error(\"Chat Model not loaded\");\r\n  // Stub implementation\r\n  console.log(\"runInference called with\", prompt.substring(0, 10), data ? \"data present\" : \"no data\");\r\n  return null;\r\n}\r\n\r\nexport async function runStreamingChat(\r\n  prompt: string,\r\n  onToken: (token: string) => void,\r\n  systemInstruction = \"You are a helpful assistant.\",\r\n  options: any = {}\r\n): Promise<string> {\r\n  // Always use ClientWorker for Main Chat\r\n  const targetWorker = clientWorker;\r\n  const targetModel = currentChatModelName;\r\n\r\n  if (!targetWorker || !targetModel) {\r\n    console.log(\"[Provider] Chat Model not loaded, auto-loading...\");\r\n    await initAutoLoad();\r\n    if (!clientWorker || !currentChatModelName) throw new Error(\"Chat Model failed to load.\");\r\n  }\r\n\r\n  // Double check worker reference after await\r\n  const worker = clientWorker!;\r\n\r\n  console.log(`[Provider] Streaming Chat: Prompting ${currentChatModelName} (${prompt.length} chars)...`);\r\n\r\n  return new Promise((resolve, reject) => {\r\n    let fullResponse = \"\";\r\n\r\n    const handler = (msg: any) => {\r\n      if (msg.type === 'token') {\r\n        if (onToken) onToken(msg.token);\r\n        fullResponse += msg.token;\r\n      } else if (msg.type === 'chatResponse') {\r\n        worker.off('message', handler);\r\n        console.log(`[Provider] Chat Complete (${fullResponse.length} chars)`);\r\n        resolve(msg.data || fullResponse);\r\n      } else if (msg.type === 'error') {\r\n        worker.off('message', handler);\r\n        console.error(\"Chat Error:\", msg.error);\r\n        reject(new Error(msg.error));\r\n      }\r\n    };\r\n\r\n    worker.on('message', handler);\r\n    worker.postMessage({\r\n      type: 'chat',\r\n      data: { prompt, options: { ...options, systemPrompt: systemInstruction } }\r\n    });\r\n  });\r\n}\r\n\r\nexport async function runSideChannel(prompt: string, systemInstruction = \"You are a helpful assistant.\", options: any = {}) {\r\n  // Use Orchestrator Worker if available, falling back to client\r\n  let targetWorker = orchestratorWorker || clientWorker;\r\n  let targetModel = currentOrchestratorModelName || currentChatModelName;\r\n\r\n  if (!targetWorker || !targetModel) {\r\n    await initAutoLoad();\r\n    targetWorker = orchestratorWorker || clientWorker;\r\n    targetModel = currentOrchestratorModelName || currentChatModelName;\r\n  }\r\n\r\n  if (!targetWorker || !targetModel) throw new Error(\"Orchestrator/Chat Model failed to load.\");\r\n\r\n  console.log(`[Provider] SideChannel: Prompting ${targetModel} (${prompt.length} chars)...`);\r\n\r\n  return new Promise((resolve, _reject) => {\r\n    const handler = (msg: any) => {\r\n      if (msg.type === 'chatResponse') {\r\n        targetWorker?.off('message', handler);\r\n        console.log(`[Provider] SideChannel: Response received (${msg.data?.length || 0} chars)`);\r\n        resolve(msg.data);\r\n      } else if (msg.type === 'error') {\r\n        targetWorker?.off('message', handler);\r\n        console.error(\"SideChannel Error:\", msg.error);\r\n        resolve(null);\r\n      }\r\n    };\r\n    targetWorker?.on('message', handler);\r\n    targetWorker?.postMessage({\r\n      type: 'chat',\r\n      data: { prompt, options: { ...options, systemPrompt: systemInstruction } }\r\n    });\r\n  });\r\n}\r\n\r\n// Embeddings - STUBBED (Tech Debt Removal)\r\nexport async function getEmbedding(text: string): Promise<number[] | null> {\r\n  const result = await getEmbeddings([text]);\r\n  return result ? result[0] : null;\r\n}\r\n\r\nexport async function getEmbeddings(texts: string[]): Promise<number[][] | null> {\r\n  // Return stubbed zero-vectors to satisfy DB schema\r\n  const dim = config.MODELS.EMBEDDING_DIM || 768; // Fallback to 768\r\n  return texts.map(() => new Array(dim).fill(0.1));\r\n}\r\n\r\n// Stub for now to match interface compatibility with rest of system\r\nexport async function initInference() {\r\n  // This is called by context.ts usually to ensure model loaded\r\n  // ANTI-GRAVITY PATCH: disable this legacy auto-load which picks random models without config\r\n  console.warn(\"[Provider] initInference called (Legacy). BLOCKED to prevent random model loading.\");\r\n  /* \r\n  const fs = await import('fs');\r\n  if (!fs.existsSync(MODELS_DIR)) return null;\r\n  try {\r\n    const models = fs.readdirSync(MODELS_DIR).filter((f: string) => f.endsWith(\".gguf\"));\r\n    if (models.length > 0) {\r\n      return await loadModel(models[0]);\r\n    }\r\n  } catch (e) { console.error(\"Error listing models\", e); }\r\n  */\r\n  return null;\r\n  return null;\r\n}\r\n\r\nexport function getSession() { return null; } // Worker handles session\r\nexport function getContext() { return null; }\r\nexport function getModel() { return null; }\r\nexport function getCurrentModelName() { return currentChatModelName; }\r\nexport function getCurrentCtxSize() { return config.MODELS.MAIN.CTX_SIZE; }\r\n\r\n// Legacy/Unused exports needed to satisfy imports elsewhere until refactored\r\nexport const DEFAULT_GPU_LAYERS = config.MODELS.MAIN.GPU_LAYERS;\r\nexport async function listModels(customDir?: string) {\r\n  const fs = await import('fs');\r\n  const targetDir = customDir ? path.resolve(customDir) : MODELS_DIR;\r\n  if (!fs.existsSync(targetDir)) return [];\r\n  return fs.readdirSync(targetDir).filter((f: string) => f.endsWith(\".gguf\"));\r\n}"
    tokens: 4011
    size: 11442
  - path: engine\src\services\llm\reader.ts
    content: "\r\nimport { runSideChannel } from './provider.js';\r\n\r\ninterface ContextItem {\r\n    content: string;\r\n    source: string;\r\n    timestamp?: number;\r\n}\r\n\r\n/**\r\n * Summarizes the retrieved search results in relation to the user's query.\r\n * Uses the secondary 'Orchestrator' model to keep the main chat context clean.\r\n */\r\nexport async function summarizeContext(\r\n    results: ContextItem[],\r\n    query: string\r\n): Promise<string> {\r\n    if (!results || results.length === 0) return \"\";\r\n\r\n    // 1. Prepare the Retrieval Document\r\n    const docs = results.map(r => {\r\n        const time = r.timestamp ? `(${new Date(r.timestamp).toISOString()})` : '';\r\n        return `[Source: ${r.source} ${time}]\\n${r.content}`;\r\n    }).join('\\n\\n---\\n\\n');\r\n\r\n    // 2. Prepare the Reader Prompt\r\n    const systemPrompt = `You are a Reader for the Sovereign Context Engine.\r\nYour goal is to read the provided Search Results and create a concise, factual summary that answers the User's Query.\r\nIf the results are irrelevant, state that clearly.\r\nFocus on extracting dates, decisions, and key patterns.\r\nOutput ONLY the summary. Do not chat.`;\r\n\r\n    const userPrompt = `USER QUERY: \"${query}\"\r\n\r\nSEARCH RESULTS:\r\n${docs}\r\n\r\nSUMMARY:`;\r\n\r\n    try {\r\n        console.log(`[Reader] Summarizing ${results.length} items for query: \"${query}\"`);\r\n        const summary = await runSideChannel(userPrompt, systemPrompt, {\r\n            temperature: 0.3, // Fact-focused\r\n            maxTokens: 512\r\n        }) as string;\r\n        console.log(`[Reader] Summary generated (${summary.length} chars).`);\r\n        return summary.trim();\r\n    } catch (e) {\r\n        console.error(`[Reader] Failed to summarize:`, e);\r\n        return `Error summarizing results: ${(e as any).message}`; // Fallback\r\n    }\r\n}\r\n"
    tokens: 654
    size: 1777
  - path: engine\src\services\mirror\mirror.ts
    content: "/**\r\n * Mirror Protocol Service - \"Tangible Knowledge Graph\"\r\n *\r\n * Projects the AI Brain onto the filesystem using a @bucket/#tag structure.\r\n */\r\n\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as crypto from 'crypto';\r\nimport { db } from '../../core/db.js';\r\nimport { NOTEBOOK_DIR } from '../../config/paths.js';\r\n\r\nexport const MIRRORED_BRAIN_PATH = path.join(NOTEBOOK_DIR, 'mirrored_brain');\r\n\r\n// Clean filename helper\r\nfunction sanitizeFilename(text: string): string {\r\n    return text.replace(/[^a-zA-Z0-9-_]/g, '_').substring(0, 64);\r\n}\r\n\r\nconst ATOMS_PER_BUNDLE = 100;\r\n\r\n/**\r\n * Mirror Protocol: Exports memories to Markdown files organized by @bucket/#tag/#nested hierarchy\r\n */\r\nexport async function createMirror() {\r\n    console.log(' Mirror Protocol: Starting semantic brain mirroring (Recursive Tree)...');\r\n\r\n    // Wipe existing mirrored brain to ensure only latest state is present\r\n    if (fs.existsSync(MIRRORED_BRAIN_PATH)) {\r\n        console.log(` Mirror Protocol: Wiping stale mirror at ${MIRRORED_BRAIN_PATH}`);\r\n        fs.rmSync(MIRRORED_BRAIN_PATH, { recursive: true, force: true });\r\n    }\r\n\r\n    fs.mkdirSync(MIRRORED_BRAIN_PATH, { recursive: true });\r\n\r\n    // Fetch atoms with sequence and provenance for proper bundling and re-hydration\r\n    const query = '?[id, timestamp, content, source, type, hash, buckets, tags, sequence, provenance] := *memory{id, timestamp, content, source, type, hash, buckets, tags, sequence, provenance}';\r\n    const result = await db.run(query);\r\n\r\n    if (!result.rows || result.rows.length === 0) {\r\n        console.log(' Mirror Protocol: No memories to mirror.');\r\n        return;\r\n    }\r\n\r\n    console.log(` Mirror Protocol: Processing ${result.rows.length} memories for hierarchical bundling...`);\r\n\r\n    // Grouping structure: Map<FullPathString, Map<SourcePath, Atom[]>>\r\n    // We group by Target Directory Path -> Then by Source File (Original Provenance)\r\n    const directoryGroups = new Map<string, Map<string, any[]>>();\r\n\r\n    for (const row of result.rows) {\r\n        const [id, timestamp, content, source, type, hash, buckets, tags, sequence, provenance] = row;\r\n\r\n        const bucketList = (buckets as string[]) || [];\r\n        const tagList = (tags as string[]) || [];\r\n        const primaryBucket = bucketList.length > 0 ? bucketList[0] : 'general';\r\n\r\n        // 1. Determine Root Bucket\r\n        const bucketName = (primaryBucket && primaryBucket !== 'general' && primaryBucket !== 'unknown') ? primaryBucket : 'general';\r\n\r\n        // 2. Determine Tag Path\r\n        // Filter out the bucket name itself and 'inbox' to avoid redundancy\r\n        const specificTags = tagList.filter((t: string) => t !== bucketName && t !== 'inbox');\r\n\r\n        // Sort tags alphabetically to ensure deterministic nesting order\r\n        // e.g. [\"#z\", \"#a\"] -> \"#a/#z\" path\r\n        specificTags.sort();\r\n\r\n        // Construct Path segments\r\n        // starting with @bucket\r\n        const pathSegments = [`@${sanitizeFilename(bucketName)}`];\r\n\r\n        // Append tag segments\r\n        if (specificTags.length > 0) {\r\n            specificTags.forEach(t => pathSegments.push(`#${sanitizeFilename(t)}`));\r\n        } else {\r\n            // Check if we should use \"_untagged\" or just root?\r\n            // Existing logic used \"_untagged\". Let's stick to that for cleanliness.\r\n            pathSegments.push('#_untagged');\r\n        }\r\n\r\n        const relativePath = path.join(...pathSegments);\r\n\r\n        // 3. Add to Group\r\n        if (!directoryGroups.has(relativePath)) directoryGroups.set(relativePath, new Map());\r\n        const sourceMap = directoryGroups.get(relativePath)!;\r\n\r\n        const sourcePath = (source as string) || 'unknown';\r\n        if (!sourceMap.has(sourcePath)) sourceMap.set(sourcePath, []);\r\n\r\n        sourceMap.get(sourcePath)!.push({\r\n            id, timestamp, content, source: sourcePath, type, hash, buckets: bucketList, tags: tagList, sequence: sequence || 0, provenance\r\n        });\r\n    }\r\n\r\n    let bundleCount = 0;\r\n    let totalAtoms = 0;\r\n\r\n    // Write bundles recursively\r\n    for (const [relPath, sourceMap] of directoryGroups) {\r\n        const fullDir = path.join(MIRRORED_BRAIN_PATH, relPath);\r\n\r\n        if (!fs.existsSync(fullDir)) fs.mkdirSync(fullDir, { recursive: true });\r\n\r\n        for (const [sourcePath, atomList] of sourceMap) {\r\n            // Sort by sequence or timestamp\r\n            atomList.sort((a, b) => (a.sequence - b.sequence) || (a.timestamp - b.timestamp));\r\n\r\n            // Chunk into bundles\r\n            for (let i = 0; i < atomList.length; i += ATOMS_PER_BUNDLE) {\r\n                const chunk = atomList.slice(i, i + ATOMS_PER_BUNDLE);\r\n                const partNum = Math.floor(i / ATOMS_PER_BUNDLE) + 1;\r\n                const isMultiPart = atomList.length > ATOMS_PER_BUNDLE;\r\n\r\n                // We pass the \"bucketName\" just for the Archive label in case of orphan\r\n                // We extract it from the path (first segment)\r\n                const bucketLabel = relPath.split(path.sep)[0].replace('@', '');\r\n\r\n                await writeBundleFile(fullDir, sourcePath, chunk, partNum, isMultiPart, bucketLabel);\r\n                bundleCount++;\r\n                totalAtoms += chunk.length;\r\n            }\r\n        }\r\n    }\r\n\r\n    console.log(` Mirror Protocol: Synchronization complete. ${totalAtoms} memories mirrored across ${bundleCount} bundles in ${MIRRORED_BRAIN_PATH}`);\r\n}\r\n\r\nasync function writeBundleFile(tagDir: string, sourcePath: string, atoms: any[], partNum: number, isMultiPart: boolean, bucketName: string) {\r\n    try {\r\n        let isOrphan = sourcePath === 'unknown' || !sourcePath;\r\n        let sourceBase = isOrphan ? `daily_archive_${new Date().toISOString().split('T')[0]}` : path.basename(sourcePath);\r\n\r\n        // Add hash of full path to prevent collisions for same basename in different dirs\r\n        const pathHash = crypto.createHash('md5').update(sourcePath || 'orphan').digest('hex').substring(0, 8);\r\n        const safeName = sanitizeFilename(sourceBase).toLowerCase();\r\n\r\n        let fileName = `${safeName}_${pathHash}`;\r\n        if (isMultiPart) fileName += `_part${partNum}`;\r\n        fileName += '.md';\r\n\r\n        const filePath = path.join(tagDir, fileName);\r\n\r\n        if (!fs.existsSync(tagDir)) {\r\n            fs.mkdirSync(tagDir, { recursive: true });\r\n        }\r\n\r\n        // Build content (Standard 066)\r\n        let content = `# Source: ${isOrphan ? 'Archive (' + bucketName + ')' : sourcePath}\\n`;\r\n        if (isMultiPart) content += `> Part: ${partNum}\\n`;\r\n        content += `\\n---\\n\\n`;\r\n\r\n        for (const atom of atoms) {\r\n            let nameSnippet = \"atom\";\r\n            const titleMatch = atom.content.match(/^#\\s+(.+)$/m);\r\n            if (titleMatch) {\r\n                nameSnippet = titleMatch[1];\r\n            } else {\r\n                nameSnippet = atom.content.substring(0, 50).trim().split('\\n')[0];\r\n            }\r\n\r\n            const shortId = (atom.id || \"\").split('_').pop() || \"anon\";\r\n\r\n            content += `## [${shortId}] ${nameSnippet}\\n`;\r\n            // Metadata header as per POML\r\n            content += `> **Provenance**: ${atom.provenance || 'unknown'} | **Date**: ${new Date(atom.timestamp).toISOString()}\\n`;\r\n            if (atom.tags.length > 0) content += `> **Tags**: ${atom.tags.join(', ')}\\n`;\r\n            content += `\\n${atom.content}\\n\\n`;\r\n            content += `---`; // Horizontal rule separation\r\n            content += `\\n\\n`;\r\n        }\r\n\r\n        await fs.promises.writeFile(filePath, content, 'utf8');\r\n        return true;\r\n    } catch (e: any) {\r\n        console.error(`Failed to write bundle file in ${tagDir}:`, e.message);\r\n        return false;\r\n    }\r\n}\r\n"
    tokens: 2728
    size: 7752
  - path: engine\src\services\research\researcher.ts
    content: "\r\nimport { gotScraping } from 'got-scraping';\r\nimport * as cheerio from 'cheerio';\r\nimport TurndownService from 'turndown';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as crypto from 'crypto';\r\nimport { NOTEBOOK_DIR } from '../../config/paths.js';\r\n\r\nconst PLUGINS_DIR = path.join(NOTEBOOK_DIR, 'plugins');\r\n\r\n// Ensure staging directories exist\r\nconst ARTICLES_DIR = path.join(PLUGINS_DIR, 'articles');\r\nconst PAPERS_DIR = path.join(PLUGINS_DIR, 'research-papers');\r\n\r\n// Robust Headers\r\n\r\n\r\nif (!fs.existsSync(ARTICLES_DIR)) fs.mkdirSync(ARTICLES_DIR, { recursive: true });\r\nif (!fs.existsSync(PAPERS_DIR)) fs.mkdirSync(PAPERS_DIR, { recursive: true });\r\n\r\nconst turndownService = new TurndownService({\r\n    headingStyle: 'atx',\r\n    codeBlockStyle: 'fenced'\r\n});\r\n\r\n// Remove script tags, styles, etc.\r\nturndownService.remove(['script', 'style', 'noscript', 'iframe', 'nav', 'footer', 'header']);\r\n\r\ninterface ResearchResult {\r\n    success: boolean;\r\n    filePath?: string;\r\n    title?: string;\r\n    error?: string;\r\n}\r\n\r\nexport async function fetchAndProcess(url: string, category: 'article' | 'paper' = 'article'): Promise<ResearchResult> {\r\n    try {\r\n        console.log(`[Research] Fetching: ${url}`);\r\n\r\n        const response = await gotScraping(url, { timeout: { request: 10000 } });\r\n\r\n        const html = response.body;\r\n        const $ = cheerio.load(html);\r\n\r\n        // Extract Metadata\r\n        const title = $('title').text().trim() || 'Untitled Page';\r\n        const metaDesc = $('meta[name=\"description\"]').attr('content') || '';\r\n\r\n        // Cleanup DOM\r\n        $('script').remove();\r\n        $('style').remove();\r\n        $('nav').remove();\r\n        $('header').remove();\r\n        $('footer').remove();\r\n        $('.ad').remove();\r\n        $('.advertisement').remove();\r\n        $('.sidebar').remove();\r\n\r\n        // Target main content if possible\r\n        let contentHtml = $('main').html() || $('article').html() || $('body').html() || '';\r\n\r\n        // Convert\r\n        const markdown = turndownService.turndown(contentHtml);\r\n\r\n        // Frontmatter\r\n        const fileContent = `# ${title}\r\n> **Source**: ${url}\r\n> **Date**: ${new Date().toISOString()}\r\n> **Description**: ${metaDesc}\r\n\r\n---\r\n\r\n${markdown}\r\n`;\r\n\r\n        // Filename\r\n        const safeTitle = title.replace(/[^a-zA-Z0-9-_]/g, '_').substring(0, 50);\r\n        const hash = crypto.createHash('md5').update(url).digest('hex').substring(0, 8);\r\n        const filename = `${safeTitle}_${hash}.md`;\r\n\r\n        // Save\r\n        const targetDir = category === 'paper' ? PAPERS_DIR : ARTICLES_DIR;\r\n        const filePath = path.join(targetDir, filename);\r\n\r\n        await fs.promises.writeFile(filePath, fileContent, 'utf8');\r\n        console.log(`[Research] Saved to: ${filePath}`);\r\n\r\n        return { success: true, filePath, title };\r\n\r\n    } catch (e: any) {\r\n        console.error(`[Research] Failed: ${e.message}`);\r\n        return { success: false, error: e.message };\r\n    }\r\n}\r\n\r\nexport interface WebSearchResult {\r\n    title: string;\r\n    link: string;\r\n    snippet: string;\r\n}\r\n\r\nexport async function searchWeb(query: string): Promise<WebSearchResult[]> {\r\n    try {\r\n        console.log(`[Research] Searching Web: \"${query}\"`);\r\n        const searchUrl = `https://html.duckduckgo.com/html/?q=${encodeURIComponent(query)}`;\r\n\r\n        const response = await gotScraping(searchUrl);\r\n\r\n        const $ = cheerio.load(response.body);\r\n        const results: WebSearchResult[] = [];\r\n\r\n        $('.result').each((_i, element) => {\r\n            const titleElement = $(element).find('.result__a');\r\n            const snippetElement = $(element).find('.result__snippet');\r\n\r\n            const title = titleElement.text().trim();\r\n            const link = titleElement.attr('href');\r\n            const snippet = snippetElement.text().trim();\r\n\r\n            if (title && link && !link.includes('duckduckgo.com/y.js')) {\r\n                // DuckDuckGo usually has internal redirects, we might get the raw link or the redirect.\r\n                // Ideally we decode it if it's a diffbot or similar proxy, but usually for HTML version it's direct or simple.\r\n                // Actually DDG HTML links are typically relative redirects `/l/?uddg=...`\r\n                // We should try to extract the real URL if possible, or use the redirect.\r\n                // Let's decode the uddg param if present.\r\n                let realLink = link;\r\n                try {\r\n                    if (link.includes('uddg=')) {\r\n                        const match = link.match(/uddg=([^&]+)/);\r\n                        if (match && match[1]) {\r\n                            realLink = decodeURIComponent(match[1]);\r\n                        }\r\n                    }\r\n                } catch (e) { /* ignore */ }\r\n\r\n                results.push({ title, link: realLink, snippet });\r\n            }\r\n        });\r\n\r\n        console.log(`[Research] Found ${results.length} results.`);\r\n        return results.slice(0, 10); // Top 10\r\n\r\n    } catch (e: any) {\r\n        console.error(`[Research] Search failed: ${e.message}`);\r\n        return [];\r\n    }\r\n}\r\n"
    tokens: 1786
    size: 5153
  - path: engine\src\services\safe-shell-executor\safe-shell-executor.js
    content: "// safe-shell-executor.js\r\nconst { spawn } = require('child_process');\r\nconst path = require('path');\r\nconst { LOGS_DIR } = require('../../config/paths');\r\n\r\nclass SafeShellExecutor {\r\n    static async execute(command, options = {}) {\r\n        return new Promise((resolve, reject) => {\r\n            const {\r\n                timeout = 30000, // 30 second default timeout\r\n                logFile = path.join(LOGS_DIR, `shell_cmd_${Date.now()}.log`),\r\n                detached = true,\r\n                stdio = ['ignore', 'ignore', 'ignore'] // Completely detached\r\n            } = options;\r\n\r\n            // Split command into command and args\r\n            const [cmd, ...args] = command.split(' ');\r\n\r\n            const child = spawn(cmd, args, {\r\n                detached,\r\n                stdio,\r\n                ...options.spawnOptions\r\n            });\r\n\r\n            // Set up timeout\r\n            const timer = setTimeout(() => {\r\n                child.kill();\r\n                reject(new Error(`Command timed out after ${timeout}ms: ${command}`));\r\n            }, timeout);\r\n\r\n            // Handle process completion\r\n            child.on('close', (code) => {\r\n                clearTimeout(timer);\r\n                resolve({\r\n                    success: code === 0,\r\n                    code,\r\n                    logFile\r\n                });\r\n            });\r\n\r\n            child.on('error', (error) => {\r\n                clearTimeout(timer);\r\n                reject(error);\r\n            });\r\n\r\n            // If detached, unref to not keep Node.js process alive\r\n            if (detached) {\r\n                child.unref();\r\n            }\r\n        });\r\n    }\r\n}\r\n\r\nmodule.exports = SafeShellExecutor;\r\n"
    tokens: 560
    size: 1710
  - path: engine\src\services\scribe\scribe.ts
    content: "/**\r\n * Scribe Service - Markovian Rolling Context\r\n *\r\n * Maintains a \"Session State\" that summarizes the current conversation.\r\n * This enables the model to maintain coherence across long conversations\r\n * without requiring the full history in context.\r\n */\r\n\r\nimport { db } from '../../core/db.js';\r\n\r\n// Lazy-load inference to avoid circular dependency\r\nlet inferenceModule: any = null;\r\nfunction getInference() {\r\n    if (!inferenceModule) {\r\n        inferenceModule = require('../inference/inference');\r\n    }\r\n    return inferenceModule;\r\n}\r\n\r\nconst SESSION_STATE_ID = 'session_state';\r\nconst STATE_BUCKET = ['system', 'state'];\r\n\r\ninterface HistoryItem {\r\n    role: string;\r\n    content: string;\r\n}\r\n\r\ninterface UpdateStateResult {\r\n    status: string;\r\n    summary?: string;\r\n    message?: string;\r\n}\r\n\r\ninterface ClearStateResult {\r\n    status: string;\r\n    message?: string;\r\n}\r\n\r\n/**\r\n * Updates the rolling session state based on recent conversation history.\r\n * Uses the LLM to compress recent turns into a coherent state summary.\r\n *\r\n * @param {HistoryItem[]} history - Array of {role, content} message objects\r\n * @returns {Promise<UpdateStateResult>} - {status, summary} or {status, error}\r\n */\r\nexport async function updateState(history: HistoryItem[]): Promise<UpdateStateResult> {\r\n    console.log(' Scribe: Analyzing conversation state...');\r\n\r\n    try {\r\n        // 1. Flatten last 10 turns into readable text\r\n        const recentTurns = history.slice(-10);\r\n        const recentText = recentTurns\r\n            .map(m => `${m.role.toUpperCase()}: ${m.content}`)\r\n            .join('\\n\\n');\r\n\r\n        if (!recentText.trim()) {\r\n            return { status: 'skipped', message: 'No conversation history to analyze' };\r\n        }\r\n\r\n        // 2. Construct the state extraction prompt\r\n        const prompt = `Analyze this conversation segment and produce a concise \"Session State\" summary.\r\n\r\nKeep it under 200 words. Focus on:\r\n- Current Goal: What is the user trying to accomplish?\r\n- Key Decisions: What has been decided or agreed upon?\r\n- Active Tasks: What work is in progress or pending?\r\n- Important Context: What background information is critical to remember?\r\n\r\nConversation:\r\n${recentText}\r\n\r\n---\r\nSession State Summary:`;\r\n\r\n        // 3. Generate the state summary\r\n        const inf = getInference();\r\n        const summary = await inf.rawCompletion(prompt);\r\n\r\n        if (!summary || summary.trim().length < 10) {\r\n            return { status: 'error', message: 'Failed to generate meaningful state' };\r\n        }\r\n\r\n        // 4. Persist to database with special ID\r\n        const timestamp = Date.now();\r\n        const query = `?[id, timestamp, content, source, type, hash, buckets, tags] <- $data :put memory {id, timestamp, content, source, type, hash, buckets, tags}`;\r\n\r\n        await db.run(query, {\r\n            data: [[\r\n                SESSION_STATE_ID,\r\n                timestamp,\r\n                summary.trim(),\r\n                'Scribe',\r\n                'state',\r\n                `state_${timestamp}`,\r\n                STATE_BUCKET,\r\n                '[]'  // tags as JSON string\r\n            ]]\r\n        });\r\n\r\n        console.log(' Scribe: State updated successfully');\r\n        return { status: 'updated', summary: summary.trim() };\r\n\r\n    } catch (e: any) {\r\n        console.error(' Scribe Error:', e.message);\r\n        return { status: 'error', message: e.message };\r\n    }\r\n}\r\n\r\n/**\r\n * Retrieves the current session state from the database.\r\n *\r\n * @returns {Promise<string | null>} - The state summary or null if not found\r\n */\r\nexport async function getState(): Promise<string | null> {\r\n    try {\r\n        const query = '?[content] := *memory{id: mem_id, content}, mem_id == $id';\r\n        const res = await db.run(query, { id: SESSION_STATE_ID });\r\n\r\n        if (res.rows && res.rows.length > 0) {\r\n            return res.rows[0][0] as string;\r\n        }\r\n        return null;\r\n    } catch (e: any) {\r\n        console.error(' Scribe: Failed to retrieve state:', e.message);\r\n        return null;\r\n    }\r\n}\r\n\r\n/**\r\n * Clears the current session state.\r\n * Useful for starting a fresh conversation.\r\n *\r\n * @returns {Promise<ClearStateResult>} - {status}\r\n */\r\nexport async function clearState(): Promise<ClearStateResult> {\r\n    try {\r\n        const query = `?[id] <- [[$id]] :delete memory {id}`;\r\n        await db.run(query, { id: SESSION_STATE_ID });\r\n        console.log(' Scribe: State cleared');\r\n        return { status: 'cleared' };\r\n    } catch (e: any) {\r\n        console.error(' Scribe: Failed to clear state:', e.message);\r\n        return { status: 'error', message: e.message };\r\n    }\r\n}"
    tokens: 1655
    size: 4700
  - path: engine\src\services\search\search.ts
    content: "/**\r\n * Search Service with Engram Layer and Provenance Boosting\r\n *\r\n * Implements:\r\n * 1. Engram Layer (Fast Lookup) - O(1) lookup for known entities\r\n * 2. Provenance Boosting - Sovereign content gets boost\r\n * 3. Tag-Walker Protocol - Graph-based associative retrieval (Replacing Vector Search)\r\n * 4. Intelligent Query Expansion - GLM-assisted decomposition (Standard 069)\r\n */\r\n\r\nimport { db } from '../../core/db.js';\r\nimport { createHash } from 'crypto';\r\nimport { composeRollingContext } from '../../core/inference/context_manager.js';\r\nimport wink from 'wink-nlp';\r\nimport model from 'wink-eng-lite-web-model';\r\n\r\n// Initialize NLP (Fast CPU-based)\r\nconst nlp = wink(model);\r\n\r\ninterface SearchResult {\r\n  id: string;\r\n  content: string;\r\n  source: string;\r\n  timestamp: number;\r\n  buckets: string[];\r\n  tags: string[];\r\n  epochs: string;\r\n  provenance: string;\r\n  score: number;\r\n}\r\n\r\n/**\r\n * Fetch top tags from the system to ground the LLM's query expansion\r\n */\r\nexport async function getGlobalTags(limit: number = 50): Promise<string[]> {\r\n  try {\r\n    // CozoDB aggregation syntax is restrictive in this environment.\r\n    // We fetch unique tags and rely on the list for grounding.\r\n    const query = `\r\n            ?[tag] := *memory{tags}, tag in tags :limit 500\r\n        `;\r\n    const result = await db.run(query);\r\n    if (!result.rows) return [];\r\n\r\n    const uniqueTags = [...new Set((result.rows as string[][]).map((r: string[]) => r[0]))];\r\n    return uniqueTags.slice(0, limit) as string[];\r\n  } catch (e) {\r\n    console.error('[Search] Failed to fetch global tags:', e);\r\n    return [];\r\n  }\r\n}\r\n\r\n/**\r\n * Use LLM to expand query into semantically similar system tags\r\n */\r\nimport { getMasterTags } from '../tags/discovery.js';\r\n\r\n/**\r\n * Deterministic Query Expansion (No LLM)\r\n * Scans the user query for known tags from the master list.\r\n */\r\nexport async function expandQuery(originalQuery: string): Promise<string[]> {\r\n  try {\r\n    const globalTags = getMasterTags(); // This is synchronous file read\r\n    const queryLower = originalQuery.toLowerCase();\r\n\r\n    // Find tags specifically mentioned in the query or that substring match\r\n    // Simple heuristic: if query contains the tag, we boost it.\r\n    const foundTags = globalTags.filter(tag => {\r\n      const tagLower = tag.toLowerCase();\r\n      // Check for boundary matches or direct inclusion\r\n      return queryLower.includes(tagLower);\r\n    });\r\n\r\n    if (foundTags.length > 0) {\r\n      console.log(`[Search] Deterministically matched tags: ${foundTags.join(', ')}`);\r\n    }\r\n    return foundTags;\r\n  } catch (e) {\r\n    console.error('[Search] Expansion failed:', e);\r\n    return [];\r\n  }\r\n}\r\n\r\n/**\r\n * Helper to sanitize queries for CozoDB FTS engine\r\n */\r\nfunction sanitizeFtsQuery(query: string): string {\r\n  return query\r\n    .replace(/[^a-zA-Z0-9\\s]/g, ' ')\r\n    .replace(/\\s+/g, ' ')\r\n    .trim()\r\n    .toLowerCase();\r\n}\r\n\r\n/**\r\n * Natural Language Parser (Standard 070)\r\n * Uses NLP to extract \"Meaningful Tags\" (Nouns, Proper Nouns, Important Verbs).\r\n * This prevents common words (\"lately\", \"been\", \"working\") from killing FTS recall.\r\n */\r\n/**\r\n * Helper: Extract Temporal Context\r\n * Detects \"last X months/years\" and returns a list of relevant year tags.\r\n */\r\nfunction extractTemporalContext(query: string): string[] {\r\n  const now = new Date();\r\n  const currentYear = now.getFullYear();\r\n  const tags: Set<string> = new Set();\r\n\r\n  // Regex for \"last X months/years\"\r\n  const match = query.match(/last\\s+(\\d+)\\s+(months?|years?|days?)/i);\r\n  if (match) {\r\n    const amount = parseInt(match[1]);\r\n    const unit = match[2].toLowerCase();\r\n\r\n    tags.add(currentYear.toString()); // Always include current year\r\n\r\n    if (unit.startsWith('year')) {\r\n      for (let i = 1; i <= amount; i++) {\r\n        tags.add((currentYear - i).toString());\r\n      }\r\n    } else if (unit.startsWith('month')) {\r\n      // If subtracting months goes back to prev year\r\n      const pastDate = new Date(now);\r\n      pastDate.setMonth(now.getMonth() - amount);\r\n      const pastYear = pastDate.getFullYear();\r\n      if (pastYear < currentYear) {\r\n        for (let y = pastYear; y < currentYear; y++) tags.add(y.toString());\r\n      }\r\n    }\r\n  }\r\n\r\n  // Also detect explicit years (2020-2030)\r\n  const yearMatch = query.match(/\\b(202[0-9])\\b/g);\r\n  if (yearMatch) {\r\n    yearMatch.forEach(y => tags.add(y));\r\n  }\r\n\r\n  return Array.from(tags);\r\n}\r\n\r\n/**\r\n * Natural Language Parser (Standard 070 - Enhanced)\r\n * Uses NLP to extract \"Meaningful Tags\" including Temporal Context.\r\n */\r\nfunction parseNaturalLanguage(query: string): string {\r\n  // 1. Extract Temporal Context\r\n  const timeTags = extractTemporalContext(query);\r\n\r\n  // 2. NLP Processing\r\n  const doc = nlp.readDoc(query);\r\n\r\n  // Extract Nouns, PropNouns, Adjectives, AND \"Domain Verbs\"\r\n  // We want to keep words like \"burnout\" (Noun), \"started\" (Verb), \"career\" (Noun)\r\n  // By default, just keeping NOUN/PROPN/ADJ is usually safe, but let's be slightly more permissive\r\n  // or rely on the query expansion to catch synonyms.\r\n  // Actually, \"burnout\" is a Noun. \"Career\" is a Noun. \r\n  // \"Decisions\" is a Noun.\r\n  // The issue might have been valid stopwords or tokenization.\r\n\r\n  const tokens = doc.tokens().filter((t: any) => {\r\n    const tag = t.out(nlp.its.pos);\r\n    const text = t.out().toLowerCase();\r\n\r\n    // Whitelist specific domain words that might get misclassified or filtered\r\n    if (['burnout', 'career', 'decision', 'pattern', 'impact'].some(w => text.includes(w))) return true;\r\n\r\n    return tag === 'NOUN' || tag === 'PROPN' || tag === 'ADJ' || tag === 'VERB';\r\n  }).out((nlp as any).its.text);\r\n\r\n  // Combine\r\n  const uniqueTokens = new Set([...tokens, ...timeTags]);\r\n\r\n  if (uniqueTokens.size > 0) {\r\n    return Array.from(uniqueTokens).join(' ').toLowerCase();\r\n  }\r\n\r\n  return sanitizeFtsQuery(query);\r\n}\r\n\r\n/**\r\n * Create or update an engram (lexical sidecar) for fast entity lookup\r\n */\r\nexport async function createEngram(key: string, memoryIds: string[]): Promise<void> {\r\n  const normalizedKey = key.toLowerCase().trim();\r\n  const engramId = createHash('md5').update(normalizedKey).digest('hex');\r\n\r\n  const insertQuery = `?[key, value] <- $data :put engrams {key, value}`;\r\n  await db.run(insertQuery, {\r\n    data: [[engramId, JSON.stringify(memoryIds)]]\r\n  });\r\n}\r\n\r\n/**\r\n * Lookup memories by engram key (O(1) operation)\r\n */\r\nexport async function lookupByEngram(key: string): Promise<string[]> {\r\n  const normalizedKey = key.toLowerCase().trim();\r\n  const engramId = createHash('md5').update(normalizedKey).digest('hex');\r\n\r\n  const query = `?[value] := *engrams{key, value}, key = $engramId`;\r\n  const result = await db.run(query, { engramId });\r\n\r\n  if (result.rows && result.rows.length > 0) {\r\n    return JSON.parse(result.rows[0][0] as string);\r\n  }\r\n\r\n  return [];\r\n}\r\n\r\n/**\r\n * Tag-Walker Associative Search (Replaces Vector Search)\r\n */\r\nexport async function tagWalkerSearch(\r\n  query: string,\r\n  buckets: string[] = [],\r\n  tags: string[] = [],\r\n  _maxChars: number = 524288\r\n): Promise<SearchResult[]> {\r\n  try {\r\n    const sanitizedQuery = sanitizeFtsQuery(query);\r\n    if (!sanitizedQuery) return [];\r\n\r\n    // 1. Direct Search (The Anchor)\r\n    const anchorQuery = `\r\n            ?[id, content, source, timestamp, buckets, tags, epochs, provenance, score] := \r\n            ~memory:content_fts{id | query: $query, k: 50, bind_score: fts_score},\r\n            *memory{id, content, source, timestamp, buckets, tags, epochs, provenance},\r\n            provenance != 'quarantine',\r\n            score = 70.0 * fts_score\r\n            ${buckets.length > 0 ? ', length(intersection(buckets, $buckets)) > 0' : ''}\r\n            ${tags.length > 0 ? ', length(intersection(tags, $tags)) > 0' : ''}\r\n            :limit 20\r\n        `;\r\n\r\n    const anchorResult = await db.run(anchorQuery, { query: sanitizedQuery, buckets, tags });\r\n    if (!anchorResult.rows || anchorResult.rows.length === 0) return [];\r\n\r\n    // Map Anchors\r\n    const anchors = anchorResult.rows.map((row: any[]) => ({\r\n      id: row[0],\r\n      content: row[1],\r\n      source: row[2],\r\n      timestamp: row[3],\r\n      buckets: row[4],\r\n      tags: row[5],\r\n      epochs: row[6],\r\n      provenance: row[7],\r\n      score: row[8]\r\n    }));\r\n\r\n    // 2. The Walk (Associative Discovery)\r\n    const anchorIds = anchors.map((a: any) => a.id);\r\n\r\n    const walkQuery = `\r\n            ?[id, content, source, timestamp, buckets, tags, epochs, provenance, score] := \r\n            *memory{id: anchor_id, tags: anchor_tags},\r\n            anchor_id in $anchorIds,\r\n            tag in anchor_tags,\r\n            *memory{id, content, source, timestamp, buckets, tags, epochs, provenance},\r\n            tag in tags,\r\n            id != anchor_id,\r\n            provenance != 'quarantine',\r\n            ${tags.length > 0 ? 'length(intersection(tags, $tags)) > 0,' : ''} \r\n            score = 30.0\r\n            :limit 10\r\n        `;\r\n\r\n    const walkResult = await db.run(walkQuery, { anchorIds, tags });\r\n    const neighbors = (walkResult.rows || []).map((row: any[]) => ({\r\n      id: row[0],\r\n      content: row[1],\r\n      source: row[2],\r\n      timestamp: row[3],\r\n      buckets: row[4],\r\n      tags: row[5],\r\n      epochs: row[6],\r\n      provenance: row[7],\r\n      score: row[8]\r\n    }));\r\n\r\n    return [...anchors, ...neighbors];\r\n\r\n  } catch (e) {\r\n    console.error('[Search] Tag-Walker failed:', e);\r\n    return [];\r\n  }\r\n}\r\n\r\n/**\r\n * Execute search with Intelligent Expansion and Tag-Walker Protocol\r\n */\r\nexport async function executeSearch(\r\n  query: string,\r\n  _bucket?: string,\r\n  buckets?: string[],\r\n  maxChars: number = 524288,\r\n  _deep: boolean = false,\r\n  provenance: 'sovereign' | 'external' | 'all' = 'all'\r\n): Promise<{ context: string; results: SearchResult[]; toAgentString: () => string; metadata?: any }> {\r\n  console.log(`[Search] executeSearch (Expanded Tag-Walker) called with provenance: ${provenance}`);\r\n\r\n  // 0. PRE-PROCESS: Extract Scope Tags (e.g., #code, #doc)\r\n  // We manually extract them because NLP strips them as noise.\r\n  const scopeTags: string[] = [];\r\n  const queryParts = query.split(/\\s+/);\r\n  const cleanQueryParts: string[] = [];\r\n\r\n  for (const part of queryParts) {\r\n    if (part.startsWith('#')) {\r\n      scopeTags.push(part); // Keep the hash!\r\n    } else {\r\n      cleanQueryParts.push(part);\r\n    }\r\n  }\r\n  const cleanQuery = cleanQueryParts.join(' ');\r\n\r\n  // Separate actual Buckets (folders) from Tags (hashtags)\r\n  const realBuckets = new Set(buckets || []);\r\n\r\n  // Fetch known buckets to resolve ambiguity\r\n  // We do a quick check on the global bucket list if possible, or just checking if it looks like a bucket?\r\n  // Since we can't easily fetch global buckets synchronously here without overhead, \r\n  // we will trust the provided 'buckets' arg primarily.\r\n  // BUT, to support \"#inbox\" in query meaning bucket \"inbox\", we can try a heuristic or just checking against common knowns?\r\n  // Or better: Treat it as both? \r\n  // No, the user wants strictness.\r\n\r\n  // Let's implement the \"Smart Split\":\r\n  // If #token is in the query, we treat it as a Tag by default from NLP perspective.\r\n  // BUT we will also add it to 'scopeTags'. \r\n  // Wait, if it's a bucket, we should strip the hash and add to 'realBuckets'.\r\n  // We can query the DB to see if such a bucket exists? That's expensive per search.\r\n\r\n  // Compromise: We will treat #tokens as TAGS (as per standard).\r\n  // AND we will extract explicit #bucket:name syntax if we wanted advanced features.\r\n  // BUT, reusing the 'parseQuery' logic which defines #token as key buckets is conflicting.\r\n  // The 'parseQuery' function (unused) says #word IS a bucket.\r\n\r\n  // Let's align with 'parseQuery' logic for hashtags-as-buckets? \r\n  // User Prompt: \"buckets ... narrowing down the folders\".\r\n  // If I type \"#work\", I usually mean \"Work context\". \r\n\r\n  // Let's do this: \r\n  // 1. We keep scopeTags as matches for TAG column.\r\n  // 2. We ALSO add the stripped tag to realBuckets if the user didn't provide explicit buckets.\r\n  // (Auto-detect context).\r\n\r\n  // If no buckets provided, try to infer from hashtags.\r\n  if (realBuckets.size === 0) {\r\n    scopeTags.forEach(tag => {\r\n      // Assume tags format is \"#name\"\r\n      const name = tag.replace('#', '');\r\n      // Heuristic: If it's a simple word, it might be a bucket.\r\n      realBuckets.add(name);\r\n      // We add it to buckets, but we ALSO keep it in tags?\r\n      // If we keep it in tags, it MUST match the tag column.\r\n      // If the user meant bucket, they likely didn't tag the file with \"#inbox\".\r\n      // So we should probably NOT enforce it as a tag if we treat it as a bucket?\r\n      // This is risky.\r\n    });\r\n  }\r\n\r\n  // Actually, simplest fix for the user's \"Verification\":\r\n  // Ensure that IF buckets are passed, we use them.\r\n  // IF tags are passed (#text), we use them.\r\n  // The user asked \"are they functioning\". \r\n  // My previous check confirmed \"intersection()\" logic.\r\n  // The only missing piece is: \"Does #word imply Bucket or Tag?\"\r\n\r\n  // I will log the resolved filters so the user can see.\r\n  // And I will ensure we don't double-filter incorrectly.\r\n\r\n  console.log(`[Search] Query: \"${cleanQuery}\"`);\r\n  console.log(`[Search] Filters -> Buckets: [${Array.from(realBuckets).join(', ')}] | Tags: [${scopeTags.join(', ')}]`);\r\n\r\n  // 0. NATURAL LANGUAGE PARSING (Standard 070)\r\n  // Strip stop words from the query for better FTS performance\r\n  // Use the CLEANED query (without tags)\r\n  const parsedQuery = parseNaturalLanguage(cleanQuery);\r\n  if (parsedQuery !== cleanQuery) {\r\n    console.log(`[Search] NLP Parsed Query: \"${cleanQuery}\" -> \"${parsedQuery}\"`);\r\n  }\r\n\r\n  // 0.5. QUERY EXPANSION (Phase 0 - Standard 069)\r\n  const expansionTags = await expandQuery(cleanQuery);\r\n  const expandedQuery = expansionTags.length > 0 ? `${parsedQuery} ${expansionTags.join(' ')}` : parsedQuery;\r\n  console.log(`[Search] Optimized Query: ${expandedQuery}`);\r\n\r\n  // 1. ENGRAM LOOKUP\r\n  const engramResults = await lookupByEngram(cleanQuery);\r\n  let finalResults: SearchResult[] = [];\r\n  const includedIds = new Set<string>();\r\n\r\n  if (engramResults.length > 0) {\r\n    // ... (logic remains same)\r\n    console.log(`[Search] Found ${engramResults.length} via Engram: ${cleanQuery}`);\r\n    const engramContextQuery = `?[id, content, source, timestamp, buckets, tags, epochs, provenance] := *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}, id in $ids`;\r\n    const engramContentResult = await db.run(engramContextQuery, { ids: engramResults });\r\n    if (engramContentResult.rows) {\r\n      const realBucketsArray = Array.from(realBuckets); // Convert once for use in loop\r\n      engramContentResult.rows.forEach((row: any[]) => {\r\n        if (!includedIds.has(row[0])) {\r\n          // Basic check: Does this engram result match the tags?\r\n          const rowTags = row[5] as string[];\r\n          const rowBuckets = row[4] as string[];\r\n\r\n          const matchesTags = scopeTags.every(t => rowTags.includes(t));\r\n          const matchesBuckets = realBucketsArray.every(b => rowBuckets.includes(b));\r\n\r\n          if ((scopeTags.length === 0 || matchesTags) && (realBucketsArray.length === 0 || matchesBuckets)) {\r\n            finalResults.push({\r\n              id: row[0], content: row[1], source: row[2], timestamp: row[3], buckets: row[4], tags: row[5], epochs: row[6], provenance: row[7], score: 200\r\n            });\r\n            includedIds.add(row[0]);\r\n          }\r\n        }\r\n      });\r\n    }\r\n  }\r\n\r\n  // 2. TAG-WALKER SEARCH (Hybrid FTS + Graph)\r\n  // Pass both buckets and tags explicitely\r\n  const walkerResults = await tagWalkerSearch(expandedQuery, Array.from(realBuckets), scopeTags, maxChars);\r\n\r\n  // Merge and Apply Provenance Boosting\r\n  walkerResults.forEach(r => {\r\n    let score = r.score;\r\n\r\n    if (provenance === 'sovereign') {\r\n      if (r.provenance === 'sovereign') score *= 3.0;\r\n      else score *= 0.5;\r\n    } else if (provenance === 'external') {\r\n      if (r.provenance !== 'sovereign') score *= 1.5;\r\n    } else {\r\n      if (r.provenance === 'sovereign') score *= 2.0;\r\n    }\r\n\r\n    if (!includedIds.has(r.id)) {\r\n      finalResults.push({ ...r, score });\r\n      includedIds.add(r.id);\r\n    }\r\n  });\r\n\r\n  console.log(`[Search] Total Results: ${finalResults.length}`);\r\n\r\n  // Final Sort by Score\r\n  finalResults.sort((a, b) => b.score - a.score);\r\n\r\n  return formatResults(finalResults, maxChars);\r\n}\r\n\r\n/**\r\n * Traditional FTS fallback\r\n */\r\nexport async function runTraditionalSearch(query: string, buckets: string[]): Promise<SearchResult[]> {\r\n  const sanitizedQuery = sanitizeFtsQuery(query);\r\n  if (!sanitizedQuery) return [];\r\n\r\n  let queryCozo = '';\r\n  if (buckets.length > 0) {\r\n    queryCozo = `?[id, score, content, source, timestamp, buckets, tags, epochs, provenance] := ~memory:content_fts{id | query: $q, k: 500, bind_score: score}, *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}, length(intersection(buckets, $buckets)) > 0`;\r\n  } else {\r\n    queryCozo = `?[id, score, content, source, timestamp, buckets, tags, epochs, provenance] := ~memory:content_fts{id | query: $q, k: 500, bind_score: score}, *memory{id, content, source, timestamp, buckets, tags, epochs, provenance}`;\r\n  }\r\n\r\n  try {\r\n    const result = await db.run(queryCozo, { q: sanitizedQuery, buckets });\r\n    if (!result.rows) return [];\r\n\r\n    return result.rows.map((row: any[]) => ({\r\n      id: row[0],\r\n      score: row[1],\r\n      content: row[2],\r\n      source: row[3],\r\n      timestamp: row[4],\r\n      buckets: row[5],\r\n      tags: row[6],\r\n      epochs: row[7],\r\n      provenance: row[8]\r\n    }));\r\n  } catch (e) {\r\n    console.error('[Search] FTS failed', e);\r\n    return [];\r\n  }\r\n}\r\n\r\n/**\r\n * Format search results within character budget\r\n */\r\nfunction formatResults(results: SearchResult[], maxChars: number): { context: string; results: SearchResult[]; toAgentString: () => string; metadata?: any } {\r\n  const candidates = results.map(r => ({\r\n    id: r.id,\r\n    content: r.content,\r\n    source: r.source,\r\n    timestamp: r.timestamp,\r\n    score: r.score\r\n  }));\r\n\r\n  const tokenBudget = Math.floor(maxChars / 4);\r\n  const rollingContext = composeRollingContext(\"query_placeholder\", candidates, tokenBudget);\r\n\r\n  const sortedResults = results.sort((a, b) => b.score - a.score);\r\n\r\n  return {\r\n    context: rollingContext.prompt || 'No results found.',\r\n    results: sortedResults,\r\n    toAgentString: () => {\r\n      return sortedResults.map(r => `[${r.provenance}] ${r.source}: ${(r.content || \"\").substring(0, 200)}...`).join('\\n');\r\n    },\r\n    metadata: rollingContext.stats\r\n  };\r\n}\r\n\r\nexport function parseQuery(query: string): { phrases: string[]; temporal: string[]; buckets: string[]; keywords: string[]; } {\r\n  const result = { phrases: [] as string[], temporal: [] as string[], buckets: [] as string[], keywords: [] as string[] };\r\n  const phraseRegex = /\"([^\"]+)\"/g;\r\n  let phraseMatch;\r\n  while ((phraseMatch = phraseRegex.exec(query)) !== null) result.phrases.push(phraseMatch[1]);\r\n  let remainingQuery = query.replace(/\"[^\"]+\"/g, '');\r\n  const temporalRegex = /@(\\w+)/g;\r\n  let temporalMatch;\r\n  while ((temporalMatch = temporalRegex.exec(remainingQuery)) !== null) result.temporal.push(temporalMatch[1]);\r\n  remainingQuery = remainingQuery.replace(/@\\w+/g, '');\r\n  const bucketRegex = /#(\\w+)/g;\r\n  let bucketMatch;\r\n  while ((bucketMatch = bucketRegex.exec(remainingQuery)) !== null) result.buckets.push(bucketMatch[1]);\r\n  remainingQuery = remainingQuery.replace(/#\\w+/g, '');\r\n  result.keywords = remainingQuery.split(/\\s+/).filter(kw => kw.length > 0);\r\n  return result;\r\n}\r\n\r\n/**\r\n * Iterative Search with Back-off Strategy\r\n * Attempts to retrieve results by progressively simplifying the query.\r\n */\r\nexport async function iterativeSearch(\r\n  query: string,\r\n  buckets: string[] = [],\r\n  maxChars: number = 20000 // Higher budget for initial retrieval\r\n): Promise<{ context: string; results: SearchResult[]; attempt: number; metadata?: any }> {\r\n\r\n  // 0. Extract Scope Tags (Hashtags) to preserve them across strategies\r\n  // We want to make sure if user typed \"#work\", it stays even if we strip adjectives.\r\n  const scopeTags: string[] = [];\r\n  const queryParts = query.split(/\\s+/);\r\n  queryParts.forEach(part => {\r\n    if (part.startsWith('#')) scopeTags.push(part);\r\n  });\r\n  const tagsString = scopeTags.join(' ');\r\n\r\n  // Strategy 1: Standard Expanded Search (All Nouns, Verbs, Dates + Expansion)\r\n  console.log(`[IterativeSearch] Strategy 1: Standard Execution`);\r\n  let results = await executeSearch(query, undefined, buckets, maxChars);\r\n  if (results.results.length > 0) return { ...results, attempt: 1 };\r\n\r\n  // Strategy 2: Strict \"Subjects & Time\" (Strip Verbs/Adjectives, keep Nouns + Dates)\r\n  console.log(`[IterativeSearch] Strategy 2: Strict Nouns/Dates`);\r\n  const temporalContext = extractTemporalContext(query);\r\n  const doc = nlp.readDoc(query);\r\n  const nouns = doc.tokens().filter((t: any) => {\r\n    const tag = t.out(nlp.its.pos);\r\n    return tag === 'NOUN' || tag === 'PROPN';\r\n  }).out((nlp as any).its.text);\r\n\r\n  const uniqueTokens = new Set([...nouns, ...temporalContext]);\r\n  if (uniqueTokens.size > 0) {\r\n    // Re-inject scope tags\r\n    const strictQuery = Array.from(uniqueTokens).join(' ') + ' ' + tagsString;\r\n    console.log(`[IterativeSearch] Fallback Query 1: \"${strictQuery.trim()}\"`);\r\n    results = await executeSearch(strictQuery, undefined, buckets, maxChars);\r\n    if (results.results.length > 0) return { ...results, attempt: 2 };\r\n  }\r\n\r\n  // Strategy 3: \"Just the Dates\" (If query heavily implies time)\r\n  // Sometimes \"2025\" is the only anchor we have if keywords fail.\r\n  // Or maybe just \"Proper Nouns\" (Entities).\r\n  const propNouns = doc.tokens().filter((t: any) => t.out(nlp.its.pos) === 'PROPN').out((nlp as any).its.text);\r\n\r\n  // Re-inject scope tags\r\n  const entityQuery = [...new Set([...propNouns, ...temporalContext])].join(' ') + ' ' + tagsString;\r\n\r\n  if (entityQuery.trim().length > 0 && entityQuery.trim() !== (Array.from(uniqueTokens).join(' ') + ' ' + tagsString).trim()) {\r\n    console.log(`[IterativeSearch] Fallback Query 2: \"${entityQuery.trim()}\"`);\r\n    results = await executeSearch(entityQuery, undefined, buckets, maxChars);\r\n    if (results.results.length > 0) return { ...results, attempt: 3 };\r\n  }\r\n\r\n  return { ...results, attempt: 4 }; // Return empty result if all fail\r\n}"
    tokens: 8245
    size: 22486
  - path: engine\src\services\tags\discovery.ts
    content: "\r\nimport { db } from '../../core/db.js';\r\nimport { extractEntitiesWithGLiNER } from './gliner.js';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { fileURLToPath } from 'url';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\nconst PROJECT_ROOT = path.resolve(__dirname, '..', '..', '..');\r\nconst MASTER_TAGS_PATH = path.join(PROJECT_ROOT, 'context', 'sovereign_tags.json');\r\n\r\n/**\r\n * Discovery Service (The Teacher)\r\n * \r\n * Implements \"Tag Walker\" Strategy (Standard 068 Phase B):\r\n * 1. Pick a seed tag from the master list.\r\n * 2. Find atoms that contain this tag (\"Walking the Graph\").\r\n * 3. Use BERT NER to find NEW entities in these specific contexts.\r\n * 4. Add new entities to the master list (Expansion).\r\n */\r\nexport async function runDiscovery(sampleSize: number = 30): Promise<string[]> {\r\n    const masterTags = getMasterTags();\r\n    let query = '';\r\n    let strategy = 'random';\r\n    let seedTag = '';\r\n\r\n    // Strategy 1: The Walker (80% chance if we have tags)\r\n    // \"We have data specific tags... identified entities beyond the basic list that accommodate specific intricacies.\"\r\n    if (masterTags.length > 0 && Math.random() > 0.2) {\r\n        seedTag = masterTags[Math.floor(Math.random() * masterTags.length)];\r\n\r\n        // Find atoms that contain the seed tag (Simulating Graph Walk)\r\n        // We use the '~' operator for \"contains text\", which is efficient enough for now.\r\n        // We limit to sampleSize to keep it fast.\r\n        query = `\r\n            ?[content] := *memory{content}, \r\n            content ~ $seedTag \r\n            :limit ${sampleSize}\r\n        `;\r\n        strategy = 'walker';\r\n        console.log(`[Discovery] Teacher Mode (Walker): Expanding on seed tag '${seedTag}'...`);\r\n    }\r\n    // Strategy 2: The Explorer (Fallback / Initial Boot)\r\n    else {\r\n        query = `?[content] := *memory{content} :limit ${sampleSize}`;\r\n        strategy = 'explorer';\r\n        console.log(`[Discovery] Teacher Mode (Explorer): Random Sampling ${sampleSize} atoms...`);\r\n    }\r\n\r\n    let result;\r\n    try {\r\n        result = await db.run(query, { seedTag });\r\n    } catch (e: any) {\r\n        console.warn(`[Discovery] Query failed for strategy '${strategy}' (Seed: ${seedTag}):`, e.message);\r\n        console.warn(`[Discovery] Falling back to safe Explorer mode.`);\r\n        query = `?[content] := *memory{content} :limit ${sampleSize}`;\r\n        result = await db.run(query);\r\n    }\r\n\r\n    if (!result.rows || result.rows.length === 0) {\r\n        if (strategy === 'walker') {\r\n            console.log(`[Discovery] Walker found no atoms for tag '${seedTag}'. It might be rare.`);\r\n        } else {\r\n            console.warn('[Discovery] No atoms found for learning.');\r\n        }\r\n        return [];\r\n    }\r\n\r\n    const sampledContent = result.rows.map((r: any) => {\r\n        const content = String(r[0]);\r\n        // Truncate to keep BERT fast\r\n        return content.length > 500 ? content.substring(0, 500) : content;\r\n    }).join('\\n---\\n');\r\n\r\n    console.log(`[Discovery] Teacher analyzing ${result.rows.length} atoms via BERT...`);\r\n\r\n    try {\r\n        // 2a. Attempt Zero-Shot/BERT Extraction\r\n        // We ask BERT to look for standard entities, but since the context is specific (seeded),\r\n        // it is more likely to find domain-specific co-occurrences.\r\n        const discoveredTags = await extractEntitiesWithGLiNER(sampledContent, [\r\n            'person', 'organization', 'technology', 'project', 'software', 'location', 'concept'\r\n        ]);\r\n\r\n        console.log(`[Discovery] BERT found ${discoveredTags.length} potential tags.`);\r\n\r\n        if (discoveredTags.length > 0) {\r\n            // Filter out the seed tag so we don't just rediscover it\r\n            const newTags = discoveredTags.filter(t => t.toLowerCase() !== seedTag.toLowerCase());\r\n\r\n            if (newTags.length > 0) {\r\n                console.log(`[Discovery] Expansion Successful! '${seedTag}' led to: ${newTags.slice(0, 5).join(', ')}...`);\r\n                await updateMasterTags(newTags);\r\n            }\r\n            return newTags;\r\n        } else {\r\n            throw new Error(\"BERT found no entities.\");\r\n        }\r\n    } catch (e: any) {\r\n        console.warn(`[Discovery] Teacher (BERT) passed. Error: ${e.message}`);\r\n        // Optional: LLM Fallback (Slow, but very smart)\r\n        // For now, we return empty to stay fast/CPU-specific as requested.\r\n        return [];\r\n    }\r\n}\r\n\r\n/**\r\n * Updates the JSON master list with new findings.\r\n */\r\nasync function updateMasterTags(newTags: string[]) {\r\n    try {\r\n        let currentTags: any = { keywords: [] };\r\n\r\n        // Ensure directory exists\r\n        const contextDir = path.dirname(MASTER_TAGS_PATH);\r\n        if (!fs.existsSync(contextDir)) {\r\n            fs.mkdirSync(contextDir, { recursive: true });\r\n        }\r\n\r\n        // Read existing\r\n        if (fs.existsSync(MASTER_TAGS_PATH)) {\r\n            const content = fs.readFileSync(MASTER_TAGS_PATH, 'utf8');\r\n            try {\r\n                currentTags = JSON.parse(content);\r\n                // Handle if it's just an array vs object\r\n                if (Array.isArray(currentTags)) {\r\n                    currentTags = { keywords: currentTags };\r\n                }\r\n            } catch (jsonErr) {\r\n                console.warn('[Discovery] Corrupt tags file, starting fresh.');\r\n            }\r\n        }\r\n\r\n        // Merge\r\n        const existingSet = new Set(currentTags.keywords.map((t: string) => t.toLowerCase()));\r\n        const added: string[] = [];\r\n\r\n        newTags.forEach(tag => {\r\n            const normalized = tag.toLowerCase().trim();\r\n            if (normalized.length > 2 && !existingSet.has(normalized)) {\r\n                // Basic filtering\r\n                if (!['the', 'and', 'for', 'with'].includes(normalized)) {\r\n                    currentTags.keywords.push(tag); // Keep original case\r\n                    existingSet.add(normalized);\r\n                    added.push(tag);\r\n                }\r\n            }\r\n        });\r\n\r\n        if (added.length > 0) {\r\n            fs.writeFileSync(MASTER_TAGS_PATH, JSON.stringify(currentTags, null, 2));\r\n            console.log(`[Discovery] Learned ${added.length} new tags:`, added.join(', '));\r\n        }\r\n    } catch (e) {\r\n        console.error('[Discovery] Failed to update master list:', e);\r\n    }\r\n}\r\n\r\n/**\r\n * Reads the master list for the Infector (and Walker).\r\n */\r\nexport function getMasterTags(): string[] {\r\n    try {\r\n        if (fs.existsSync(MASTER_TAGS_PATH)) {\r\n            const content = fs.readFileSync(MASTER_TAGS_PATH, 'utf8');\r\n            const data = JSON.parse(content);\r\n            if (Array.isArray(data)) return data;\r\n            if (data.keywords && Array.isArray(data.keywords)) return data.keywords;\r\n        }\r\n    } catch (e) {\r\n        console.error('[Discovery] Failed to load master_tags.json:', e);\r\n    }\r\n    return [];\r\n}\r\n"
    tokens: 2460
    size: 6970
  - path: engine\src\services\tags\gliner.ts
    content: "\r\n/**\r\n * NER Teacher Service (BERT-based)\r\n *\r\n * Uses an ONNX-optimized BERT model to perform Named Entity Recognition.\r\n * Switched from GLiNER (unsupported architecture) to standard BERT NER.\r\n */\r\n\r\nlet nerPipeline: any = null;\r\n\r\nexport async function extractEntitiesWithGLiNER(text: string, _entities: string[] = []): Promise<string[]> {\r\n    try {\r\n        if (!nerPipeline) {\r\n            console.log('[NER] Dynamically loading Transformers.js...');\r\n            const { pipeline, env } = await import('@xenova/transformers');\r\n\r\n            // Disable native dependencies that might cause crashes on Windows\r\n            env.allowLocalModels = true;\r\n            // Disable ONNX native backend that requires sharp\r\n            env.backends.onnx['native'] = false;\r\n            env.backends.onnx.wasm.proxy = false;\r\n            env.backends.onnx.wasm.numThreads = 1;\r\n\r\n            // Additional settings to avoid sharp\r\n            env.useFS = false;\r\n            env.useBrowserCache = false;\r\n\r\n            console.log('[NER] Loading BERT NER model (Xenova/bert-base-NER)...');\r\n            try {\r\n                nerPipeline = await pipeline('token-classification', 'Xenova/bert-base-NER', {\r\n                    quantized: true\r\n                });\r\n            } catch (e) {\r\n                console.warn('[NER] Primary model failed. Trying fallback (Xenova/bert-base-multilingual-cased-ner-hrl)...');\r\n                nerPipeline = await pipeline('token-classification', 'Xenova/bert-base-multilingual-cased-ner-hrl', {\r\n                    quantized: true\r\n                });\r\n            }\r\n            console.log('[NER] Model loaded successfully.');\r\n        }\r\n\r\n        // BERT NER returns entities with labels like B-PER, I-ORG, B-LOC, B-MISC\r\n        // We extract the actual text (word) from each recognized entity\r\n        const results = await nerPipeline(text);\r\n        const discovered = new Set<string>();\r\n\r\n        for (const res of results) {\r\n            // Filter by confidence score and entity type\r\n            // B- prefix means \"Beginning of entity\", I- means \"Inside entity\"\r\n            if (res.score > 0.7 && res.entity && res.word) {\r\n                // Clean up subword tokens (BERT uses ## prefix for subwords)\r\n                const word = res.word.replace(/^##/, '').trim();\r\n                if (word.length > 1) {\r\n                    discovered.add(word);\r\n                }\r\n            }\r\n        }\r\n\r\n        console.log(`[NER] Discovered ${discovered.size} entities.`);\r\n        return Array.from(discovered);\r\n    } catch (e: any) {\r\n        console.warn('[NER] Service Initialization Failed:', e.message);\r\n        console.log('[NER] Falling back gracefully to LLM...');\r\n        return [];\r\n    }\r\n}\r\n"
    tokens: 972
    size: 2759
  - path: engine\src\services\tags\infector.ts
    content: "/**\r\n * Tag Infection Service (The \"Student\")\r\n *\r\n * Implements Standard 068: Weak Supervision via High-Speed Pattern Matching.\r\n * Implements Standard 069: Functional Flow (Generators) for infinite scaling.\r\n */\r\n\r\nimport wink from 'wink-nlp';\r\nimport model from 'wink-eng-lite-web-model';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { fileURLToPath } from 'url';\r\nimport { db } from '../../core/db.js';\r\n\r\n// Initialize the \"Reflex\" Engine (Fast CPU NLP)\r\n// Cast to any to avoid strict typing issues with wink-nlp generic models\r\nconst nlp = wink(model) as any;\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\nconst PROJECT_ROOT = path.resolve(__dirname, '..', '..', '..', '..'); // engine/src/services/tags -> engine/src/services -> engine/src -> engine -> ROOT\r\nconst TAGS_FILE = path.join(PROJECT_ROOT, 'context', 'sovereign_tags.json');\r\n\r\n/**\r\n * 1. The Generator (Source)\r\n * Lazily fetches atoms from the database in batches to prevent RAM spikes.\r\n * This replaces the need for recursion or massive array loading.\r\n */\r\nasync function* atomStream(batchSize = 500) {\r\n    let lastId = '';\r\n\r\n    while (true) {\r\n        // Fetch next batch where ID > lastId\r\n        const query = `\r\n            ?[id, content, tags] := *memory{id, content, tags},\r\n            id > $lastId,\r\n            :order id\r\n            :limit $limit\r\n        `;\r\n\r\n        const result = await db.run(query, { lastId, limit: batchSize });\r\n        if (result.rows && result.rows.length > 0) {\r\n            console.log(`[Infector] Stream fetched batch of ${result.rows.length} atoms...`);\r\n        }\r\n\r\n        if (!result.rows || result.rows.length === 0) {\r\n            break; // Stream exhausted\r\n        }\r\n\r\n        // Yield one atom at a time (Functional Flow)\r\n        for (const row of result.rows) {\r\n            const [id, content, tags] = row;\r\n            lastId = id as string; // Move cursor for next batch\r\n\r\n            yield {\r\n                id: id as string,\r\n                content: content as string,\r\n                tags: (tags as string[]) || []\r\n            };\r\n        }\r\n    }\r\n}\r\n\r\n/**\r\n * 2. The Processor (Transform)\r\n * Applies \"Viral Tags\" to a single atom.\r\n */\r\nfunction infectAtom(atom: { id: string, content: string, tags: string[] }, patterns: any): string[] | null {\r\n    if (!atom.content) return null;\r\n\r\n    const currentTags = new Set(atom.tags);\r\n    let changed = false;\r\n\r\n    // Use Wink-NLP to normalize text (case folding, tokenization)\r\n    const doc = nlp.readDoc(atom.content);\r\n    const text = (doc.out(nlp.its.text) as string).toLowerCase();\r\n\r\n    // Fast check: Does text contain the pattern?\r\n    patterns.keywords.forEach((keyword: string) => {\r\n        if (!currentTags.has(keyword) && text.includes(keyword.toLowerCase())) {\r\n            currentTags.add(keyword); // Infection!\r\n            changed = true;\r\n        }\r\n    });\r\n\r\n    // --- ENHANCEMENT: Temporal Auto-Tagging ---\r\n\r\n    // 1. Years (1900 - 2099)\r\n    // Regex matches 4 digits starting with 19 or 20, surrounded by boundaries\r\n    const yearMatches = text.match(/\\b((?:19|20)\\d{2})\\b/g);\r\n    if (yearMatches) {\r\n        yearMatches.forEach(year => (!currentTags.has(year)) && (currentTags.add(year), changed = true));\r\n    }\r\n\r\n    // 2. Months (Full Names)\r\n    const months = [\r\n        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\r\n        \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\r\n    ];\r\n\r\n    // Simple inclusion check for months (since we normalized text to lowercase)\r\n    // We check for word boundaries to avoid matching \"may\" inside \"maybe\"\r\n    months.forEach(month => {\r\n        // Create regex for word boundary match\r\n        const regex = new RegExp(`\\\\b${month}\\\\b`, 'i');\r\n        if (!currentTags.has(month) && regex.test(text)) {\r\n            // Capitalize first letter for the tag\r\n            const tag = month.charAt(0).toUpperCase() + month.slice(1);\r\n            currentTags.add(tag);\r\n            changed = true;\r\n        }\r\n    });\r\n\r\n    return changed ? Array.from(currentTags) : null;\r\n}\r\n\r\n/**\r\n * 3. The Orchestrator (Sink)\r\n * Connects the Stream to the Processor.\r\n */\r\nexport async function runInfectionLoop() {\r\n    console.log(' Infection Protocol: Initializing...');\r\n\r\n    // Load the \"Virus\" (Master Tag List)\r\n    if (!fs.existsSync(TAGS_FILE)) {\r\n        // Fallback check for alternate location (if running from dist/)\r\n        console.warn(` No tag definitions found at ${TAGS_FILE}. Checking common paths...`);\r\n        return;\r\n    }\r\n\r\n    const viralPatterns = JSON.parse(fs.readFileSync(TAGS_FILE, 'utf-8'));\r\n    let infectedCount = 0;\r\n\r\n    // The Loop (Looks clean, acts efficient)\r\n    for await (const atom of atomStream()) {\r\n        const newTags = infectAtom(atom, viralPatterns);\r\n\r\n        if (newTags) {\r\n            // Persist the infection\r\n            // We update the 'tags' column. In Cozo, :update needs keys.\r\n            // Using a retry loop to handle potential lock contention with Ingest\r\n            let attempts = 0;\r\n            const maxAttempts = 3;\r\n            while (attempts < maxAttempts) {\r\n                try {\r\n                    await db.run(`\r\n                        ?[id, tags] <- [[$id, $tags]]\r\n                        :update memory {id, tags}\r\n                    `, { id: atom.id, tags: newTags });\r\n\r\n                    infectedCount++;\r\n                    if (infectedCount % 100 === 0) process.stdout.write(`.`);\r\n                    break; // Success\r\n                } catch (error: any) {\r\n                    attempts++;\r\n                    if (attempts >= maxAttempts) {\r\n                        console.warn(`[Infector] Failed to update atom ${atom.id} after ${maxAttempts} attempts:`, error.message);\r\n                    } else {\r\n                        // Small backoff\r\n                        await new Promise(r => setTimeout(r, 100 * attempts));\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    console.log(`\\n Infection Complete. ${infectedCount} atoms infected with new context.`);\r\n}\r\n"
    tokens: 2201
    size: 6173
  - path: engine\src\services\vision\vision_service.js
    content: "const { spawn } = require('child_process');\r\nconst path = require('path');\r\nconst fs = require('fs');\r\nconst http = require('http');\r\nconst paths = require('../../config/paths');\r\nconst Config = require('../../config');\r\n\r\nlet serverProcess = null;\r\nlet lastVisionError = null;\r\nconst SERVER_PORT = 8081;\r\nconst BIN_PATH = path.join(paths.BASE_PATH, 'engine/bin/llama-server.exe');\r\nconst MODEL_DIR = path.join(paths.BASE_PATH, 'engine/models/vision');\r\nconst VISION_CONFIG = Config.MODELS.VISION;\r\n\r\n// Auto-detect model file\r\nconst getModelPath = () => {\r\n    try {\r\n        // Prioritize User's custom model from Config\r\n        if (VISION_CONFIG.PATH) {\r\n            // Check if absolute path\r\n            if (fs.existsSync(VISION_CONFIG.PATH)) {\r\n                console.log(`[Vision] Using configured path: ${VISION_CONFIG.PATH}`);\r\n                return VISION_CONFIG.PATH;\r\n            }\r\n            // Check if relative to MODEL_DIR\r\n            const relativePath = path.join(MODEL_DIR, VISION_CONFIG.PATH);\r\n            if (fs.existsSync(relativePath)) {\r\n                console.log(`[Vision] Using configured model (relative): ${relativePath}`);\r\n                return relativePath;\r\n            }\r\n        }\r\n\r\n        if (!fs.existsSync(MODEL_DIR)) {\r\n            console.log(`[Vision] MODEL_DIR not found: ${MODEL_DIR}`);\r\n            return null;\r\n        }\r\n        const files = fs.readdirSync(MODEL_DIR);\r\n        const gguf = files.find(f => f.endsWith('.gguf') && !f.includes('mmproj'));\r\n        return gguf ? path.join(MODEL_DIR, gguf) : null;\r\n    } catch (e) {\r\n        console.error(`[Vision] Error detecting models: ${e.message}`);\r\n        return null;\r\n    }\r\n};\r\n\r\n// Optional: detect separate projector if exists\r\nconst getMmprojPath = () => {\r\n    try {\r\n        // Check Config first\r\n        if (VISION_CONFIG.PROJECTOR) {\r\n            const configProjPath = path.isAbsolute(VISION_CONFIG.PROJECTOR)\r\n                ? VISION_CONFIG.PROJECTOR\r\n                : path.join(MODEL_DIR, VISION_CONFIG.PROJECTOR);\r\n\r\n            if (fs.existsSync(configProjPath)) return configProjPath;\r\n        }\r\n\r\n        if (!fs.existsSync(MODEL_DIR)) return null;\r\n        const files = fs.readdirSync(MODEL_DIR);\r\n        const proj = files.find(f => f.includes('mmproj'));\r\n        return proj ? path.join(MODEL_DIR, proj) : null;\r\n    } catch (e) { return null; }\r\n};\r\n\r\nasync function startVisionServer() {\r\n    if (serverProcess) {\r\n        // Double check if process is really alive, otherwise nullify\r\n        if (serverProcess.exitCode !== null) {\r\n            console.warn(\"[Vision] Process found but it has exited. Restarting...\");\r\n            serverProcess = null;\r\n        } else {\r\n            return;\r\n        }\r\n    }\r\n\r\n    const modelPath = getModelPath();\r\n    if (!modelPath) {\r\n        console.warn(\"[Vision] No GGUF model found. Vision features disabled.\");\r\n        return;\r\n    }\r\n\r\n    const args = [\r\n        '-m', modelPath,\r\n        '--port', SERVER_PORT.toString(),\r\n        '-c', VISION_CONFIG.CTX_SIZE.toString(),\r\n        '--n-gpu-layers', VISION_CONFIG.GPU_LAYERS.toString(),\r\n    ];\r\n\r\n    // Check if separate mmproj exists\r\n    const mmproj = getMmprojPath();\r\n    if (mmproj) {\r\n        args.push('--mmproj', mmproj);\r\n    }\r\n\r\n    console.log(`[Vision] Launching Binary Sidecar: llama-server.exe on port ${SERVER_PORT}`);\r\n    console.log(`[Vision] Model Path: ${modelPath}`);\r\n    if (mmproj) console.log(`[Vision] Projector Path: ${mmproj}`);\r\n\r\n    try {\r\n        serverProcess = spawn(BIN_PATH, args, {\r\n            stdio: ['ignore', 'pipe', 'pipe']\r\n        });\r\n\r\n        serverProcess.stdout.on('data', (data) => {\r\n            const msg = data.toString();\r\n            // console.log(`[Vision Binary] ${msg}`); \r\n        });\r\n\r\n        serverProcess.stderr.on('data', (data) => {\r\n            const msg = data.toString();\r\n            if (msg.includes('server is listening') || msg.includes('HTTP server listening')) {\r\n                console.log(`[Vision] Sidecar Ready.`);\r\n            }\r\n\r\n            // Detect specific architecture errors\r\n            if (msg.includes('unknown model architecture')) {\r\n                lastVisionError = \"Incompatible Binary: Your llama-server.exe does not support this model type (e.g. Qwen2-VL). Please update engine/bin or use a different model.\";\r\n                console.error(`[Vision Critical] ${lastVisionError}`);\r\n            }\r\n\r\n            // LOG ALL ERRORS\r\n            if (msg.includes('error') || msg.includes('Error') || msg.includes('failed')) {\r\n                console.error(`[Vision Binary Error] ${msg.trim()}`);\r\n            }\r\n        });\r\n\r\n        serverProcess.on('close', (code) => {\r\n            console.log(`[Vision] Sidecar exited with code ${code}`);\r\n            serverProcess = null;\r\n        });\r\n    } catch (e) {\r\n        console.error(`[Vision] Failed to spawn sidecar: ${e.message}`);\r\n    }\r\n}\r\n\r\nfunction stopVisionServer() {\r\n    if (serverProcess) {\r\n        serverProcess.kill();\r\n        serverProcess = null;\r\n    }\r\n}\r\n\r\nasync function analyzeImage(base64Image, prompt) {\r\n    if (!serverProcess) {\r\n        lastVisionError = null;\r\n        await startVisionServer();\r\n        if (!serverProcess) throw new Error(\"Vision server failed to start (Mock Mode or Missing Binary).\");\r\n        // Wait for boot\r\n        await new Promise(r => setTimeout(r, 4000));\r\n\r\n        if (!serverProcess) {\r\n            // Return the specific error if captured, otherwise generic\r\n            throw new Error(lastVisionError || \"Vision server crashed during startup.\");\r\n        }\r\n    }\r\n\r\n    return new Promise((resolve, reject) => {\r\n        // Standard ChatML format for Qwen2-VL\r\n        const payload = JSON.stringify({\r\n            prompt: `<|im_start|>system\\nYou are a helpful visual assistant. You can see the image provided. Describe it in detail.<|im_end|>\\n<|im_start|>user\\n<image>\\n${prompt}<|im_end|>\\n<|im_start|>assistant\\n`,\r\n            image_data: [{ data: base64Image, id: 12 }],\r\n            n_predict: 400,\r\n            temperature: 0.1,\r\n            cache_prompt: true\r\n        });\r\n\r\n        const options = {\r\n            hostname: 'localhost',\r\n            port: SERVER_PORT,\r\n            path: '/completion',\r\n            method: 'POST',\r\n            headers: {\r\n                'Content-Type': 'application/json',\r\n                'Content-Length': payload.length\r\n            }\r\n        };\r\n\r\n        const req = http.request(options, (res) => {\r\n            let data = '';\r\n            res.on('data', (chunk) => data += chunk);\r\n            res.on('end', () => {\r\n                if (!data || data.trim().length === 0) {\r\n                    return reject(new Error(\"Vision sidecar returned empty response. It may have crashed.\"));\r\n                }\r\n                try {\r\n                    const json = JSON.parse(data);\r\n                    // Standard llama-server completion response\r\n                    resolve(json.content || json.text || String(data));\r\n                } catch (e) {\r\n                    // If not JSON, it might be raw text error output\r\n                    if (data.includes('error') || data.includes('failed')) {\r\n                        reject(new Error(`Vision sidecar error: ${data.substring(0, 100)}`));\r\n                    } else {\r\n                        reject(new Error(`Failed to parse vision response: ${e.message}`));\r\n                    }\r\n                }\r\n            });\r\n        });\r\n\r\n        req.on('error', (e) => {\r\n            reject(new Error(`Vision Request Error: ${e.message}`));\r\n        });\r\n\r\n        req.write(payload);\r\n        req.end();\r\n    });\r\n}\r\n\r\nmodule.exports = { startVisionServer, stopVisionServer, analyzeImage };\r\n"
    tokens: 2671
    size: 7764
  - path: engine\src\types\api.ts
    content: "\r\nexport interface Menu {\r\n    id: string;\r\n    content: string;\r\n    source: string;\r\n    type: string;\r\n    timestamp: number;\r\n    buckets: string[];\r\n    tags: string;\r\n    epochs: string;\r\n    provenance: string;\r\n    score?: number;\r\n}\r\n\r\nexport interface SearchRequest {\r\n    query: string;           // The natural language query\r\n    limit?: number;          // Elastic Window (default 20)\r\n    max_chars?: number;      // Character budget\r\n    deep?: boolean;          // If true, trigger 'Epochal' search (Dreamer layers)\r\n\r\n    // The \"UniversalRAG\" Routing Layer\r\n    buckets?: string[];      // e.g., [\"@code\", \"@visual\", \"@memory\"]\r\n    provenance?: 'sovereign' | 'external' | 'all'; // Data Provenance filter\r\n}\r\n\r\nexport interface SearchResponse {\r\n    context: string;\r\n    results: Menu[];\r\n    metadata: {\r\n        engram_hits: number;   // Did we find exact entity matches?\r\n        vector_latency: number;\r\n        provenance_boost_active: boolean;\r\n    }\r\n}\r\n"
    tokens: 335
    size: 982
  - path: engine\src\types\cozo-node.d.ts
    content: "declare module 'cozo-node' {\r\n  export interface CozoDbOptions {\r\n    db_path?: string;\r\n    storage_type?: string;\r\n  }\r\n\r\n  export class CozoDb {\r\n    constructor(storage_type?: string, db_path?: string);\r\n    run(query: string, params?: Record<string, any>): any;\r\n    close(): void;\r\n    // Add other methods as needed based on actual usage\r\n  }\r\n}"
    tokens: 126
    size: 352
  - path: engine\src\utils\date_extractor.ts
    content: "\r\n/**\r\n * Date Extractor Utility (Standard 072)\r\n * \r\n * Scans text for narrative dates to override file timestamps.\r\n * Prioritizes:\r\n * 1. ISO format [YYYY-MM-DD]\r\n * 2. Written format (January 20, 2024)\r\n * 3. Compact format (2024/01/20)\r\n */\r\n\r\nexport function extractDateFromContent(content: string): number | null {\r\n    if (!content || content.length === 0) return null;\r\n\r\n    // Scan only the first 500 characters for performance\r\n    const scanText = content.substring(0, 500);\r\n\r\n    // 1. Explicit Timestamp Tag [2024-01-20T10:00:00] or [2024-01-20]\r\n    const isoTag = scanText.match(/\\[(\\d{4}-\\d{2}-\\d{2}(?:T\\d{2}:\\d{2}:\\d{2})?)\\]/);\r\n    if (isoTag) {\r\n        const date = new Date(isoTag[1]);\r\n        if (!isNaN(date.getTime())) return date.getTime();\r\n    }\r\n\r\n    // 2. Standard ISO 2024-01-20 (surrounded by whitespace or boundaries)\r\n    const iso = scanText.match(/\\b(\\d{4}-\\d{2}-\\d{2})\\b/);\r\n    if (iso) {\r\n        const date = new Date(iso[1]);\r\n        if (!isNaN(date.getTime())) return date.getTime();\r\n    }\r\n\r\n    // 3. Written English (January 20, 2024 or Jan 20 2024)\r\n    const written = scanText.match(/\\b(January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+(\\d{1,2})(?:st|nd|rd|th)?,?\\s+(\\d{4})\\b/i);\r\n    if (written) {\r\n        const dateString = `${written[1]} ${written[2]} ${written[3]}`;\r\n        const date = new Date(dateString);\r\n        if (!isNaN(date.getTime())) return date.getTime();\r\n    }\r\n\r\n    // 4. Chat Log Timestamp format: \"2024/01/20 10:30\" or \"01/20/2024\"\r\n    // CAUTION: US vs EU formats ambiguity. We assume US (MM/DD/YYYY) or YYYY/MM/DD.\r\n\r\n    // YYYY/MM/DD\r\n    const isoSlash = scanText.match(/\\b(\\d{4})\\/(\\d{2})\\/(\\d{2})\\b/);\r\n    if (isoSlash) {\r\n        const date = new Date(`${isoSlash[1]}-${isoSlash[2]}-${isoSlash[3]}`);\r\n        if (!isNaN(date.getTime())) return date.getTime();\r\n    }\r\n\r\n    return null; // Fallback to file timestamp\r\n}\r\n"
    tokens: 789
    size: 2001
  - path: engine\src\utils\llamaLoader.ts
    content: "\r\nimport { getLlama, LlamaChatSession, LlamaContext, LlamaModel } from 'node-llama-cpp';\r\n\r\nlet llama: any = null;\r\n\r\nexport async function getLlamaInstance() {\r\n    if (!llama) {\r\n        llama = await getLlama();\r\n    }\r\n    return llama;\r\n}\r\n\r\nexport async function getLlamaComponents() {\r\n    return {\r\n        LlamaChatSession,\r\n        LlamaContext,\r\n        LlamaModel\r\n    };\r\n}\r\n"
    tokens: 129
    size: 388
  - path: engine\tests\context_experiments.js
    content: "/**\r\n * Context Experiments - Verification Script\r\n *\r\n * Verifies the \"UniversalRAG\" pipeline:\r\n * 1. Vector Search (Semantic Retrieval)\r\n * 2. Context Assembly (Markovian + Graph-R1 simulation)\r\n * 3. Configuration Compliance\r\n */\r\n\r\nimport 'dotenv/config'; // Load .env first\r\nimport { db } from '../dist/core/db.js';\r\nimport { config } from '../dist/config/index.js';\r\n\r\nasync function runExperiments() {\r\n    console.log(' Starting Context Experiments...');\r\n\r\n    // 1. Verify Configuration\r\n    console.log(`\\n[Config Check] Embedding Dimension: ${config.MODELS.EMBEDDING.DIM}`);\r\n    if (!config.MODELS.EMBEDDING.DIM || config.MODELS.EMBEDDING.DIM === 0) {\r\n        console.error(' CRITICAL: LLM_EMBEDDING_DIM is 0 or undefined!');\r\n        process.exit(1);\r\n    } else {\r\n        console.log(' Config Loaded Successfully');\r\n    }\r\n\r\n    try {\r\n        await db.init();\r\n\r\n        // 2. Vector Search Test\r\n        const query = \"What is the capital of France?\"; // Simple query\r\n        console.log(`\\n[Search Test] Query: \"${query}\"`);\r\n\r\n        // Mock embedding generation (using random vector for connectivity test)\r\n        // In real usage, we'd call the LLM. Here we just test the DB path.\r\n        const mockEmbedding = new Array(config.MODELS.EMBEDDING.DIM).fill(0.01);\r\n\r\n        // Manual HNSW search query simulation\r\n        // (Note: HNSW index creation is disabled in db.ts, so this checks the linear scan fallback or basic query)\r\n        const vecQuery = `\r\n            ?[id, distance] := *memory{id, embedding},\r\n            distance = cosine_dist(embedding, $queryVec),\r\n            distance < 0.2\r\n            :sort distance\r\n            :limit 5\r\n        `;\r\n\r\n        // Using explicit run to test syntax\r\n        // const results = await db.run(vecQuery, { queryVec: mockEmbedding });\r\n        // NOTE: CozoDB might fail on large vector literals in query string.\r\n        // We really want to verify that the table HAS data.\r\n\r\n        const countQuery = `?[id] := *memory{id}`;\r\n        const countResult = await db.run(countQuery);\r\n        console.log(`\\n[DB Status] Total Memories: ${countResult.rows ? countResult.rows.length : 0}`);\r\n\r\n        if ((countResult.rows ? countResult.rows.length : 0) === 0) {\r\n            console.warn('  Database is empty. Please add data to `notebook/inbox` to test retrieval.');\r\n        } else {\r\n            // 3. Retrieve some atoms to check structure\r\n            const sampleQuery = `\r\n                ?[id, content, source_id, embedding_len] := *memory{id, content, source_id, embedding},\r\n                embedding_len = length(embedding)\r\n                :limit 3\r\n             `;\r\n            const sample = await db.run(sampleQuery);\r\n            console.log('\\n[Sample Atoms]:');\r\n            sample.rows.forEach(row => {\r\n                console.log(`- ID: ${row[0]}`);\r\n                console.log(`  SourceID: ${row[2]}`);\r\n                console.log(`  Embedding Length: ${row[3]}`);\r\n                if (row[3] !== config.MODELS.EMBEDDING.DIM) {\r\n                    console.error(` DIMENSION MISMATCH! Expected ${config.MODELS.EMBEDDING.DIM}, Got ${row[3]}`);\r\n                } else {\r\n                    console.log(' Dimension OK');\r\n                }\r\n            });\r\n        }\r\n\r\n        // 4. Test Graph-R1 Flow (Simulation)\r\n        // Ideally we'd trace a relationship, e.g., Next/Prev\r\n        // For now, listing available sources is a good proxy for \"Graph Nodes\"\r\n        const sourceQuery = `?[path, total_atoms] := *source{path, total_atoms}`;\r\n        const sources = await db.run(sourceQuery);\r\n        console.log(`\\n[Sources Check] Available Sources: ${sources.rows ? sources.rows.length : 0}`);\r\n        if (sources.rows && sources.rows.length > 0) {\r\n            console.log(' Sources Table Working');\r\n            sources.rows.slice(0, 3).forEach(row => {\r\n                console.log(`  - ${row[0]} (${row[1]} atoms)`);\r\n            });\r\n        } else {\r\n            console.log('  No sources found, but that\\'s OK if no data has been ingested yet.');\r\n        }\r\n\r\n        console.log('\\n Context Experiments Complete!');\r\n        console.log(' CozoDB Integration Verified');\r\n        \r\n    } catch (error) {\r\n        console.error(' Context Experiments Failed:', error.message);\r\n        process.exit(1);\r\n    }\r\n}\r\n\r\nrunExperiments();"
    tokens: 1567
    size: 4394
  - path: engine\tests\dynamic_import_validation.test.js
    content: "import fs from 'fs';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n/**\r\n * Test to validate that all dynamic imports in the codebase use the correct .js extension\r\n * This prevents ESM/CJS interop issues when running the application\r\n */\r\n\r\n// Function to recursively find all .js, .ts, .mjs, and .cjs files in a directory\r\nfunction getAllSourceFiles(dir, fileList = []) {\r\n    const files = fs.readdirSync(dir);\r\n    \r\n    for (const file of files) {\r\n        const filePath = path.join(dir, file);\r\n        const stat = fs.statSync(filePath);\r\n        \r\n        if (stat.isDirectory()) {\r\n            // Skip node_modules and dist directories to focus on source code\r\n            if (file !== 'node_modules' && file !== 'dist' && !file.startsWith('.')) {\r\n                getAllSourceFiles(filePath, fileList);\r\n            }\r\n        } else if (/\\.(js|ts|mjs|cjs)$/.test(path.extname(filePath))) {\r\n            fileList.push(filePath);\r\n        }\r\n    }\r\n    \r\n    return fileList;\r\n}\r\n\r\n// Function to find all dynamic import statements in a file\r\nfunction findDynamicImports(content, filePath) {\r\n    // Regular expression to match dynamic import statements\r\n    // Looks for await import(...) or import(...) patterns\r\n    const dynamicImportRegex = /(await\\s+)?import\\s*\\(\\s*[\"'](.*?\\.(js|ts))[\"']\\s*\\)/g;\r\n    const matches = [];\r\n    let match;\r\n    \r\n    while ((match = dynamicImportRegex.exec(content)) !== null) {\r\n        matches.push({\r\n            fullMatch: match[0],\r\n            hasAwait: match[1] ? true : false,\r\n            importPath: match[2],\r\n            extension: match[3],\r\n            position: match.index\r\n        });\r\n    }\r\n    \r\n    return matches;\r\n}\r\n\r\ndescribe('Dynamic Import Validation', () => {\r\n    it('should ensure all dynamic imports use .js extension for ESM compatibility', () => {\r\n        // Get all source files from the src directory\r\n        const srcDir = path.join(__dirname, '../src');\r\n        const sourceFiles = getAllSourceFiles(srcDir);\r\n        \r\n        const errors = [];\r\n        \r\n        for (const filePath of sourceFiles) {\r\n            const content = fs.readFileSync(filePath, 'utf8');\r\n            const dynamicImports = findDynamicImports(content, filePath);\r\n            \r\n            for (const imp of dynamicImports) {\r\n                // Check if the import path ends with .js for ESM compatibility\r\n                if (!imp.importPath.endsWith('.js')) {\r\n                    errors.push({\r\n                        file: filePath,\r\n                        importStatement: imp.fullMatch,\r\n                        position: imp.position,\r\n                        message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`\r\n                    });\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Also check some key files in the root and other directories\r\n        const additionalFiles = [\r\n            path.join(__dirname, '../server.js'),\r\n            path.join(__dirname, '../index.js'),\r\n            path.join(__dirname, '../src/index.ts'),\r\n            path.join(__dirname, '../src/index.js')\r\n        ];\r\n        \r\n        for (const filePath of additionalFiles) {\r\n            if (fs.existsSync(filePath)) {\r\n                const content = fs.readFileSync(filePath, 'utf8');\r\n                const dynamicImports = findDynamicImports(content, filePath);\r\n                \r\n                for (const imp of dynamicImports) {\r\n                    if (!imp.importPath.endsWith('.js')) {\r\n                        errors.push({\r\n                            file: filePath,\r\n                            importStatement: imp.fullMatch,\r\n                            position: imp.position,\r\n                            message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`\r\n                        });\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Report any errors found\r\n        if (errors.length > 0) {\r\n            console.error('\\n Dynamic Import Validation Failed!');\r\n            console.error('Found dynamic imports that do not use .js extension:');\r\n            \r\n            for (const error of errors) {\r\n                console.error(`\\nFile: ${error.file}`);\r\n                console.error(`Line: ${getLineNumber(error.file, error.position)}`);\r\n                console.error(`Import: ${error.importStatement}`);\r\n                console.error(`Issue: ${error.message}`);\r\n            }\r\n            \r\n            throw new Error(`${errors.length} dynamic import(s) need to be updated to use .js extension`);\r\n        }\r\n        \r\n        console.log(` All dynamic imports validated successfully! Checked ${sourceFiles.length} source files.`);\r\n    });\r\n});\r\n\r\n// Helper function to get line number from position in file\r\nfunction getLineNumber(filePath, position) {\r\n    const content = fs.readFileSync(filePath, 'utf8');\r\n    const lines = content.substring(0, position).split('\\n');\r\n    return lines.length;\r\n}\r\n\r\n// Additional test to validate specific known problematic files\r\ndescribe('Specific Dynamic Import Checks', () => {\r\n    it('should validate dynamic imports in key service files', () => {\r\n        const keyFilesToCheck = [\r\n            path.join(__dirname, '../src/services/inference/inference.ts'),\r\n            path.join(__dirname, '../src/controllers/SearchController.js'),\r\n            path.join(__dirname, '../src/controllers/ChatController.js'),\r\n            path.join(__dirname, '../src/services/scribe/scribe.js'),\r\n            path.join(__dirname, '../src/services/dreamer/dreamer.js'),\r\n            path.join(__dirname, '../src/services/refiner/refiner.js')\r\n        ];\r\n        \r\n        const errors = [];\r\n        \r\n        for (const filePath of keyFilesToCheck) {\r\n            if (fs.existsSync(filePath)) {\r\n                const content = fs.readFileSync(filePath, 'utf8');\r\n                const dynamicImports = findDynamicImports(content, filePath);\r\n                \r\n                for (const imp of dynamicImports) {\r\n                    if (!imp.importPath.endsWith('.js')) {\r\n                        errors.push({\r\n                            file: filePath,\r\n                            importStatement: imp.fullMatch,\r\n                            position: imp.position,\r\n                            message: `Dynamic import uses '${imp.extension}' extension but should use '.js' for ESM compatibility`\r\n                        });\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        if (errors.length > 0) {\r\n            console.error('\\n Specific Dynamic Import Validation Failed!');\r\n            console.error('Found issues in key service files:');\r\n            \r\n            for (const error of errors) {\r\n                console.error(`\\nFile: ${error.file}`);\r\n                console.error(`Line: ${getLineNumber(error.file, error.position)}`);\r\n                console.error(`Import: ${error.importStatement}`);\r\n                console.error(`Issue: ${error.message}`);\r\n            }\r\n            \r\n            throw new Error(`${errors.length} dynamic import(s) in key files need to be updated`);\r\n        }\r\n        \r\n        console.log(` All key service files validated successfully!`);\r\n    });\r\n});"
    tokens: 2557
    size: 7471
  - path: engine\tests\suite.js
    content: "/**\r\n * ECE Test Suite\r\n * \r\n * Verifies core API functionality:\r\n * - Health endpoint\r\n * - Ingestion pipeline\r\n * - Search/Retrieval\r\n * - Scribe (Markovian State)\r\n * \r\n * Run: npm test (or node tests/suite.js)\r\n */\r\n\r\nconst BASE_URL = process.env.ECE_URL || 'http://localhost:3000';\r\n\r\n// Test results tracking\r\nlet passed = 0;\r\nlet failed = 0;\r\n\r\n/**\r\n * Test runner with pretty output\r\n */\r\nasync function test(name, fn) {\r\n    try {\r\n        process.stdout.write(`  ${name}... `);\r\n        await fn();\r\n        console.log(' PASS');\r\n        passed++;\r\n    } catch (e) {\r\n        console.log(' FAIL');\r\n        console.error(`      ${e.message}`);\r\n        failed++;\r\n    }\r\n}\r\n\r\n// Shim for ESM __dirname if needed\r\nimport { fileURLToPath } from 'url';\r\nimport { dirname } from 'path';\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = dirname(__filename);\r\n\r\n/**\r\n * Assert helper\r\n */\r\nfunction assert(condition, message) {\r\n    if (!condition) throw new Error(message || 'Assertion failed');\r\n}\r\n\r\n/**\r\n * Main test suite\r\n */\r\nasync function runSuite() {\r\n    console.log('\\n');\r\n    console.log('     ECE TEST SUITE                     ');\r\n    console.log('\\n');\r\n    console.log(`Target: ${BASE_URL}\\n`);\r\n\r\n    // \r\n    // SECTION 1: Core Health\r\n    // \r\n    console.log(' Core Health ');\r\n\r\n    await test('Health Endpoint', async () => {\r\n        const res = await fetch(`${BASE_URL}/health`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'Sovereign', `Unexpected status: ${json.status}`);\r\n    });\r\n\r\n    await test('Models List', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/models`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const models = await res.json();\r\n        assert(Array.isArray(models), 'Expected array of models');\r\n    });\r\n\r\n    // \r\n    // SECTION 2: Ingestion Pipeline\r\n    // \r\n    console.log('\\n Ingestion Pipeline ');\r\n\r\n    const testId = `test_${Date.now()}`;\r\n    const testContent = `ECE Test Memory: ${testId}. The secret code is ALPHA_BRAVO.`;\r\n\r\n    await test('Ingest Memory', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/ingest`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                content: testContent,\r\n                source: 'Test Suite',\r\n                type: 'test',\r\n                buckets: ['test', 'verification']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'success', `Ingest failed: ${JSON.stringify(json)}`);\r\n    });\r\n\r\n    // Brief pause for consistency (increased to 1500ms for FTS indexing/flush)\r\n    await new Promise(r => setTimeout(r, 1500));\r\n\r\n    // \r\n    // SECTION 3: Retrieval\r\n    // \r\n    console.log('\\n Retrieval ');\r\n\r\n    await test('Search by ID', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: testId,\r\n                buckets: ['test']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // Log response if failure suspected\r\n        if (!json.context || !json.context.includes(testId)) {\r\n            console.log('     [DEBUG] Search by ID Response:', JSON.stringify(json).substring(0, 200));\r\n        }\r\n        assert(json.context && json.context.includes(testId), 'Test memory not found in search results');\r\n    });\r\n\r\n    await test('Search by Content', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: 'ALPHA_BRAVO',\r\n                buckets: ['test']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.context && json.context.includes('ALPHA_BRAVO'), 'Secret code not found');\r\n    });\r\n\r\n    await test('Bucket Filtering', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({\r\n                query: testId,\r\n                buckets: ['nonexistent_bucket']\r\n            })\r\n        });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // Should NOT find results in wrong bucket\r\n        const found = json.context && json.context.includes(testId);\r\n        assert(!found, 'Should not find test memory in wrong bucket');\r\n    });\r\n\r\n    // \r\n    // SECTION 4: Scribe (Markovian State)\r\n    // \r\n    console.log('\\n Scribe (Markovian State) ');\r\n\r\n    await test('Get State (Empty)', async () => {\r\n        // Clear first\r\n        await fetch(`${BASE_URL}/v1/scribe/state`, { method: 'DELETE' });\r\n\r\n        const res = await fetch(`${BASE_URL}/v1/scribe/state`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        // State might be null or have previous data - just check structure\r\n        assert('state' in json, 'Missing state field');\r\n    });\r\n\r\n    await test('Clear State', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/scribe/state`, { method: 'DELETE' });\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const json = await res.json();\r\n        assert(json.status === 'cleared' || json.status === 'error', 'Unexpected response');\r\n    });\r\n\r\n    // \r\n    // SECTION 5: Buckets\r\n    // \r\n    console.log('\\n Buckets ');\r\n\r\n    await test('List Buckets', async () => {\r\n        const res = await fetch(`${BASE_URL}/v1/buckets`);\r\n        assert(res.ok, `Status ${res.status}`);\r\n        const buckets = await res.json();\r\n        assert(Array.isArray(buckets), 'Expected array of buckets');\r\n        assert(buckets.includes('test'), 'Test bucket should exist');\r\n    });\r\n\r\n    // \r\n    // SECTION 6: Watchdog & Mirror Verification\r\n    // \r\n    console.log('\\n Watchdog & Mirror Verification ');\r\n\r\n    // NOTE: This test requires the engine to be running with access to NOTEBOOK_DIR\r\n    // We will attempt to write a file to the inbox and verify it appears in search\r\n    // and then after a dream, appears in the mirror.\r\n\r\n    await test('Watchdog Ingestion', async () => {\r\n        // 1. Create a dummy file in the inbox\r\n        // We need to know where the inbox is. \r\n        // We can't easily import 'path' or config here if we want to be a standalone test suite\r\n        // relying only on API. BUT, we are running in the same environment likely.\r\n        // Let's assume we can use 'fs' and 'path' if we import them.\r\n\r\n        // Dynamic import for fs/path to avoid top-level issues if running in browser-like environment (though this is node)\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n        const os = await import('os');\r\n\r\n        // Resolve Notebook Dir - this is tricky without config.\r\n        // We'll rely on the user's setup effectively matching what we expect.\r\n        // Test suite is running in engine/tests/\r\n        // __dirname is .../engine/tests\r\n        // .. -> engine\r\n        // .. -> ECE_Core\r\n        // .. -> Projects\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const INBOX_DIR = path.join(NOTEBOOK_DIR, 'inbox');\r\n\r\n        if (!fs.existsSync(INBOX_DIR)) {\r\n            // Create it if missing (recovery)\r\n            fs.mkdirSync(INBOX_DIR, { recursive: true });\r\n        }\r\n\r\n        const uniqueId = `watchdog_test_${Date.now()}`;\r\n        const filePath = path.join(INBOX_DIR, `${uniqueId}.txt`);\r\n        const fileContent = `This is a watchdog test file. ID: ${uniqueId}`;\r\n\r\n        await fs.promises.writeFile(filePath, fileContent);\r\n\r\n        // Wait for Watchdog to pick it up (debounce is small but depends on poll)\r\n        // Give it 2 seconds\r\n        await new Promise(r => setTimeout(r, 2000));\r\n\r\n        // Search for it\r\n        let found = false;\r\n        let attempts = 0;\r\n        while (!found && attempts < 3) {\r\n            const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n                method: 'POST',\r\n                headers: { 'Content-Type': 'application/json' },\r\n                body: JSON.stringify({\r\n                    query: uniqueId,\r\n                    buckets: ['inbox'] // It should be in 'inbox' bucket\r\n                })\r\n            });\r\n            const json = await res.json();\r\n            if (json.context && json.context.includes(uniqueId)) {\r\n                found = true;\r\n            } else {\r\n                await new Promise(r => setTimeout(r, 1000));\r\n                attempts++;\r\n            }\r\n        }\r\n\r\n        assert(found, `Watchdog failed to ingest file ${uniqueId}`);\r\n\r\n        // Cleanup input file\r\n        await fs.promises.unlink(filePath);\r\n    });\r\n\r\n    await test('Mirror Protocol', async () => {\r\n        // Trigger Dream\r\n        const res = await fetch(`${BASE_URL}/v1/dream`, { method: 'POST' });\r\n        assert(res.ok, `Dream request failed: ${res.status}`);\r\n\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const MIRROR_DIR = path.join(NOTEBOOK_DIR, 'mirrored_brain');\r\n        const inboxMirror = path.join(MIRROR_DIR, 'inbox'); // Bucket is likely 'inbox'\r\n        const year = new Date().getFullYear().toString();\r\n        const yearDir = path.join(inboxMirror, year);\r\n\r\n        // Verification might be flaky if dream queue is slow, but we awaited the response which awaits the dream\r\n        // Check for ANY file in recent mirror\r\n        if (fs.existsSync(yearDir)) {\r\n            const files = await fs.promises.readdir(yearDir);\r\n            assert(files.length >= 0, 'Directory exists');\r\n            if (files.length > 0) console.log(`      Verified ${files.length} mirrored memories.`);\r\n        } else {\r\n            console.log('      Mirror directory not yet created (acceptable if no new memories processed)');\r\n        }\r\n    });\r\n\r\n    // \r\n    // SECTION 7: Semantic Decompression (Atomizer)\r\n    // \r\n    console.log('\\n Semantic Decompression (Atomizer) ');\r\n\r\n    await test('Atomizer splitting', async () => {\r\n        const fs = await import('fs');\r\n        const path = await import('path');\r\n        const NOTEBOOK_DIR = path.resolve(path.join(__dirname, '..', '..', '..', 'notebook'));\r\n        const INBOX_DIR = path.join(NOTEBOOK_DIR, 'inbox');\r\n\r\n        const atomId = `atom_test_${Date.now()}`;\r\n        const filePath = path.join(INBOX_DIR, `${atomId}.md`);\r\n        // Create 3 paragraphs -> Should be 3 atoms\r\n        const content = `Block 1: ${atomId}.\\n\\nBlock 2: ${atomId} continued.\\n\\nBlock 3: ${atomId} ending.`;\r\n\r\n        await fs.promises.writeFile(filePath, content);\r\n        await new Promise(r => setTimeout(r, 2000)); // Wait for Watchdog\r\n\r\n        // Search should return 3 results or we check context\r\n        const res = await fetch(`${BASE_URL}/v1/memory/search`, {\r\n            method: 'POST',\r\n            headers: { 'Content-Type': 'application/json' },\r\n            body: JSON.stringify({ query: atomId, buckets: ['inbox'] })\r\n        });\r\n        const json = await res.json();\r\n\r\n        // This is a rough check. Ideally we'd inspect the DB structure directly or backup\r\n        // But if we find the content, ingestion worked.\r\n        assert(json.context && json.context.includes(atomId), 'Atom content not found');\r\n\r\n        // Cleanup\r\n        await fs.promises.unlink(filePath);\r\n    });\r\n\r\n    // \r\n    // SECTION 8: Abstraction Pyramid\r\n    // \r\n    console.log('\\n Abstraction Pyramid ');\r\n\r\n    await test('Dreamer / Abstraction', async () => {\r\n        // Trigger Dream again to process new atoms\r\n        const res = await fetch(`${BASE_URL}/v1/dream`, { method: 'POST' });\r\n        assert(res.ok, 'Dream failed');\r\n        const json = await res.json();\r\n\r\n        // Check \"updated\" or \"analyzed\" count\r\n        if (json.analyzed > 0) {\r\n            console.log(`      Analyzed ${json.analyzed} memories.`);\r\n        }\r\n        // Use Backup API to inspect for 'summary_node' if possible, or just trust the dream status.\r\n        // If we implement 'GET /v1/backup', we could check it.\r\n        // For now, status Verified.\r\n        assert(json.status === 'success', 'Dream status not success');\r\n    });\r\n\r\n    // \r\n    // RESULTS\r\n    // \r\n    console.log('\\n');\r\n    console.log(`  Results: ${passed} passed, ${failed} failed`.padEnd(41) + '');\r\n    console.log('\\n');\r\n\r\n    process.exit(failed > 0 ? 1 : 0);\r\n}\r\n\r\n// Run\r\nrunSuite().catch(e => {\r\n    console.error('Suite crashed:', e);\r\n    process.exit(1);\r\n});\r\n"
    tokens: 4951
    size: 16157
  - path: engine\tests\test_cozo_aggr.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testAggr() {\r\n    await db.init();\r\n\r\n    // Variant 4: :group syntax\r\n    try {\r\n        console.log('Testing Variant 4: :group tag, count() -> tag_count');\r\n        const q4 = '?[tag, tag_count] := *memory{tags}, tag in tags, :group tag, count() -> tag_count :sort -tag_count :limit 5';\r\n        const r4 = await db.run(q4);\r\n        console.log('Result 4:', r4.rows);\r\n    } catch (e) {\r\n        console.error('Variant 4 Failed:', e);\r\n    }\r\n\r\n    process.exit(0);\r\n}\r\n\r\ntestAggr();\r\n"
    tokens: 205
    size: 548
  - path: engine\tests\test_cozo_headers.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testHead() {\r\n    await db.init();\r\n    try {\r\n        const q = '?[tag, count()] := *memory{tags}, tag in tags :limit 1';\r\n        const r = await db.run(q);\r\n        console.log('Headers:', r.headers);\r\n        console.log('Rows:', r.rows);\r\n    } catch (e) {\r\n        console.error('Failed:', e);\r\n    }\r\n    process.exit(0);\r\n}\r\n\r\ntestHead();\r\n"
    tokens: 153
    size: 408
  - path: engine\tests\test_dreamer_optimization.ts
    content: "\r\nimport { dream } from '../src/services/dreamer/dreamer.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testDreamer() {\r\n    console.log('--- Testing Dreamer Performance Optimizations ---');\r\n    await db.init();\r\n\r\n    const startTime = Date.now();\r\n    try {\r\n        const result = await dream();\r\n        const duration = (Date.now() - startTime) / 1000;\r\n\r\n        console.log('--- Dream Cycle Results ---');\r\n        console.log(`Status: ${result.status}`);\r\n        console.log(`Analyzed: ${result.analyzed}`);\r\n        console.log(`Updated: ${result.updated}`);\r\n        console.log(`Duration: ${duration.toFixed(2)}s`);\r\n\r\n        if (result.status === 'success') {\r\n            console.log(' PASS: Dream cycle completed successfully.');\r\n        } else {\r\n            console.log(` INFO: Dream cycle status: ${result.status}`);\r\n        }\r\n    } catch (e) {\r\n        console.error(' FAIL: Dream cycle crashed:', e);\r\n    }\r\n\r\n    process.exit(0);\r\n}\r\n\r\ntestDreamer();\r\n"
    tokens: 352
    size: 1008
  - path: engine\tests\test_infection_generator.ts
    content: "import { runInfectionLoop } from '../src/services/tags/infector.ts';\r\nimport { db } from '../src/core/db.ts';\r\n\r\nasync function test() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    console.log(\"Creating dummy atoms...\");\r\n    // Insert test data\r\n    // We intentionally omit 'tags' or provide empty tags\r\n    try {\r\n        await db.run(`\r\n            ?[id, content, tags, type, timestamp, source, internal_monologue, complexity, embedding, last_accessed, access_count, status] <- [\r\n                ['test_1', 'I love TypeScript and CozoDB very much.', [], 'text', 123456, 'manual', '', 1, [], 123, 0, 'active'],\r\n                ['test_2', 'Recursion is dangerous in Node.js, use Generators instead.', [], 'text', 123457, 'manual', '', 1, [], 123, 0, 'active'],\r\n                ['test_3', 'This text has no keywords.', [], 'text', 123458, 'manual', '', 1, [], 123, 0, 'active']\r\n            ]\r\n            :insert memory {id, content, tags, type, timestamp, source, internal_monologue, complexity, embedding, last_accessed, access_count, status}\r\n        `);\r\n    } catch (e: any) {\r\n        console.warn(\"Insert warning (might already exist):\", e.message);\r\n    }\r\n\r\n    console.log(\"Running Infection...\");\r\n    await runInfectionLoop();\r\n\r\n    console.log(\"Verifying Results...\");\r\n\r\n    const res1 = await db.run(\"?[tags] := *memory{id: 'test_1', tags}\");\r\n    console.log(\"Test 1 (TypeScript, CozoDB):\", res1.rows[0]);\r\n\r\n    const res2 = await db.run(\"?[tags] := *memory{id: 'test_2', tags}\");\r\n    console.log(\"Test 2 (Recursion, Generators):\", res2.rows[0]);\r\n\r\n    const res3 = await db.run(\"?[tags] := *memory{id: 'test_3', tags}\");\r\n    console.log(\"Test 3 (Empty):\", res3.rows[0]);\r\n}\r\n\r\ntest();\r\n"
    tokens: 630
    size: 1738
  - path: engine\tests\test_ingest_atom_debug.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\nimport config from '../src/config/index.js';\r\n\r\nasync function runTest() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    // Raw Failing Atom (Copied from Debug Log)\r\n    // \"atom_3449feb29c1ea488\", 1768844295178, \"Block 2...\", \"inbox\\\\atom_test...\", \"beacbd...\", 1, \"text\", \"3449fe...\", [\"inbox\"], [], [], \"external\", [0.1...]\r\n\r\n    // Construct exactly as ingestAtoms does\r\n    // Schema: id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding\r\n\r\n    const atomData = [\r\n        \"atom_3449feb29c1ea488\",\r\n        1768844295178,\r\n        \"Block 2: atom_test_1768659429156 continued.\",\r\n        \"inbox\\\\atom_test_1768659429156.md\",\r\n        \"beacbd2a7598600c6acb4fe2e7c36323\",\r\n        1,\r\n        \"text\",\r\n        \"3449feb29c1ea488\",\r\n        [\"inbox\"],\r\n        [], // epochs\r\n        [], // tags\r\n        \"external\",\r\n        new Array(768).fill(0.1)\r\n    ];\r\n\r\n    const chunk = [atomData];\r\n\r\n    console.log(\"Attempting Insert...\");\r\n    try {\r\n        await db.run(`\r\n            ?[id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding] <- $data\r\n            :put memory {id, timestamp, content, source, source_id, sequence, type, hash, buckets, epochs, tags, provenance, embedding}\r\n        `, { data: chunk });\r\n        console.log(\"SUCCESS: Insert worked!\");\r\n    } catch (e: any) {\r\n        console.error(\"FAILURE: Insert failed:\", e.message);\r\n    }\r\n}\r\n\r\nrunTest().catch(console.error);\r\n"
    tokens: 545
    size: 1578
  - path: engine\tests\test_mirror_trigger.ts
    content: "\r\nimport { createMirror } from '../src/services/mirror/mirror.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testMirror() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    console.log(\"Triggering Mirror 2.0...\");\r\n    await createMirror();\r\n    console.log(\"Mirroring complete.\");\r\n}\r\n\r\ntestMirror().catch(console.error);\r\n"
    tokens: 132
    size: 361
  - path: engine\tests\test_query_expansion.ts
    content: "\r\nimport { db } from '../src/core/db.js';\r\nimport { executeSearch, getGlobalTags } from '../src/services/search/search.js';\r\nimport { ingestContent } from '../src/services/ingest/ingest.js';\r\n\r\nasync function testQueryExpansion() {\r\n    console.log('--- Testing Intelligent Query Expansion ---');\r\n    await db.init();\r\n\r\n    // 1. Setup Grounding Tags\r\n    console.log('Step 1: Ingesting tagged data for grounding...');\r\n    const data = [\r\n        { content: \"The ECE_Core engine uses CozoDB for efficient graph storage.\", source: \"docs/architecture.md\", tags: [\"ECE_Core\", \"CozoDB\", \"Architecture\"] },\r\n        { content: \"Tag-Walker provides associative search without vectors.\", source: \"docs/search.md\", tags: [\"Tag-Walker\", \"Search\", \"Graph\"] },\r\n        { content: \"Mirror 2.0 projects the brain onto the filesystem.\", source: \"docs/mirror.md\", tags: [\"Mirror\", \"Filesystem\", \"Projection\"] }\r\n    ];\r\n\r\n    for (const item of data) {\r\n        await ingestContent(item.content, item.source, 'text', ['tech_bucket']);\r\n        // Manual tag update as current ingestContent might not use all tags provided in this array format if not mapped\r\n        // Actually, let's assume ingestContent handles it or we'll check global tags\r\n    }\r\n\r\n    const tags = await getGlobalTags(10);\r\n    console.log('Current System Tags:', tags);\r\n\r\n    // 2. Perform Expanded Search\r\n    console.log('Step 2: Performing complex query search...');\r\n    const complexQuery = \"How does the core engine handle persistent graph storage and filesystem projection?\";\r\n\r\n    // Note: This will trigger expandQuery which uses runSideChannel (GLM)\r\n    const result = await executeSearch(complexQuery, 'tech_bucket');\r\n\r\n    console.log('--- Search Results ---');\r\n    console.log('Context Snippet:', result.context.substring(0, 500));\r\n    console.log('Result Count:', result.results.length);\r\n\r\n    if (result.results.length > 0) {\r\n        console.log(' PASS: Retrieval successful with query expansion.');\r\n    } else {\r\n        console.log(' FAIL: No results found for complex query.');\r\n    }\r\n\r\n    process.exit(0);\r\n}\r\n\r\ntestQueryExpansion().catch(err => {\r\n    console.error(err);\r\n    process.exit(1);\r\n});\r\n"
    tokens: 805
    size: 2200
  - path: engine\tests\test_search_walker.ts
    content: "\r\nimport { executeSearch } from '../src/services/search/search.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testSearch() {\r\n    console.log(\"Initializing DB...\");\r\n    await db.init();\r\n\r\n    const query = \"atom test\";\r\n    console.log(`Running Search for: \"${query}\"`);\r\n\r\n    const results = await executeSearch(query);\r\n\r\n    console.log(\"\\n--- Search Results ---\");\r\n    console.log(`Context Length: ${results.context.length}`);\r\n    console.log(`Result Count: ${results.results.length}`);\r\n\r\n    results.results.slice(0, 5).forEach((r, i) => {\r\n        console.log(`\\n[${i + 1}] Score: ${r.score.toFixed(2)} | Provenance: ${r.provenance}`);\r\n        console.log(`Source: ${r.source}`);\r\n        console.log(`Snippet: ${r.content.substring(0, 100)}...`);\r\n    });\r\n}\r\n\r\ntestSearch().catch(console.error);\r\n"
    tokens: 305
    size: 829
  - path: engine\tests\test_tag_infection_v2.ts
    content: "\r\nimport { runDiscovery } from '../src/services/tags/discovery.js';\r\nimport { runInfection } from '../src/services/tags/infector.js';\r\nimport { db } from '../src/core/db.js';\r\n\r\nasync function testInfectionProtocol() {\r\n    console.log('--- Testing Tag Infection Protocol (Standard 068) ---');\r\n    await db.init();\r\n\r\n    try {\r\n        // 1. Run Discovery (The Teacher)\r\n        console.log('Step 1: Learning from data...');\r\n        const discovered = await runDiscovery(5);\r\n        console.log(`Discovered tags: ${discovered.join(', ')}`);\r\n\r\n        // 2. Run Infection (The Student)\r\n        console.log('Step 2: Infecting the graph...');\r\n        const result = await runInfection();\r\n        console.log(`Infection results: ${result.atomsUpdated} atoms updated in ${result.durationMs}ms`);\r\n\r\n        if (discovered.length >= 0) {\r\n            console.log(' PASS: Teacher-Student loop completed.');\r\n        }\r\n    } catch (e) {\r\n        console.error(' FAIL: Infection Protocol error:', e);\r\n    }\r\n\r\n    process.exit(0);\r\n}\r\n\r\ntestInfectionProtocol();\r\n"
    tokens: 380
    size: 1070
  - path: engine\test_cozo_fts.js
    content: "const { CozoDb } = require('cozo-node');\r\n\r\nasync function test() {\r\n    const db = new CozoDb('rocksdb', 'test_fts.db'); // Use rocksdb\r\n    try {\r\n        await db.run(':create test_mem {id: String => content: String}');\r\n        await db.run('::fts create test_mem:idx {extractor: content, tokenizer: Simple}');\r\n        await db.run('?[id, content] <- [[\"1\", \"hello world\"], [\"2\", \"foo bar\"]] :put test_mem');\r\n        \r\n        console.log(\"Testing Option 1 (index.js style): ?[id, score] := ~test_mem:idx{content | query: 'hello', k: 1, bind_score: s}, score = s\");\r\n        try {\r\n            const res1 = await db.run(\"?[id, score] := ~test_mem:idx{content | query: 'hello', k: 1, bind_score: s}, score = s\");\r\n            console.log(\"Option 1 result:\", JSON.stringify(res1));\r\n        } catch (e) {\r\n            console.log(\"Option 1 failed:\", e.message);\r\n        }\r\n\r\n        console.log(\"Testing Option 11: ?[id] := *test_mem{id, content}, ~test_mem:idx{content | query: 'hello', k: 1}\");\r\n        try {\r\n            const res11 = await db.run(\"?[id] := *test_mem{id, content}, ~test_mem:idx{content | query: 'hello', k: 1}\");\r\n            console.log(\"Option 11 result:\", JSON.stringify(res11));\r\n        } catch (e) {\r\n            console.log(\"Option 11 failed:\", e.message);\r\n        }\r\n\r\n        console.log(\"\\nTesting Option 3: ?[id] := ~test_mem:idx{content | query: 'hello', k: 1}, *test_mem{id}\");\r\n        try {\r\n            const res3 = await db.run(\"?[id] := ~test_mem:idx{content | query: 'hello', k: 1}, *test_mem{id}\");\r\n            console.log(\"Option 3 result:\", JSON.stringify(res3));\r\n        } catch (e) {\r\n            console.log(\"Option 3 failed:\", e.message);\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(\"Setup failed:\", e);\r\n    }\r\n}\r\n\r\ntest();\r\n"
    tokens: 663
    size: 1793
  - path: engine\test_db_syntax.js
    content: "\r\nimport { CozoDb } from 'cozo-node';\r\nimport fs from 'fs';\r\n\r\nasync function test() {\r\n    if (fs.existsSync('./test.db')) {\r\n        fs.rmSync('./test.db', { recursive: true, force: true });\r\n    }\r\n    const db = new CozoDb('rocksdb', './test.db');\r\n\r\n    try {\r\n        await db.run(`\r\n            :create memory {\r\n                id: String\r\n                =>\r\n                embedding: <F32; 4>\r\n            }\r\n        `);\r\n\r\n        console.log(\"Attempt 1: FTS-like syntax ::hnsw create idx { config }\");\r\n        try {\r\n            await db.run(`\r\n                ::hnsw create idx_hnsw {\r\n                    fields: [embedding],\r\n                    dim: 4,\r\n                    m: 50,\r\n                    ef_construction: 200,\r\n                    dtype: 'f32'\r\n                }\r\n            `);\r\n            console.log(\"SUCCESS: Attempt 1\");\r\n            return;\r\n        } catch (e) {\r\n            console.log(\"FAILED Attempt 1:\", e.message);\r\n        }\r\n\r\n        console.log(\"Attempt 2: keys as strings?\");\r\n        try {\r\n            await db.run(`\r\n                ::hnsw create idx_hnsw {\r\n                    \"fields\": [\"embedding\"],\r\n                    \"dim\": 4,\r\n                    \"m\": 50,\r\n                    \"ef_construction\": 200,\r\n                    \"dtype\": \"f32\"\r\n                }\r\n            `);\r\n            console.log(\"SUCCESS: Attempt 2\");\r\n            return;\r\n        } catch (e) {\r\n            console.log(\"FAILED Attempt 2:\", e.message);\r\n        }\r\n\r\n    } catch (e) {\r\n        console.error(\"Setup failed:\", e);\r\n    }\r\n}\r\ntest();\r\n"
    tokens: 529
    size: 1583
  - path: engine\test_regex.js
    content: "const { CozoDb } = require('cozo-lib-node'); const db = new CozoDb(); (async () => { await db.run('?[] <- [[\\'foo\\']]:put t{a}'); try { await db.run('?[a] := *t{a}, regex(\\'f\\', a)'); console.log('regex works'); } catch(e) { console.log('regex failed', e.message); } try { await db.run('?[a] := *t{a}, regex_match(\\'f\\', a)'); console.log('regex_match works'); } catch(e) { console.log('regex_match failed', e.message); } })()\r\n"
    tokens: 169
    size: 428
  - path: engine\tsconfig.json
    content: "{\r\n  \"compilerOptions\": {\r\n    \"target\": \"ES2022\",\r\n    \"module\": \"ESNext\",\r\n    \"moduleResolution\": \"node\",\r\n    \"esModuleInterop\": true,\r\n    \"allowSyntheticDefaultImports\": true,\r\n    \"strict\": true,\r\n    \"skipLibCheck\": true,\r\n    \"forceConsistentCasingInFileNames\": true,\r\n    \"outDir\": \"./dist\",\r\n    \"rootDir\": \"./src\",\r\n    \"resolveJsonModule\": true,\r\n    \"declaration\": true,\r\n    \"declarationMap\": true,\r\n    \"sourceMap\": true,\r\n    \"removeComments\": false,\r\n    \"noImplicitAny\": true,\r\n    \"strictNullChecks\": true,\r\n    \"strictFunctionTypes\": true,\r\n    \"noImplicitThis\": true,\r\n    \"noImplicitReturns\": true,\r\n    \"alwaysStrict\": true,\r\n    \"noUnusedLocals\": true,\r\n    \"noUnusedParameters\": true,\r\n    \"exactOptionalPropertyTypes\": false,\r\n    \"noImplicitOverride\": true,\r\n    \"noPropertyAccessFromIndexSignature\": true\r\n  },\r\n  \"include\": [\r\n    \"src/**/*\"\r\n  ],\r\n  \"exclude\": [\r\n    \"node_modules\",\r\n    \"dist\",\r\n    \"tests\"\r\n  ]\r\n}"
    tokens: 296
    size: 948
  - path: engine\user_settings.json
    content: "{\r\n    \"llm\": {\r\n        \"model_dir\": \"../../models\",\r\n        \"chat_model\": \"glm-edge-1.5b-chat.Q5_K_M.gguf\",\r\n        \"task_model\": \"Qwen3-4B-Function-Calling-Pro.gguf\",\r\n        \"gpu_layers\": 11,\r\n        \"ctx_size\": 8192\r\n    },\r\n    \"dreamer\": {\r\n        \"enabled\": true,\r\n        \"schedule\": \"0 3 * * *\"\r\n    }\r\n}"
    tokens: 108
    size: 319
  - path: frontend\.gitignore
    content: "# Logs\r\nlogs\r\n*.log\r\nnpm-debug.log*\r\nyarn-debug.log*\r\nyarn-error.log*\r\npnpm-debug.log*\r\nlerna-debug.log*\r\n\r\nnode_modules\r\ndist\r\ndist-ssr\r\n*.local\r\n\r\n# Editor directories and files\r\n.vscode/*\r\n!.vscode/extensions.json\r\n.idea\r\n.DS_Store\r\n*.suo\r\n*.ntvs*\r\n*.njsproj\r\n*.sln\r\n*.sw?\r\n"
    tokens: 108
    size: 277
  - path: frontend\eslint.config.js
    content: "import js from '@eslint/js'\r\nimport globals from 'globals'\r\nimport reactHooks from 'eslint-plugin-react-hooks'\r\nimport reactRefresh from 'eslint-plugin-react-refresh'\r\nimport tseslint from 'typescript-eslint'\r\nimport { defineConfig, globalIgnores } from 'eslint/config'\r\n\r\nexport default defineConfig([\r\n  globalIgnores(['dist']),\r\n  {\r\n    files: ['**/*.{ts,tsx}'],\r\n    extends: [\r\n      js.configs.recommended,\r\n      tseslint.configs.recommended,\r\n      reactHooks.configs.flat.recommended,\r\n      reactRefresh.configs.vite,\r\n    ],\r\n    languageOptions: {\r\n      ecmaVersion: 2020,\r\n      globals: globals.browser,\r\n    },\r\n  },\r\n])\r\n"
    tokens: 222
    size: 639
  - path: frontend\index.html
    content: "<!doctype html>\r\n<html lang=\"en\">\r\n  <head>\r\n    <meta charset=\"UTF-8\" />\r\n    <link rel=\"icon\" type=\"image/svg+xml\" href=\"/vite.svg\" />\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\r\n    <title>frontend</title>\r\n  </head>\r\n  <body>\r\n    <div id=\"root\"></div>\r\n    <script type=\"module\" src=\"/src/main.tsx\"></script>\r\n  </body>\r\n</html>\r\n"
    tokens: 143
    size: 370
  - path: frontend\package.json
    content: "{\r\n  \"name\": \"frontend\",\r\n  \"private\": true,\r\n  \"version\": \"0.0.0\",\r\n  \"type\": \"module\",\r\n  \"scripts\": {\r\n    \"dev\": \"vite\",\r\n    \"build\": \"tsc -b && vite build\",\r\n    \"lint\": \"eslint .\",\r\n    \"preview\": \"vite preview\"\r\n  },\r\n  \"dependencies\": {\r\n    \"react\": \"^19.2.0\",\r\n    \"react-dom\": \"^19.2.0\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@eslint/js\": \"^9.39.1\",\r\n    \"@types/node\": \"^24.10.1\",\r\n    \"@types/react\": \"^19.2.5\",\r\n    \"@types/react-dom\": \"^19.2.3\",\r\n    \"@vitejs/plugin-react\": \"^5.1.1\",\r\n    \"eslint\": \"^9.39.1\",\r\n    \"eslint-plugin-react-hooks\": \"^7.0.1\",\r\n    \"eslint-plugin-react-refresh\": \"^0.4.24\",\r\n    \"globals\": \"^16.5.0\",\r\n    \"typescript\": \"~5.9.3\",\r\n    \"typescript-eslint\": \"^8.46.4\",\r\n    \"vite\": \"npm:rolldown-vite@7.2.5\"\r\n  },\r\n  \"overrides\": {\r\n    \"vite\": \"npm:rolldown-vite@7.2.5\"\r\n  }\r\n}\r\n"
    tokens: 312
    size: 819
  - path: frontend\README.md
    content: "# React + TypeScript + Vite\r\n\r\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\r\n\r\nCurrently, two official plugins are available:\r\n\r\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh\r\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\r\n\r\n## React Compiler\r\n\r\nThe React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).\r\n\r\n## Expanding the ESLint configuration\r\n\r\nIf you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:\r\n\r\n```js\r\nexport default defineConfig([\r\n  globalIgnores(['dist']),\r\n  {\r\n    files: ['**/*.{ts,tsx}'],\r\n    extends: [\r\n      // Other configs...\r\n\r\n      // Remove tseslint.configs.recommended and replace with this\r\n      tseslint.configs.recommendedTypeChecked,\r\n      // Alternatively, use this for stricter rules\r\n      tseslint.configs.strictTypeChecked,\r\n      // Optionally, add this for stylistic rules\r\n      tseslint.configs.stylisticTypeChecked,\r\n\r\n      // Other configs...\r\n    ],\r\n    languageOptions: {\r\n      parserOptions: {\r\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\r\n        tsconfigRootDir: import.meta.dirname,\r\n      },\r\n      // other options...\r\n    },\r\n  },\r\n])\r\n```\r\n\r\nYou can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:\r\n\r\n```js\r\n// eslint.config.js\r\nimport reactX from 'eslint-plugin-react-x'\r\nimport reactDom from 'eslint-plugin-react-dom'\r\n\r\nexport default defineConfig([\r\n  globalIgnores(['dist']),\r\n  {\r\n    files: ['**/*.{ts,tsx}'],\r\n    extends: [\r\n      // Other configs...\r\n      // Enable lint rules for React\r\n      reactX.configs['recommended-typescript'],\r\n      // Enable lint rules for React DOM\r\n      reactDom.configs.recommended,\r\n    ],\r\n    languageOptions: {\r\n      parserOptions: {\r\n        project: ['./tsconfig.node.json', './tsconfig.app.json'],\r\n        tsconfigRootDir: import.meta.dirname,\r\n      },\r\n      // other options...\r\n    },\r\n  },\r\n])\r\n```\r\n"
    tokens: 967
    size: 2628
  - path: frontend\src\App.css
    content: "#root {\r\n  max-width: 1280px;\r\n  margin: 0 auto;\r\n  padding: 2rem;\r\n  text-align: center;\r\n}\r\n\r\n.logo {\r\n  height: 6em;\r\n  padding: 1.5em;\r\n  will-change: filter;\r\n  transition: filter 300ms;\r\n}\r\n.logo:hover {\r\n  filter: drop-shadow(0 0 2em #646cffaa);\r\n}\r\n.logo.react:hover {\r\n  filter: drop-shadow(0 0 2em #61dafbaa);\r\n}\r\n\r\n@keyframes logo-spin {\r\n  from {\r\n    transform: rotate(0deg);\r\n  }\r\n  to {\r\n    transform: rotate(360deg);\r\n  }\r\n}\r\n\r\n@media (prefers-reduced-motion: no-preference) {\r\n  a:nth-of-type(2) .logo {\r\n    animation: logo-spin infinite 20s linear;\r\n  }\r\n}\r\n\r\n.card {\r\n  padding: 2em;\r\n}\r\n\r\n.read-the-docs {\r\n  color: #888;\r\n}\r\n"
    tokens: 242
    size: 648
  - path: frontend\src\App.tsx
    content: "\r\nimport { useState, useEffect } from 'react';\r\nimport './index.css';\r\n\r\n// Simple Router (Single File for now for speed)\r\nconst Dashboard = () => (\r\n  <div className=\"flex-col-center\" style={{ height: '100%', justifyContent: 'center', alignItems: 'center', gap: '2rem' }}>\r\n    <h1 style={{ fontSize: '3rem', background: 'linear-gradient(to right, #fff, #646cff)', WebkitBackgroundClip: 'text', WebkitTextFillColor: 'transparent' }}>\r\n      Sovereign Context Engine\r\n    </h1>\r\n    <div style={{ display: 'flex', gap: '1rem' }}>\r\n      <button className=\"btn-primary\" onClick={() => window.location.hash = '#search'}>\r\n        Search Memories\r\n      </button>\r\n      <button className=\"btn-primary\" onClick={() => window.location.hash = '#chat'}>\r\n        Launch Chat\r\n      </button>\r\n    </div>\r\n  </div>\r\n);\r\n\r\nconst SearchPage = () => {\r\n  const [query, setQuery] = useState('');\r\n  const [results, setResults] = useState<any[]>([]);\r\n  const [context, setContext] = useState('');\r\n  const [loading, setLoading] = useState(false);\r\n  const [viewMode, setViewMode] = useState<'cards' | 'raw'>('cards');\r\n\r\n  // Feature 8/9/10 State\r\n  const [tokenBudget, setTokenBudget] = useState(2048);\r\n  const [activeMode, setActiveMode] = useState(false);\r\n  const [sovereignBias, setSovereignBias] = useState(true);\r\n  const [metadata, setMetadata] = useState<any>(null); // { tokenCount, filledPercent, atomCount }\r\n  const [scope, setScope] = useState<'all' | 'code' | 'docs'>('all'); // <--- NEW SCOPE STATE\r\n\r\n  // Feature 7 State\r\n  const [backupStatus, setBackupStatus] = useState('');\r\n\r\n  // Research Station State\r\n  const [showResearch, setShowResearch] = useState(false);\r\n  const [researchTab, setResearchTab] = useState<'search' | 'direct'>('search');\r\n  const [webQuery, setWebQuery] = useState('');\r\n  const [webResults, setWebResults] = useState<any[]>([]); // { title, link, snippet }\r\n  const [webSearching, setWebSearching] = useState(false);\r\n\r\n  // Debounce Logic for Live Mode\r\n  // Sync query to delay search\r\n  useEffect(() => {\r\n    if (!activeMode) return;\r\n    const timer = setTimeout(() => {\r\n      if (query.trim()) handleSearch();\r\n    }, 500); // 500ms debounce\r\n    return () => clearTimeout(timer);\r\n  }, [query, activeMode, tokenBudget, sovereignBias]);\r\n\r\n  const handleBackup = async () => {\r\n    setBackupStatus('Backing up...');\r\n    try {\r\n      const res = await fetch('/v1/backup', { method: 'POST' });\r\n      const data = await res.json();\r\n      setBackupStatus(`Backup Saved: ${data.filename}`);\r\n      setTimeout(() => setBackupStatus(''), 3000);\r\n    } catch (e) {\r\n      setBackupStatus('Backup Failed');\r\n    }\r\n  };\r\n\r\n  const handleQuarantine = async (id: string) => {\r\n    if (!confirm('Quarantine this atom? It will be tagged #manually_quarantined.')) return;\r\n\r\n    // Optimistic UI Update\r\n    setResults(prev => prev.filter(r => r.id !== id));\r\n    setMetadata((prev: any) => prev ? ({ ...prev, atomCount: prev.atomCount - 1 }) : null);\r\n\r\n    try {\r\n      await fetch(`/v1/atoms/${id}/quarantine`, { method: 'POST' });\r\n    } catch (e) {\r\n      console.error('Quarantine failed', e);\r\n      alert('Failed to quarantine atom server-side.');\r\n    }\r\n  };\r\n\r\n  const handleSearch = async () => {\r\n    if (!query.trim()) return;\r\n    setLoading(true);\r\n    setResults([]);\r\n    try {\r\n      const res = await fetch('/v1/memory/search', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({\r\n          query: scope === 'all' ? query : `${query} ${scope === 'code' ? '#code' : '#doc'}`, // <--- INJECT TAGS\r\n          // buckets: ['notebook'], // Removed to allow global search (inbox, journals, etc.)\r\n          max_chars: tokenBudget * 4, // Approx chars\r\n          token_budget: tokenBudget, // For backend slicer if supported\r\n          provenance: sovereignBias ? 'sovereign' : 'all'\r\n        })\r\n      });\r\n\r\n      const data = await res.json();\r\n\r\n      if (data.results) {\r\n        setResults(data.results);\r\n        setContext(data.context || '');\r\n        setMetadata(data.metadata); // Capture metadata\r\n      } else {\r\n        setResults([]);\r\n        setContext('No results found.');\r\n        setMetadata(null);\r\n      }\r\n\r\n    } catch (e) {\r\n      console.error(e);\r\n      setContext('Error searching memories.');\r\n    } finally {\r\n      setLoading(false);\r\n    }\r\n  };\r\n\r\n  const handleWebSearch = async () => {\r\n    if (!webQuery.trim()) return;\r\n    setWebSearching(true);\r\n    try {\r\n      const res = await fetch(`/v1/research/web-search?q=${encodeURIComponent(webQuery)}`);\r\n      const data = await res.json();\r\n      setWebResults(Array.isArray(data) ? data : []);\r\n    } catch (e) {\r\n      console.error(e);\r\n      alert('Research Search Failed');\r\n    } finally {\r\n      setWebSearching(false);\r\n    }\r\n  };\r\n\r\n  const handleSaveArticle = async (url: string) => {\r\n    try {\r\n      const res = await fetch('/v1/research/scrape', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({ url, category: 'article' })\r\n      });\r\n      const data = await res.json();\r\n      if (res.ok) alert(`Saved to Staging: ${data.title}`);\r\n      else alert(`Error: ${data.error}`);\r\n    } catch (e: any) {\r\n      alert(`Failed: ${e.message}`);\r\n    }\r\n  };\r\n\r\n  const copyContext = () => {\r\n    navigator.clipboard.writeText(context);\r\n  };\r\n\r\n  return (\r\n    <div className=\"glass-panel\" style={{ margin: '2rem', padding: '2rem', height: 'calc(100% - 4rem)', display: 'flex', flexDirection: 'column', gap: '1rem' }}>\r\n      <div style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>\r\n        <h2>Memory Search</h2>\r\n        {/* ... (Previous header content) ... */}\r\n\r\n\r\n\r\n\r\n\r\n        {/* Helper Controls */}\r\n        <div style={{ display: 'flex', gap: '0.5rem', alignItems: 'center' }}>\r\n          {/* Backup Button (Feature 7) */}\r\n          <button className=\"btn-primary\" onClick={handleBackup} style={{ fontSize: '0.8rem', padding: '0.4rem' }}>\r\n             {backupStatus || 'Backup'}\r\n          </button>\r\n\r\n          {/* Research Button (Triggers Modal) */}\r\n          <button\r\n            className=\"btn-primary\"\r\n            style={{ fontSize: '0.9rem', padding: '0.4rem 0.8rem', border: '1px solid var(--accent-primary)', color: 'white', background: 'var(--bg-tertiary)' }}\r\n            onClick={() => setShowResearch(true)}\r\n          >\r\n             Research\r\n          </button>\r\n\r\n\r\n\r\n\r\n          {/* Dream Button (Restored) */}\r\n          <button\r\n            className=\"btn-primary\"\r\n            style={{ background: 'rgba(100, 108, 255, 0.1)', border: '1px solid var(--accent-primary)', fontSize: '0.8rem', padding: '0.4rem' }}\r\n            onClick={async () => {\r\n              const btn = document.activeElement as HTMLButtonElement;\r\n              if (btn) btn.disabled = true;\r\n              try {\r\n                const res = await fetch('/v1/dream', { method: 'POST' });\r\n                const data = await res.json();\r\n                alert(`Dream Cycle Complete:\\nAnalyzed: ${data.analyzed}\\nUpdated: ${data.updated}`);\r\n              } catch (e) {\r\n                alert('Dream Failed');\r\n                console.error(e);\r\n              } finally {\r\n                if (btn) btn.disabled = false;\r\n              }\r\n            }}\r\n          >\r\n             Dream\r\n          </button>\r\n\r\n          {/* View Mode */}\r\n          <button className=\"btn-primary\" style={{ background: 'transparent', border: '1px solid var(--border-subtle)', fontSize: '0.8rem', padding: '0.4rem' }} onClick={() => setViewMode(viewMode === 'cards' ? 'raw' : 'cards')}>\r\n            {viewMode === 'cards' ? 'Raw' : 'Cards'}\r\n          </button>\r\n        </div>\r\n      </div>\r\n\r\n      {/* RAG IDE Controls (Features 8 & 9 & 10) */}\r\n      <div className=\"glass-panel\" style={{ padding: '1rem', display: 'flex', flexDirection: 'column', gap: '0.5rem', background: 'var(--bg-secondary)' }}>\r\n\r\n        {/* Scope Filters (New UI) */}\r\n        <div style={{ display: 'flex', gap: '0.5rem', marginBottom: '0.5rem' }}>\r\n          {\r\n            ['all', 'code', 'docs'].map(s => (\r\n              <button\r\n                key={s}\r\n                className=\"btn-primary\"\r\n                style={{\r\n                  fontSize: '0.8rem',\r\n                  padding: '0.3rem 0.8rem',\r\n                  background: scope === s ? 'var(--accent-primary)' : 'transparent',\r\n                  border: scope === s ? 'none' : '1px solid var(--border-subtle)',\r\n                  opacity: scope === s ? 1 : 0.7\r\n                }}\r\n                onClick={() => setScope(s as any)}\r\n              >\r\n                {s.toUpperCase()}\r\n              </button>\r\n            ))\r\n          }\r\n        </div>\r\n\r\n        <div style={{ display: 'flex', gap: '2rem', alignItems: 'center' }}>\r\n          {/* Active Mode Toggle */}\r\n          <label style={{ display: 'flex', gap: '0.5rem', alignItems: 'center', cursor: 'pointer' }}>\r\n            <input type=\"checkbox\" checked={activeMode} onChange={(e) => setActiveMode(e.target.checked)} />\r\n            <span style={{ fontSize: '0.9rem', fontWeight: 'bold', color: activeMode ? 'var(--accent-primary)' : 'var(--text-dim)' }}>\r\n               Live Search\r\n            </span>\r\n          </label>\r\n\r\n          {/* Sovereign Bias Toggle */}\r\n          <label style={{ display: 'flex', gap: '0.5rem', alignItems: 'center', cursor: 'pointer' }}>\r\n            <input type=\"checkbox\" checked={sovereignBias} onChange={(e) => setSovereignBias(e.target.checked)} />\r\n            <span style={{ fontSize: '0.9rem', color: sovereignBias ? '#FFD700' : 'var(--text-dim)' }}>\r\n               Sovereign Bias\r\n            </span>\r\n          </label>\r\n\r\n          {/* Budget Slider */}\r\n          <div style={{ flex: 1, display: 'flex', gap: '1rem', alignItems: 'center' }}>\r\n            <span style={{ fontSize: '0.8rem', whiteSpace: 'nowrap' }}>Budget: {tokenBudget} tokens</span>\r\n            <input\r\n              type=\"range\"\r\n              min=\"512\"\r\n              max=\"131072\"\r\n              step=\"512\"\r\n              value={tokenBudget}\r\n              onChange={(e) => setTokenBudget(parseInt(e.target.value))}\r\n              style={{ flex: 1 }}\r\n            />\r\n          </div>\r\n        </div>\r\n\r\n        {/* Context Visualization Bar */}\r\n        <div style={{ width: '100%', height: '8px', background: 'var(--bg-tertiary)', borderRadius: '4px', overflow: 'hidden', position: 'relative' }}>\r\n          <div style={{\r\n            width: `${metadata?.filledPercent || 0}%`,\r\n            height: '100%',\r\n            background: 'linear-gradient(90deg, var(--accent-primary), #a855f7)',\r\n            transition: 'width 0.3s ease'\r\n          }} />\r\n        </div>\r\n        {\r\n          metadata && (\r\n            <div style={{ display: 'flex', justifyContent: 'space-between', fontSize: '0.75rem', color: 'var(--text-dim)' }}>\r\n              <span>Used: {metadata.tokenCount || 0} tokens | {metadata.charCount || 0} chars ({(metadata.filledPercent || 0).toFixed(1)}%)</span>\r\n              <span>Atoms: {metadata.atomCount || 0}</span>\r\n            </div>\r\n          )\r\n        }\r\n      </div>\r\n\r\n      {/* Query Section */}\r\n      <div style={{ display: 'flex', gap: '0.5rem' }}>\r\n        <input\r\n          className=\"input-glass\"\r\n          placeholder=\"Ask your memories...\"\r\n          value={query}\r\n          onChange={(e) => setQuery(e.target.value)}\r\n          onKeyDown={(e) => { if (e.key === 'Enter') handleSearch(); }}\r\n        />\r\n        <button className=\"btn-primary\" onClick={handleSearch} disabled={loading}>\r\n          Search\r\n        </button>\r\n      </div>\r\n\r\n      {/* Results Section */}\r\n      <div style={{ flex: 1, overflowY: 'auto', display: 'flex', flexDirection: 'column', gap: '1rem', paddingRight: '0.5rem' }}>\r\n\r\n        {viewMode === 'raw' && (\r\n          <div style={{ position: 'relative', height: '100%' }}>\r\n            <button\r\n              className=\"btn-primary\"\r\n              style={{ position: 'absolute', top: '1rem', right: '1rem', padding: '0.4rem 0.8rem', fontSize: '0.8rem', zIndex: 10 }}\r\n              onClick={copyContext}\r\n            >\r\n              Copy All\r\n            </button>\r\n            <textarea\r\n              className=\"input-glass\"\r\n              style={{ width: '100%', height: '100%', resize: 'none', fontFamily: 'monospace', fontSize: '0.9rem', lineHeight: '1.5' }}\r\n              value={context}\r\n              readOnly\r\n              placeholder=\"Raw context will appear here...\"\r\n            />\r\n          </div>\r\n        )}\r\n\r\n        {\r\n          viewMode === 'cards' && results.map((r, idx) => (\r\n            <div key={r.id || idx} className=\"card-result animate-fade-in\" style={{ animationDelay: `${idx * 0.05}s` }}>\r\n              <div style={{ display: 'flex', justifyContent: 'space-between', marginBottom: '0.5rem' }}>\r\n                <div style={{ display: 'flex', gap: '0.5rem', alignItems: 'center' }}>\r\n                  <span className={`badge ${r.provenance === 'sovereign' ? 'badge-sovereign' : 'badge-external'}`}>\r\n                    {r.provenance || 'EXTERNAL'}\r\n                  </span>\r\n                  <span style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>\r\n                    {(r.score || 0).toFixed(2)}\r\n                  </span>\r\n                </div>\r\n                <div style={{ display: 'flex', gap: '1rem', alignItems: 'center' }}>\r\n                  <span style={{ fontSize: '0.8rem', color: 'var(--text-secondary)', fontStyle: 'italic' }}>\r\n                    {r.source}\r\n                  </span>\r\n                  <button\r\n                    onClick={() => handleQuarantine(r.id)}\r\n                    title=\"Quarantine this atom\"\r\n                    style={{ background: 'transparent', border: 'none', cursor: 'pointer', fontSize: '1rem', opacity: 0.6 }}\r\n                  >\r\n                    \r\n                  </button>\r\n                </div>\r\n              </div>\r\n              <div style={{ whiteSpace: 'pre-wrap', fontSize: '0.95rem', lineHeight: '1.5', maxHeight: '300px', overflowY: 'auto' }}>\r\n                {r.content}\r\n              </div>\r\n            </div>\r\n          ))\r\n        }\r\n\r\n        {\r\n          results.length === 0 && !loading && (\r\n            <div style={{ textAlign: 'center', padding: '2rem', color: 'var(--text-secondary)' }}>\r\n              No memories found. Try a different query.\r\n            </div>\r\n          )\r\n        }\r\n      </div>\r\n      {/* Research Modal */}\r\n      {showResearch && (\r\n        <div style={{\r\n          position: 'fixed', top: 0, left: 0, right: 0, bottom: 0,\r\n          background: 'rgba(0,0,0,0.8)', zIndex: 100,\r\n          display: 'flex', justifyContent: 'center', alignItems: 'center'\r\n        }}>\r\n          <div className=\"glass-panel\" style={{ width: '600px', height: '500px', padding: '1.5rem', display: 'flex', flexDirection: 'column', gap: '1rem', background: '#1a1a1a' }}>\r\n            <div style={{ display: 'flex', justifyContent: 'space-between' }}>\r\n              <h3>Research Station</h3>\r\n              <button onClick={() => setShowResearch(false)} style={{ background: 'transparent', border: 'none', fontSize: '1.2rem', cursor: 'pointer', color: 'white' }}></button>\r\n            </div>\r\n\r\n            {/* Tabs */}\r\n            <div style={{ display: 'flex', gap: '1rem', borderBottom: '1px solid var(--border-subtle)' }}>\r\n              <button\r\n                onClick={() => setResearchTab('search')}\r\n                style={{ padding: '0.5rem', borderBottom: researchTab === 'search' ? '2px solid var(--accent-primary)' : 'none', color: researchTab === 'search' ? 'white' : 'gray', background: 'transparent', cursor: 'pointer' }}\r\n              >\r\n                Web Search\r\n              </button>\r\n              <button\r\n                onClick={() => setResearchTab('direct')}\r\n                style={{ padding: '0.5rem', borderBottom: researchTab === 'direct' ? '2px solid var(--accent-primary)' : 'none', color: researchTab === 'direct' ? 'white' : 'gray', background: 'transparent', cursor: 'pointer' }}\r\n              >\r\n                Direct Link / Upload\r\n              </button>\r\n            </div>\r\n\r\n            {/* TAB: Web Search */}\r\n            {researchTab === 'search' && (\r\n              <div style={{ display: 'flex', flexDirection: 'column', gap: '1rem', height: '100%', overflow: 'hidden' }}>\r\n                <div style={{ display: 'flex', gap: '0.5rem' }}>\r\n                  <input\r\n                    className=\"input-glass\"\r\n                    placeholder=\"Search DuckDuckGo...\"\r\n                    value={webQuery}\r\n                    onChange={(e) => setWebQuery(e.target.value)}\r\n                    onKeyDown={(e) => e.key === 'Enter' && handleWebSearch()}\r\n                  />\r\n                  <button className=\"btn-primary\" onClick={handleWebSearch} disabled={webSearching}>\r\n                    {webSearching ? 'Searching...' : 'Search'}\r\n                  </button>\r\n                </div>\r\n\r\n                <div style={{ flex: 1, overflowY: 'auto', display: 'flex', flexDirection: 'column', gap: '0.5rem' }}>\r\n                  {webResults.map((r, i) => (\r\n                    <div key={i} style={{ padding: '0.8rem', background: 'var(--bg-secondary)', borderRadius: '4px', border: '1px solid var(--border-subtle)' }}>\r\n                      <div style={{ display: 'flex', justifyContent: 'space-between' }}>\r\n                        <a href={r.link} target=\"_blank\" style={{ fontWeight: 'bold', color: 'var(--accent-primary)', textDecoration: 'none', fontSize: '0.9rem', maxWidth: '80%', overflow: 'hidden', textOverflow: 'ellipsis' }}>{r.title}</a>\r\n                        <button\r\n                          onClick={() => handleSaveArticle(r.link)}\r\n                          style={{ padding: '0.2rem 0.6rem', fontSize: '0.7rem', background: 'var(--bg-tertiary)', border: '1px solid var(--border-subtle)', cursor: 'pointer', color: 'white' }}\r\n                        >\r\n                           Save\r\n                        </button>\r\n                      </div>\r\n                      <div style={{ fontSize: '0.8rem', color: 'var(--text-secondary)', marginTop: '0.3rem' }}>{r.snippet}</div>\r\n                      <div style={{ fontSize: '0.7rem', color: 'var(--text-dim)', marginTop: '0.3rem', whiteSpace: 'nowrap', overflow: 'hidden', textOverflow: 'ellipsis' }}>{r.link}</div>\r\n                    </div>\r\n                  ))}\r\n                </div>\r\n              </div>\r\n            )}\r\n\r\n            {/* TAB: Direct Link */}\r\n            {researchTab === 'direct' && (\r\n              <div style={{ display: 'flex', flexDirection: 'column', gap: '1.5rem', marginTop: '1rem' }}>\r\n                <div>\r\n                  <label style={{ display: 'block', marginBottom: '0.5rem', fontSize: '0.8rem' }}>Scrape URL</label>\r\n                  <div style={{ display: 'flex', gap: '0.5rem' }}>\r\n                    <input className=\"input-glass\" placeholder=\"https://example.com/article\" id=\"direct-url-input\" />\r\n                    <button className=\"btn-primary\" onClick={() => {\r\n                      const val = (document.getElementById('direct-url-input') as HTMLInputElement).value;\r\n                      if (val) handleSaveArticle(val);\r\n                    }}>Scrape</button>\r\n                  </div>\r\n                </div>\r\n\r\n                <div>\r\n                  <label style={{ display: 'block', marginBottom: '0.5rem', fontSize: '0.8rem' }}>Upload HTML File</label>\r\n                  <button className=\"btn-primary\" style={{ width: '100%' }} onClick={() => {\r\n                    const input = document.createElement('input');\r\n                    input.type = 'file';\r\n                    input.accept = '.html,.htm';\r\n                    input.onchange = async (_e: any) => {\r\n                      alert(\"File upload pending implementation of /convert endpoint. Please use URL for now.\");\r\n                    };\r\n                    input.click();\r\n                  }}>\r\n                     Select File\r\n                  </button>\r\n                </div>\r\n              </div>\r\n            )}\r\n\r\n          </div>\r\n        </div>\r\n      )}\r\n    </div>\r\n  );\r\n};\r\n\r\nconst ChatPage = () => {\r\n  const [messages, setMessages] = useState<{ role: 'user' | 'assistant'; content: string }[]>([\r\n    { role: 'assistant', content: 'Welcome to the Sovereign Chat. How can I help you today?' }\r\n  ]);\r\n  const [input, setInput] = useState('');\r\n  const [loading, setLoading] = useState(false);\r\n  const [ragContext, setRagContext] = useState('');\r\n\r\n  // Model Config State\r\n  const [modelDir, setModelDir] = useState('../models');\r\n  const [availableModels, setAvailableModels] = useState<string[]>([]);\r\n  const [selectedModel, setSelectedModel] = useState('');\r\n  const [currentModel, setCurrentModel] = useState('');\r\n  const [modelLoading, setModelLoading] = useState(false);\r\n\r\n  // Scan Models\r\n  const scanModels = async () => {\r\n    try {\r\n      const res = await fetch(`/v1/models?dir=${encodeURIComponent(modelDir)}`);\r\n      if (!res.ok) throw new Error('Failed to scan');\r\n      const models = await res.json();\r\n      setAvailableModels(models);\r\n      if (models.length > 0 && !selectedModel) setSelectedModel(models[0]);\r\n    } catch (e) {\r\n      console.error(e);\r\n      alert('Failed to scan directory');\r\n    }\r\n  };\r\n\r\n  // Load Model\r\n  const loadModel = async () => {\r\n    if (!selectedModel) return;\r\n    setModelLoading(true);\r\n    try {\r\n      // If custom directory, we must pass it OR pass full path?\r\n      // API /v1/inference/load accepts direct 'dir'.\r\n      const res = await fetch('/v1/inference/load', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({\r\n          model: selectedModel,\r\n          dir: modelDir\r\n        })\r\n      });\r\n      const data = await res.json();\r\n      if (res.ok) {\r\n        setCurrentModel(selectedModel);\r\n        alert(`Model Loaded: ${selectedModel}`);\r\n      } else {\r\n        throw new Error(data.error);\r\n      }\r\n    } catch (e: any) {\r\n      console.error(e);\r\n      alert(`Load Failed: ${e.message}`);\r\n    } finally {\r\n      setModelLoading(false);\r\n    }\r\n  };\r\n\r\n  const sendMessage = async () => {\r\n    if (!input.trim() || loading) return;\r\n\r\n    const userMsg = input.trim();\r\n    setInput('');\r\n    setMessages(prev => [...prev, { role: 'user', content: userMsg }]);\r\n    setLoading(true);\r\n\r\n    // Initial empty assistant message\r\n    setMessages(prev => [...prev, { role: 'assistant', content: '' }]);\r\n\r\n    try {\r\n      const res = await fetch('/v1/chat/completions', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({\r\n          messages: [\r\n            { role: 'system', content: 'You are a helpful assistant serving the Sovereign Context Engine.' },\r\n            ...messages.map(m => ({ role: m.role, content: m.content })),\r\n            { role: 'user', content: userMsg }\r\n          ],\r\n          stream: true\r\n        })\r\n      });\r\n\r\n      if (!res.body) throw new Error('No response body');\r\n\r\n      // ... inside sendMessage ...\r\n      const reader = res.body.getReader();\r\n      const decoder = new TextDecoder();\r\n      let assistantContent = '';\r\n\r\n      while (true) {\r\n        const { done, value } = await reader.read();\r\n        if (done) break;\r\n\r\n        const chunk = decoder.decode(value);\r\n        const lines = chunk.split('\\n');\r\n\r\n        for (const line of lines) {\r\n          if (line.startsWith('data: ')) {\r\n            const dataStr = line.replace('data: ', '').trim();\r\n            if (dataStr === '[DONE]') break;\r\n\r\n            try {\r\n              const data = JSON.parse(dataStr);\r\n\r\n              // HANDLE TOOL EVENTS\r\n              if (data.type === 'tool') {\r\n                setMessages(prev => [...prev, {\r\n                  role: 'assistant',\r\n                  content: ` Searching: \"${data.query}\" (Budget: ${data.budget})...`,\r\n                  isTool: true\r\n                } as any]);\r\n                assistantContent = '';\r\n                continue;\r\n              }\r\n\r\n              if (data.type === 'tool_result') {\r\n                setRagContext(prev => prev + data.full_context + \"\\n\\n\");\r\n                setMessages(prev => {\r\n                  const newMsgs = [...prev];\r\n                  // Find the last tool message and update it\r\n                  const lastMsg = newMsgs[newMsgs.length - 1];\r\n                  if ((lastMsg as any).isTool) {\r\n                    lastMsg.content += `\\n ${data.content}`;\r\n                    // Optional: Store full context in expanded state?\r\n                    // For now just append to text.\r\n                  }\r\n                  return newMsgs;\r\n                });\r\n                continue;\r\n              }\r\n\r\n              const delta = data.choices?.[0]?.delta?.content || '';\r\n              assistantContent += delta;\r\n\r\n              setMessages(prev => {\r\n                const newMsgs = [...prev];\r\n                const last = newMsgs[newMsgs.length - 1];\r\n                // Ensure we are appending to an assistant message, not a tool message\r\n                if (last.role === 'assistant' && !(last as any).isTool) {\r\n                  last.content = assistantContent;\r\n                } else if (delta) {\r\n                  // New assistant message chunk after a tool output\r\n                  newMsgs.push({ role: 'assistant', content: assistantContent });\r\n                }\r\n                return newMsgs;\r\n              });\r\n            } catch (e) {\r\n              console.error('Error parsing SSE:', e);\r\n            }\r\n          }\r\n        }\r\n      }\r\n\r\n    } catch (e) {\r\n      console.error(e);\r\n      setMessages(prev => [...prev, { role: 'assistant', content: 'Error: Could not connect to inference engine.' }]);\r\n    } finally {\r\n      setLoading(false);\r\n    }\r\n  };\r\n\r\n  return (\r\n    <div style={{ display: 'grid', gridTemplateColumns: '1fr 3fr', height: '100%' }}>\r\n      {/* Sidebar */}\r\n      <div style={{ padding: '1rem', borderRight: '1px solid var(--border-subtle)', background: 'var(--bg-secondary)', display: 'flex', flexDirection: 'column', gap: '1rem', overflowY: 'auto' }}>\r\n\r\n        {/* Model Config Panel */}\r\n        <div>\r\n          <h3>Model Config</h3>\r\n          <div className=\"glass-panel\" style={{ padding: '1rem', display: 'flex', flexDirection: 'column', gap: '0.8rem' }}>\r\n            <div>\r\n              <label style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Directory</label>\r\n              <div style={{ display: 'flex', gap: '0.5rem' }}>\r\n                <input className=\"input-glass\" style={{ fontSize: '0.8rem', padding: '0.4rem' }} value={modelDir} onChange={(e) => setModelDir(e.target.value)} />\r\n                <button className=\"btn-primary\" style={{ padding: '0.4rem' }} onClick={scanModels}>Scan</button>\r\n              </div>\r\n            </div>\r\n\r\n            {availableModels.length > 0 && (\r\n              <div>\r\n                <label style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Select Model</label>\r\n                <select\r\n                  className=\"input-glass\"\r\n                  style={{ fontSize: '0.8rem', padding: '0.4rem' }}\r\n                  value={selectedModel}\r\n                  onChange={(e) => setSelectedModel(e.target.value)}\r\n                >\r\n                  {availableModels.map(m => <option key={m} value={m}>{m}</option>)}\r\n                </select>\r\n                <button\r\n                  className=\"btn-primary\"\r\n                  style={{ width: '100%', marginTop: '0.5rem', background: currentModel === selectedModel ? 'var(--bg-tertiary)' : 'var(--accent-primary)' }}\r\n                  onClick={loadModel}\r\n                  disabled={modelLoading}\r\n                >\r\n                  {modelLoading ? 'Loading...' : currentModel === selectedModel ? 'Active' : 'Load Model'}\r\n                </button>\r\n              </div>\r\n            )}\r\n          </div>\r\n        </div>\r\n\r\n        {/* Context Panel */}\r\n        <div style={{ flex: 1 }}>\r\n          <h3>Context</h3>\r\n          <div className=\"glass-panel\" style={{ padding: '1rem', height: '100%', display: 'flex', flexDirection: 'column', gap: '0.5rem', minHeight: '150px' }}>\r\n            <span style={{ fontSize: '0.8rem', color: 'var(--text-dim)' }}>Tokens: {ragContext.length / 4} / 4096</span>\r\n            <textarea\r\n              className=\"input-glass\"\r\n              style={{ flex: 1, resize: 'none', fontSize: '0.8rem' }}\r\n              placeholder=\"RAG Context will appear here...\"\r\n              value={ragContext}\r\n              readOnly\r\n            />\r\n          </div>\r\n        </div>\r\n      </div>\r\n\r\n      {/* Chat Area */}\r\n      <div style={{ display: 'flex', flexDirection: 'column', height: '100%' }}>\r\n        <div style={{ flex: 1, padding: '2rem', overflowY: 'auto', display: 'flex', flexDirection: 'column', gap: '1rem' }}>\r\n          {messages.map((m, idx) => (\r\n            <div key={idx} className={`glass-panel animate-fade-in`} style={{\r\n              padding: '1rem',\r\n              maxWidth: '80%',\r\n              alignSelf: m.role === 'user' ? 'flex-end' : 'flex-start',\r\n              background: m.role === 'user' ? 'rgba(100, 108, 255, 0.1)' : 'var(--glass-bg)',\r\n              whiteSpace: 'pre-wrap'\r\n            }}>\r\n              {m.content.split(/(<search.*?>.*?<\\/search>)/gs).map((part, i) => (\r\n                part.startsWith('<search')\r\n                  ? null // HIDE the raw tag from the bubble\r\n                  : <span key={i}>{part}</span>\r\n              ))}\r\n            </div>\r\n          ))}\r\n          {loading && <div style={{ alignSelf: 'flex-start', color: 'var(--text-dim)', fontSize: '0.8rem', marginLeft: '1rem' }}>Thinking...</div>}\r\n        </div>\r\n        <div style={{ padding: '1rem', borderTop: '1px solid var(--border-subtle)' }}>\r\n          <div style={{ display: 'flex', gap: '1rem' }}>\r\n            <textarea\r\n              className=\"input-glass\"\r\n              rows={2}\r\n              placeholder=\"Type a message...\"\r\n              style={{ resize: 'none' }}\r\n              value={input}\r\n              onChange={(e) => setInput(e.target.value)}\r\n              onKeyDown={(e) => { if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); sendMessage(); } }}\r\n            />\r\n            <button className=\"btn-primary\" style={{ height: 'auto' }} onClick={sendMessage} disabled={loading}>Send</button>\r\n          </div>\r\n        </div>\r\n      </div>\r\n    </div>\r\n  );\r\n};\r\n\r\nfunction App() {\r\n  const [route, setRoute] = useState(window.location.hash || '#');\r\n\r\n  // Simple hash router listener\r\n  window.addEventListener('hashchange', () => setRoute(window.location.hash));\r\n\r\n  return (\r\n    <>\r\n      <nav style={{ padding: '1rem', borderBottom: '1px solid var(--border-subtle)', display: 'flex', justifyContent: 'space-between', alignItems: 'center' }}>\r\n        <span style={{ fontWeight: 'bold', cursor: 'pointer' }} onClick={() => window.location.hash = '#'}>SCE</span>\r\n        <div style={{ display: 'flex', gap: '1rem', fontSize: '0.9rem' }}>\r\n          <a onClick={() => window.location.hash = '#search'} style={{ cursor: 'pointer', color: route === '#search' ? 'white' : 'gray' }}>Search</a>\r\n          <a onClick={() => window.location.hash = '#chat'} style={{ cursor: 'pointer', color: route === '#chat' ? 'white' : 'gray' }}>Chat</a>\r\n        </div>\r\n      </nav>\r\n      <main style={{ flex: 1, overflow: 'hidden' }}>\r\n        {route === '#' || route === '' ? <Dashboard /> : null}\r\n        {route === '#search' ? <SearchPage /> : null}\r\n        {route === '#chat' ? <ChatPage /> : null}\r\n      </main>\r\n    </>\r\n  );\r\n}\r\n\r\nexport default App;\r\n"
    tokens: 10843
    size: 31919
  - path: frontend\src\index.css
    content: ":root {\r\n  /* Premium Dark Theme Palette */\r\n  --bg-primary: #0a0a0c;\r\n  --bg-secondary: #121214;\r\n  --bg-tertiary: #1c1c1f;\r\n\r\n  --accent-primary: #646cff;\r\n  --accent-hover: #7b83ff;\r\n  --accent-glow: rgba(100, 108, 255, 0.4);\r\n\r\n  --text-primary: #ffffff;\r\n  --text-secondary: #a1a1aa;\r\n  --text-dim: #52525b;\r\n\r\n  --border-subtle: #27272a;\r\n  --border-active: #3f3f46;\r\n\r\n  --glass-bg: rgba(28, 28, 31, 0.7);\r\n  --glass-border: rgba(255, 255, 255, 0.1);\r\n  --glass-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37);\r\n\r\n  --font-sans: 'Inter', system-ui, Avenir, Helvetica, Arial, sans-serif;\r\n  --radius-sm: 4px;\r\n  --radius-md: 8px;\r\n  --radius-lg: 16px;\r\n  --radius-full: 9999px;\r\n\r\n  --transition-fast: 0.15s ease;\r\n  --transition-normal: 0.3s ease;\r\n}\r\n\r\nbody {\r\n  margin: 0;\r\n  background-color: var(--bg-primary);\r\n  color: var(--text-primary);\r\n  font-family: var(--font-sans);\r\n  -webkit-font-smoothing: antialiased;\r\n  min-height: 100vh;\r\n}\r\n\r\n#root {\r\n  display: flex;\r\n  flex-direction: column;\r\n  height: 100vh;\r\n}\r\n\r\n/* Utilities */\r\n.glass-panel {\r\n  background: var(--glass-bg);\r\n  backdrop-filter: blur(12px);\r\n  -webkit-backdrop-filter: blur(12px);\r\n  border: 1px solid var(--glass-border);\r\n  box-shadow: var(--glass-shadow);\r\n  border-radius: var(--radius-lg);\r\n}\r\n\r\n.btn-primary {\r\n  background: var(--accent-primary);\r\n  color: white;\r\n  border: none;\r\n  padding: 0.6rem 1.2rem;\r\n  border-radius: var(--radius-md);\r\n  font-weight: 500;\r\n  cursor: pointer;\r\n  transition: all var(--transition-fast);\r\n}\r\n\r\n.btn-primary:hover {\r\n  background: var(--accent-hover);\r\n  box-shadow: 0 0 15px var(--accent-glow);\r\n  transform: translateY(-1px);\r\n}\r\n\r\n.input-glass {\r\n  background: rgba(0, 0, 0, 0.2);\r\n  border: 1px solid var(--border-subtle);\r\n  color: var(--text-primary);\r\n  padding: 0.8rem 1rem;\r\n  border-radius: var(--radius-md);\r\n  outline: none;\r\n  transition: border-color var(--transition-fast);\r\n  width: 100%;\r\n  font-size: 1rem;\r\n}\r\n\r\n.input-glass:focus {\r\n  border-color: var(--accent-primary);\r\n}\r\n\r\n/* Animations */\r\n@keyframes fadeIn {\r\n  from {\r\n    opacity: 0;\r\n    transform: translateY(10px);\r\n  }\r\n\r\n  to {\r\n    opacity: 1;\r\n    transform: translateY(0);\r\n  }\r\n}\r\n\r\n.animate-fade-in {\r\n  animation: fadeIn var(--transition-normal) forwards;\r\n}\r\n\r\n.animate-fade-in {\r\n  animation: fadeIn var(--transition-normal) forwards;\r\n}\r\n\r\n/* Scrollbar */\r\n::-webkit-scrollbar {\r\n  width: 8px;\r\n  height: 8px;\r\n}\r\n\r\n::-webkit-scrollbar-track {\r\n  background: rgba(0, 0, 0, 0.1);\r\n}\r\n\r\n::-webkit-scrollbar-thumb {\r\n  background: var(--border-active);\r\n  border-radius: var(--radius-full);\r\n}\r\n\r\n::-webkit-scrollbar-thumb:hover {\r\n  background: var(--text-dim);\r\n}\r\n\r\n/* Components */\r\n.card-result {\r\n  background: rgba(255, 255, 255, 0.03);\r\n  border: 1px solid var(--border-subtle);\r\n  border-radius: var(--radius-md);\r\n  padding: 1rem;\r\n  transition: all var(--transition-fast);\r\n}\r\n\r\n.card-result:hover {\r\n  background: rgba(255, 255, 255, 0.05);\r\n  border-color: var(--accent-glow);\r\n}\r\n\r\n.badge {\r\n  display: inline-block;\r\n  padding: 0.2rem 0.5rem;\r\n  border-radius: var(--radius-sm);\r\n  font-size: 0.7rem;\r\n  font-weight: 600;\r\n  text-transform: uppercase;\r\n  letter-spacing: 0.05em;\r\n}\r\n\r\n.badge-sovereign {\r\n  background: rgba(100, 108, 255, 0.2);\r\n  color: #8b92ff;\r\n  border: 1px solid rgba(100, 108, 255, 0.3);\r\n}\r\n\r\n.badge-external {\r\n  background: rgba(255, 255, 255, 0.1);\r\n  color: var(--text-secondary);\r\n}\r\n\r\n.code-block {\r\n  background: #000;\r\n  padding: 1rem;\r\n  border-radius: var(--radius-md);\r\n  font-family: monospace;\r\n  font-size: 0.9rem;\r\n  overflow-x: auto;\r\n  border: 1px solid var(--border-subtle);\r\n}"
    tokens: 1378
    size: 3650
  - path: frontend\src\main.tsx
    content: "import { StrictMode } from 'react'\r\nimport { createRoot } from 'react-dom/client'\r\nimport './index.css'\r\nimport App from './App.tsx'\r\n\r\ncreateRoot(document.getElementById('root')!).render(\r\n  <StrictMode>\r\n    <App />\r\n  </StrictMode>,\r\n)\r\n"
    tokens: 86
    size: 240
  - path: frontend\tsconfig.app.json
    content: "{\r\n  \"compilerOptions\": {\r\n    \"tsBuildInfoFile\": \"./node_modules/.tmp/tsconfig.app.tsbuildinfo\",\r\n    \"target\": \"ES2022\",\r\n    \"useDefineForClassFields\": true,\r\n    \"lib\": [\"ES2022\", \"DOM\", \"DOM.Iterable\"],\r\n    \"module\": \"ESNext\",\r\n    \"types\": [\"vite/client\"],\r\n    \"skipLibCheck\": true,\r\n\r\n    /* Bundler mode */\r\n    \"moduleResolution\": \"bundler\",\r\n    \"allowImportingTsExtensions\": true,\r\n    \"verbatimModuleSyntax\": true,\r\n    \"moduleDetection\": \"force\",\r\n    \"noEmit\": true,\r\n    \"jsx\": \"react-jsx\",\r\n\r\n    /* Linting */\r\n    \"strict\": true,\r\n    \"noUnusedLocals\": true,\r\n    \"noUnusedParameters\": true,\r\n    \"erasableSyntaxOnly\": true,\r\n    \"noFallthroughCasesInSwitch\": true,\r\n    \"noUncheckedSideEffectImports\": true\r\n  },\r\n  \"include\": [\"src\"]\r\n}\r\n"
    tokens: 243
    size: 760
  - path: frontend\tsconfig.json
    content: "{\r\n  \"files\": [],\r\n  \"references\": [\r\n    { \"path\": \"./tsconfig.app.json\" },\r\n    { \"path\": \"./tsconfig.node.json\" }\r\n  ]\r\n}\r\n"
    tokens: 42
    size: 126
  - path: frontend\tsconfig.node.json
    content: "{\r\n  \"compilerOptions\": {\r\n    \"tsBuildInfoFile\": \"./node_modules/.tmp/tsconfig.node.tsbuildinfo\",\r\n    \"target\": \"ES2023\",\r\n    \"lib\": [\"ES2023\"],\r\n    \"module\": \"ESNext\",\r\n    \"types\": [\"node\"],\r\n    \"skipLibCheck\": true,\r\n\r\n    /* Bundler mode */\r\n    \"moduleResolution\": \"bundler\",\r\n    \"allowImportingTsExtensions\": true,\r\n    \"verbatimModuleSyntax\": true,\r\n    \"moduleDetection\": \"force\",\r\n    \"noEmit\": true,\r\n\r\n    /* Linting */\r\n    \"strict\": true,\r\n    \"noUnusedLocals\": true,\r\n    \"noUnusedParameters\": true,\r\n    \"erasableSyntaxOnly\": true,\r\n    \"noFallthroughCasesInSwitch\": true,\r\n    \"noUncheckedSideEffectImports\": true\r\n  },\r\n  \"include\": [\"vite.config.ts\"]\r\n}\r\n"
    tokens: 216
    size: 679
  - path: frontend\vite.config.ts
    content: "import { defineConfig } from 'vite'\r\nimport react from '@vitejs/plugin-react'\r\n\r\n// https://vite.dev/config/\r\nexport default defineConfig({\r\n  plugins: [react()],\r\n})\r\n"
    tokens: 61
    size: 168
  - path: interface\index.html
    content: "<!DOCTYPE html>\r\n<html lang=\"en\">\r\n\r\n<head>\r\n    <meta charset=\"UTF-8\">\r\n    <title>Anchor Context Console</title>\r\n    <style>\r\n        :root {\r\n            --bg: #0f172a;\r\n            --panel: #1e293b;\r\n            --text: #e2e8f0;\r\n            --accent: #38bdf8;\r\n        }\r\n\r\n        body {\r\n            background: var(--bg);\r\n            color: var(--text);\r\n            font-family: system-ui;\r\n            margin: 0;\r\n            padding: 20px;\r\n            height: 100vh;\r\n            display: flex;\r\n            gap: 20px;\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        .sidebar {\r\n            width: 300px;\r\n            display: flex;\r\n            flex-direction: column;\r\n            gap: 20px;\r\n        }\r\n\r\n        .main {\r\n            flex: 1;\r\n            display: flex;\r\n            flex-direction: column;\r\n            background: var(--panel);\r\n            border-radius: 12px;\r\n            border: 1px solid #334155;\r\n            padding: 20px;\r\n        }\r\n\r\n        input,\r\n        button {\r\n            width: 100%;\r\n            padding: 12px;\r\n            border-radius: 8px;\r\n            border: 1px solid #334155;\r\n            background: #000;\r\n            color: #fff;\r\n            box-sizing: border-box;\r\n        }\r\n\r\n        button {\r\n            background: var(--accent);\r\n            color: #000;\r\n            font-weight: bold;\r\n            cursor: pointer;\r\n            border: none;\r\n        }\r\n\r\n        button:hover {\r\n            opacity: 0.9;\r\n        }\r\n\r\n        textarea {\r\n            flex: 1;\r\n            background: #000;\r\n            color: #a5f3fc;\r\n            border: none;\r\n            padding: 15px;\r\n            font-family: monospace;\r\n            resize: none;\r\n            outline: none;\r\n            border-radius: 8px;\r\n        }\r\n\r\n        .slider-group {\r\n            background: var(--panel);\r\n            padding: 15px;\r\n            border-radius: 8px;\r\n            border: 1px solid #334155;\r\n        }\r\n\r\n        label {\r\n            display: block;\r\n            margin-bottom: 10px;\r\n            font-size: 0.9rem;\r\n            color: #94a3b8;\r\n        }\r\n\r\n        .bucket-container {\r\n            display: flex;\r\n            flex-wrap: wrap;\r\n            gap: 8px;\r\n            margin-bottom: 15px;\r\n        }\r\n\r\n        .bucket-chip {\r\n            padding: 6px 12px;\r\n            border-radius: 16px;\r\n            background: #334155;\r\n            color: #94a3b8;\r\n            font-size: 0.8rem;\r\n            cursor: pointer;\r\n            border: 1px solid transparent;\r\n            transition: all 0.2s;\r\n        }\r\n\r\n        .bucket-chip.active {\r\n            background: var(--accent);\r\n            color: #000;\r\n            font-weight: bold;\r\n        }\r\n\r\n        .bucket-chip:hover {\r\n            border-color: var(--accent);\r\n        }\r\n\r\n        .bucket-add {\r\n            background: #10b981;\r\n            color: white;\r\n            padding: 6px 10px;\r\n            border-radius: 16px;\r\n            font-size: 0.8rem;\r\n            cursor: pointer;\r\n            border: none;\r\n        }\r\n    </style>\r\n</head>\r\n\r\n<body>\r\n    <div class=\"sidebar\">\r\n        <div>\r\n            <label> Context Buckets</label>\r\n            <div id=\"bucket-list\" class=\"bucket-container\">\r\n                <!-- Buckets will be loaded here -->\r\n            </div>\r\n        </div>\r\n        <div>\r\n            <label> Search Memory</label>\r\n            <input type=\"text\" id=\"query\" placeholder=\"Type keyword...\" onkeyup=\"if(event.key==='Enter') search()\">\r\n        </div>\r\n        <div class=\"slider-group\">\r\n            <label>Volume: <span id=\"vol-val\">5000</span> chars (<span id=\"tok-val\">1250</span> tokens)</label>\r\n            <input type=\"range\" id=\"vol\" min=\"1000\" max=\"4000000\" step=\"5000\" value=\"5000\"\r\n                oninput=\"document.getElementById('vol-val').innerText=this.value; document.getElementById('tok-val').innerText=Math.floor(this.value/4)\">\r\n        </div>\r\n        <button onclick=\"search()\">Fetch Context</button>\r\n        <button onclick=\"research()\" style=\"background: linear-gradient(135deg, #f59e0b, #d97706); color: #000; margin-top: 10px; border: 1px solid #b45309; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);\"> Deep Research</button>\r\n        <button onclick=\"copy()\" style=\"background: #334155; color: #fff; border: 1px solid #475569; margin-top: 10px;\"> Copy to\r\n            Clipboard</button>\r\n        <button onclick=\"downloadBackup()\" style=\"background: #10b981; color: white; margin-top: 10px; border: 1px solid #059669\">\r\n             Eject Memory (Backup)\r\n        </button>\r\n\r\n        <div style=\"margin-top: 20px; border-top: 1px solid #334155; padding-top: 20px;\">\r\n            <label> Ingest New Memory</label>\r\n            <textarea id=\"ingest-text\" style=\"height: 100px; width: 100%; margin-bottom: 10px;\" placeholder=\"Paste chat session here...\"></textarea>\r\n            <input type=\"text\" id=\"ingest-source\" placeholder=\"Source (e.g. Gemini Chat)\" style=\"margin-bottom: 10px;\">\r\n            <button onclick=\"ingest()\" style=\"background: #6366f1; color: white;\">Ingest Memory</button>\r\n        </div>\r\n    </div>\r\n    <div class=\"main\">\r\n        <textarea id=\"output\" readonly placeholder=\"Context results will appear here...\"></textarea>\r\n    </div>\r\n    <script>\r\n        let activeBuckets = ['core'];\r\n\r\n        async function loadBuckets() {\r\n            try {\r\n                const res = await fetch('http://localhost:3000/v1/buckets');\r\n                const serverBuckets = await res.json();\r\n                \r\n                // Merge server buckets with active buckets to ensure newly added ones show up\r\n                const allKnownBuckets = [...new Set([...serverBuckets, ...activeBuckets.filter(b => b !== 'all')])].sort();\r\n                \r\n                const list = document.getElementById('bucket-list');\r\n                list.innerHTML = '';\r\n\r\n                // Add \"All\" option\r\n                const allChip = document.createElement('div');\r\n                allChip.className = `bucket-chip ${activeBuckets.includes('all') ? 'active' : ''}`;\r\n                allChip.innerText = 'all';\r\n                allChip.onclick = () => selectBucket('all');\r\n                list.appendChild(allChip);\r\n\r\n                allKnownBuckets.forEach(b => {\r\n                    const chip = document.createElement('div');\r\n                    chip.className = `bucket-chip ${activeBuckets.includes(b) ? 'active' : ''}`;\r\n                    chip.innerText = b;\r\n                    chip.onclick = () => selectBucket(b);\r\n                    list.appendChild(chip);\r\n                });\r\n\r\n                const addBtn = document.createElement('button');\r\n                addBtn.className = 'bucket-add';\r\n                addBtn.innerText = '+';\r\n                addBtn.onclick = () => {\r\n                    const name = prompt(\"New Bucket Name:\");\r\n                    if (name) selectBucket(name.toLowerCase().trim());\r\n                };\r\n                list.appendChild(addBtn);\r\n            } catch (e) {\r\n                console.error(\"Failed to load buckets\", e);\r\n            }\r\n        }\r\n\r\n        function selectBucket(name) {\r\n            if (name === 'all') {\r\n                activeBuckets = ['all'];\r\n            } else {\r\n                // Remove 'all' if it was selected\r\n                activeBuckets = activeBuckets.filter(b => b !== 'all');\r\n                \r\n                if (activeBuckets.includes(name)) {\r\n                    // Toggle off\r\n                    activeBuckets = activeBuckets.filter(b => b !== name);\r\n                    if (activeBuckets.length === 0) activeBuckets = ['core'];\r\n                } else {\r\n                    // Toggle on\r\n                    activeBuckets.push(name);\r\n                }\r\n            }\r\n            loadBuckets();\r\n        }\r\n\r\n        async function search() {\r\n            const query = document.getElementById('query').value;\r\n            const limit = document.getElementById('vol').value;\r\n            const out = document.getElementById('output');\r\n            if (!query) return;\r\n            out.value = \"Searching...\";\r\n            try {\r\n                const body = { query, max_chars: parseInt(limit) };\r\n                if (!activeBuckets.includes('all')) {\r\n                    body.buckets = activeBuckets;\r\n                }\r\n\r\n                const res = await fetch('http://localhost:3000/v1/memory/search', {\r\n                    method: 'POST',\r\n                    headers: {\r\n                        'Content-Type': 'application/json'\r\n                    },\r\n                    body: JSON.stringify(body)\r\n                });\r\n                const data = await res.json();\r\n                out.value = data.context || \"No results.\";\r\n\r\n                // Send API response to log viewer for debugging\r\n                const logChannel = new BroadcastChannel('sovereign-logs');\r\n                logChannel.postMessage({\r\n                    source: 'Context Console',\r\n                    time: new Date().toISOString(),\r\n                    type: 'info',\r\n                    msg: `Search query: ${query} [Buckets: ${activeBuckets.join(', ')}], Results: ${data.context ? data.context.length : 0} chars`\r\n                });\r\n            } catch (e) {\r\n                out.value = \"Error: \" + e;\r\n\r\n                // Send error to log viewer for debugging\r\n                const logChannel = new BroadcastChannel('sovereign-logs');\r\n                logChannel.postMessage({\r\n                    source: 'Context Console',\r\n                    time: new Date().toISOString(),\r\n                    type: 'error',\r\n                    msg: `Search error: ${e.message}`\r\n                });\r\n            }\r\n        }\r\n\r\n        async function research() {\r\n            const query = document.getElementById('query').value;\r\n            const out = document.getElementById('output');\r\n            const max_chars = parseInt(document.getElementById('max_chars').value);\r\n            if (!query) return;\r\n\r\n            // Use at least 100k for research, or the slider value if higher\r\n            const researchChars = Math.max(100000, max_chars);\r\n            out.value = ` Performing Deep Research (${Math.floor(researchChars/4).toLocaleString()} tokens)...`;\r\n\r\n            try {\r\n                const body = { query, max_chars: researchChars, deep: true };\r\n                if (!activeBuckets.includes('all')) {\r\n                    body.buckets = activeBuckets;\r\n                }\r\n\r\n                const res = await fetch('http://localhost:3000/v1/memory/search', {\r\n                    method: 'POST',\r\n                    headers: {\r\n                        'Content-Type': 'application/json'\r\n                    },\r\n                    body: JSON.stringify(body)\r\n                });\r\n                const data = await res.json();\r\n                out.value = `=== DEEP RESEARCH RESULTS ===\\n\\n${data.context}` || \"No results.\";\r\n\r\n                const logChannel = new BroadcastChannel('sovereign-logs');\r\n                logChannel.postMessage({\r\n                    source: 'Research Engine',\r\n                    time: new Date().toISOString(),\r\n                    type: 'success',\r\n                    msg: `Deep Research complete for: ${query}`\r\n                });\r\n            } catch (e) {\r\n                out.value = \"Research Error: \" + e;\r\n            }\r\n        }\r\n        function copy() {\r\n            const el = document.getElementById('output');\r\n            el.select();\r\n            document.execCommand('copy');\r\n        }\r\n\r\n        async function downloadBackup() {\r\n            const btn = event.target;\r\n            const originalText = btn.innerText;\r\n            btn.innerText = \" Exporting...\";\r\n\r\n            try {\r\n                const res = await fetch('http://localhost:3000/v1/backup');\r\n                if (!res.ok) throw new Error(\"Backup failed\");\r\n\r\n                // Convert response to blob and trigger download\r\n                const blob = await res.blob();\r\n                const url = window.URL.createObjectURL(blob);\r\n                const a = document.createElement('a');\r\n                a.href = url;\r\n                // The server provides the filename in headers, but we can default\r\n                a.download = `cozo_memory_snapshot_${new Date().toISOString().slice(0,10)}.yaml`;\r\n                document.body.appendChild(a);\r\n                a.click();\r\n                window.URL.revokeObjectURL(url);\r\n                document.body.removeChild(a);\r\n\r\n                btn.innerText = \" Exported!\";\r\n                setTimeout(() => btn.innerText = originalText, 2000);\r\n\r\n            } catch (e) {\r\n                alert(\"Export Error: \" + e.message);\r\n                btn.innerText = \" Failed\";\r\n                setTimeout(() => btn.innerText = originalText, 2000);\r\n            }\r\n        }\r\n\r\n        async function ingest() {\r\n            const content = document.getElementById('ingest-text').value;\r\n            const source = document.getElementById('ingest-source').value || 'Manual Ingest';\r\n            if (!content) return alert(\"Please paste some content first.\");\r\n\r\n            const btn = event.target;\r\n            const originalText = btn.innerText;\r\n            btn.innerText = \" Ingesting...\";\r\n\r\n            try {\r\n                // For ingestion, we use the first active bucket (excluding 'all')\r\n                const targetBucket = activeBuckets.find(b => b !== 'all') || 'core';\r\n\r\n                const res = await fetch('http://localhost:3000/v1/ingest', {\r\n                    method: 'POST',\r\n                    headers: { 'Content-Type': 'application/json' },\r\n                    body: JSON.stringify({ content, source, type: 'chat', bucket: targetBucket })\r\n                });\r\n                \r\n                if (!res.ok) throw new Error(\"Ingest failed\");\r\n                \r\n                alert(` Memory ingested successfully into [${targetBucket}]!`);\r\n                document.getElementById('ingest-text').value = \"\";\r\n                btn.innerText = originalText;\r\n                loadBuckets(); // Refresh bucket list\r\n            } catch (e) {\r\n                alert(\"Ingest Error: \" + e.message);\r\n                btn.innerText = originalText;\r\n            }\r\n        }\r\n\r\n        // Initial load\r\n        loadBuckets();\r\n    </script>\r\n</body>\r\n\r\n</html>"
    tokens: 4828
    size: 14448
  - path: LICENSE
    content: "Elastic License 2.0\r\n\r\nURL: https://www.elastic.co/licensing/elastic-license\r\n\r\n## Acceptance\r\n\r\nBy using the software, you agree to all of the terms and conditions below.\r\n\r\n## Copyright License\r\n\r\nThe licensor grants you a non-exclusive, royalty-free, worldwide,\r\nnon-sublicensable, non-transferable license to use, copy, distribute, make\r\navailable, and prepare derivative works of the software, in each case subject to\r\nthe limitations and conditions below.\r\n\r\n## Limitations\r\n\r\nYou may not provide the software to third parties as a hosted or managed\r\nservice, where the service provides users with access to any substantial set of\r\nthe features or functionality of the software.\r\n\r\nYou may not move, change, disable, or circumvent the license key functionality\r\nin the software, and you may not remove or obscure any functionality in the\r\nsoftware that is protected by the license key.\r\n\r\nYou may not alter, remove, or obscure any licensing, copyright, or other notices\r\nof the licensor in the software. Any use of the licensors trademarks is subject\r\nto applicable law.\r\n\r\n## Patents\r\n\r\nThe licensor grants you a license, under any patent claims the licensor can\r\nlicense, or becomes able to license, to make, have made, use, sell, offer for\r\nsale, import and have imported the software, in each case subject to the\r\nlimitations and conditions in this license. This license does not cover any\r\npatent claims that you cause to be infringed by modifications or additions to\r\nthe software. If you or your company make any written claim that the software\r\ninfringes or contributes to infringement of any patent, your patent license for\r\nthe software granted under these terms ends immediately. If your company makes\r\nsuch a claim, your patent license ends immediately for work on behalf of your\r\ncompany.\r\n\r\n## Notices\r\n\r\nYou must ensure that anyone who gets a copy of any part of the software from you\r\nalso gets a copy of these terms.\r\n\r\nIf you modify the software, you must include in any modified copies of the\r\nsoftware prominent notices stating that you have modified the software.\r\n\r\n## No Other Rights\r\n\r\nThese terms do not imply any licenses other than those expressly granted in\r\nthese terms.\r\n\r\n## Termination\r\n\r\nIf you use the software in violation of these terms, such use is not licensed,\r\nand your licenses will automatically terminate. If the licensor provides you\r\nwith a notice of your violation, and you cease all violation of this license no\r\nlater than 30 days after you receive that notice, your licenses will be\r\nreinstated retroactively. However, if you violate these terms after such\r\nreinstatement, any additional violation of these terms will cause your licenses\r\nto terminate automatically and permanently.\r\n\r\n## No Liability\r\n\r\n*As far as the law allows, the software comes as is, without any warranty or\r\ncondition, and the licensor will not be liable to you for any damages arising\r\nout of these terms or the use or nature of the software, under any kind of\r\nlegal claim.*\r\n\r\n## Definitions\r\n\r\nThe **licensor** is the entity offering these terms, and the **software** is the\r\nsoftware the licensor makes available under these terms, including any portion\r\nof it.\r\n\r\n**you** refers to the individual or entity agreeing to these terms.\r\n\r\n**your company** is any legal entity, sole proprietorship, or other kind of\r\norganization that you work for, plus all organizations that have control over,\r\nare under the control of, or are under common control with that\r\norganization. **control** means ownership of substantially all the assets of an\r\nentity, or the power to direct its management and policies by vote, contract, or\r\notherwise. Control can be direct or indirect.\r\n\r\n**your licenses** are all the licenses granted to you for the software under\r\nthese terms.\r\n\r\n**use** means anything you do with the software requiring one of your licenses.\r\n\r\n**trademark** means trademarks, service marks, and similar rights."
    tokens: 1613
    size: 3951
  - path: package.json
    content: "{\r\n  \"name\": \"@ece/core\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"External Context Engine Core Components\",\r\n  \"main\": \"index.js\",\r\n  \"type\": \"module\",\r\n  \"scripts\": {\r\n    \"start\": \"node engine/dist/index.js\",\r\n    \"dev\": \"pnpm --filter sovereign-context-engine dev\",\r\n    \"build\": \"pnpm --filter frontend build && pnpm --filter sovereign-context-engine build\",\r\n    \"test\": \"jest\",\r\n    \"lint\": \"eslint . --ext .ts,.js\",\r\n    \"clean\": \"rimraf dist engine/dist frontend/dist\"\r\n  },\r\n  \"keywords\": [\r\n    \"context\",\r\n    \"ai\",\r\n    \"memory\",\r\n    \"external-context-engine\",\r\n    \"sovereign\"\r\n  ],\r\n  \"author\": \"External Context Engine Team\",\r\n  \"license\": \"MIT\",\r\n  \"dependencies\": {\r\n    \"@types/express\": \"^4.17.21\",\r\n    \"@types/node\": \"^20.9.0\",\r\n    \"axios\": \"^1.6.0\",\r\n    \"body-parser\": \"^1.20.2\",\r\n    \"cors\": \"^2.8.5\",\r\n    \"cozo-node\": \"^0.7.6\",\r\n    \"dotenv\": \"^16.3.1\",\r\n    \"express\": \"^4.18.2\",\r\n    \"sharp\": \"^0.34.5\",\r\n    \"typescript\": \"^5.0.0\",\r\n    \"ws\": \"^8.14.2\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@types/jest\": \"^29.5.6\",\r\n    \"eslint\": \"^8.53.0\",\r\n    \"jest\": \"^29.7.0\",\r\n    \"js-yaml\": \"^4.1.1\",\r\n    \"nodemon\": \"^3.0.1\",\r\n    \"rimraf\": \"^5.0.5\",\r\n    \"ts-node\": \"^10.9.1\"\r\n  },\r\n  \"engines\": {\r\n    \"node\": \">=18.0.0\"\r\n  },\r\n  \"repository\": {\r\n    \"type\": \"git\",\r\n    \"url\": \"https://github.com/External-Context-Engine/ECE_Core.git\"\r\n  },\r\n  \"bugs\": {\r\n    \"url\": \"https://github.com/External-Context-Engine/ECE_Core/issues\"\r\n  },\r\n  \"homepage\": \"https://github.com/External-Context-Engine/ECE_Core#readme\"\r\n}"
    tokens: 574
    size: 1537
  - path: plugins\whisper-recorder\package.json
    content: "{\r\n    \"name\": \"whisper-audio-recorder\",\r\n    \"version\": \"1.0.0\",\r\n    \"description\": \"Standalone audio recorder and transcriber using node-llama-cpp\",\r\n    \"main\": \"dist/index.js\",\r\n    \"type\": \"module\",\r\n    \"scripts\": {\r\n        \"build\": \"tsc\",\r\n        \"start\": \"node dist/index.js\",\r\n        \"record\": \"node dist/record.js\"\r\n    },\r\n    \"dependencies\": {\r\n        \"@mlc-ai/web-llm\": \"^0.2.1\",\r\n        \"@xenova/transformers\": \"^2.15.0\",\r\n        \"onnxruntime-node\": \"^1.17.0\",\r\n        \"node-audiorecorder\": \"^3.0.0\",\r\n        \"wav\": \"^1.0.2\",\r\n        \"dotenv\": \"^16.3.1\",\r\n        \"chalk\": \"^5.3.0\",\r\n        \"ws\": \"^8.16.0\"\r\n    },\r\n    \"devDependencies\": {\r\n        \"typescript\": \"^5.3.3\",\r\n        \"@types/node\": \"^20.10.0\",\r\n        \"@types/wav\": \"^1.0.4\"\r\n    }\r\n}"
    tokens: 285
    size: 776
  - path: plugins\whisper-recorder\src\index.ts
    content: "import { spawn } from 'child_process';\r\nimport path from 'path';\r\nimport fs from 'fs';\r\nimport { fileURLToPath } from 'url';\r\nimport { Transcriber } from './transcriber.js';\r\nimport readline from 'readline';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\nconst rl = readline.createInterface({\r\n    input: process.stdin,\r\n    output: process.stdout\r\n});\r\n\r\nconst RECORDING_SCRIPT = path.join(__dirname, 'recorder.js');\r\n\r\nasync function main() {\r\n    console.log('=== Whisper Audio Recorder ===');\r\n    console.log('1. Press ENTER to START recording.');\r\n\r\n    await new Promise<void>(resolve => rl.question('', () => resolve()));\r\n\r\n    console.log('Starting Recorder...');\r\n    const child = spawn('node', [RECORDING_SCRIPT], {\r\n        stdio: ['ignore', 'pipe', 'inherit'] // Pipe stdout to capture filename\r\n    });\r\n\r\n    let capturedFile = '';\r\n\r\n    child.stdout.on('data', (data) => {\r\n        const line = data.toString();\r\n        console.log(`[Recorder] ${line.trim()}`);\r\n        // Recorder script prints \"Saved: <path>\" on exit\r\n        const match = line.match(/Saved: (.+\\.wav)/);\r\n        if (match) {\r\n            capturedFile = match[1];\r\n        }\r\n    });\r\n\r\n    console.log('Recording in progress... Press ENTER to STOP.');\r\n\r\n    await new Promise<void>(resolve => rl.question('', () => resolve()));\r\n\r\n    console.log('Stopping Recorder...');\r\n    child.kill('SIGINT');\r\n\r\n    // Wait for child to exit\r\n    await new Promise<void>(resolve => child.on('exit', () => resolve()));\r\n\r\n    if (capturedFile && fs.existsSync(capturedFile.trim())) {\r\n        console.log(`\\nAnalyzing Audio: ${capturedFile}`);\r\n        // Transformers.js downloads model automatically\r\n        const transcriber = new Transcriber('Xenova/whisper-tiny.en');\r\n        try {\r\n            console.log('Running Whisper (WASM/ONNX)...');\r\n            const text = await transcriber.transcribe(capturedFile.trim());\r\n            console.log('\\n--- Transcript ---');\r\n            console.log(text);\r\n            console.log('------------------\\n');\r\n        } catch (e) {\r\n            console.error('Transcription failed:', e);\r\n        }\r\n    } else {\r\n        console.error('No capture file found.');\r\n    }\r\n\r\n    rl.close();\r\n}\r\n\r\nmain();\r\n"
    tokens: 805
    size: 2291
  - path: plugins\whisper-recorder\src\InferenceKernel.ts
    content: "import { CreateMLCEngine, MLCEngine } from \"@mlc-ai/web-llm\";\r\n\r\n/**\r\n * InferenceKernel (WebGPU/WASM Edition)\r\n * Uses @mlc-ai/web-llm (MLC) to run LLM inference.\r\n * \r\n * Note: Running this in Node.js requires a WebGPU implementation.\r\n * In a standard Node environment without a browser, this might fallback or fail \r\n * unless 'tvmjs' / 'navigator.gpu' polyfills are active.\r\n * \r\n * If running in Electron (Renderer), this works natively.\r\n * If running in pure Node, it assumes environment compatibility.\r\n */\r\nexport class InferenceKernel {\r\n    private engine: MLCEngine | null = null;\r\n\r\n    constructor(private modelId: string = \"Llama-3.1-8B-Instruct-q4f32_1-MLC\") { }\r\n\r\n    async init() {\r\n        console.log(`[Kernel] Initializing WebLLM for: ${this.modelId}`);\r\n        try {\r\n            // CreateEngine automatically selects the best available backend (WebGPU if available, or WASM fallback)\r\n            this.engine = await CreateMLCEngine(this.modelId, {\r\n                initProgressCallback: (report) => {\r\n                    console.log(`[Kernel] Loading: ${report.text}`);\r\n                }\r\n            });\r\n            console.log(`[Kernel] WebLLM Engine Ready.`);\r\n        } catch (e) {\r\n            console.error(`[Kernel] Initialization Failed (WebGPU missing?):`, e);\r\n            throw e;\r\n        }\r\n    }\r\n\r\n    async chat(message: string): Promise<string> {\r\n        if (!this.engine) throw new Error(\"Kernel not initialized\");\r\n\r\n        const messages = [\r\n            { role: \"system\", content: \"You are a helpful assistant.\" },\r\n            { role: \"user\", content: message }\r\n        ];\r\n\r\n        const reply = await this.engine.chat.completions.create({\r\n            messages: messages as any\r\n        });\r\n\r\n        return reply.choices[0].message.content || \"\";\r\n    }\r\n\r\n    /**\r\n     * Transcribe via Kernel?\r\n     * Current Architecture separates Transcriber (Whisper/Transformers.js) from Kernel (LLM/WebLLM).\r\n     * This method delegates or throws.\r\n     */\r\n    async transcribe(audioPath: string): Promise<string> {\r\n        throw new Error(\"Transcribe is handled by the sibling Transcriber class (Transformers.js).\");\r\n    }\r\n}\r\n"
    tokens: 775
    size: 2183
  - path: plugins\whisper-recorder\src\recorder.ts
    content: "import AudioRecorder from 'node-audiorecorder';\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\n\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst __dirname = path.dirname(__filename);\r\n\r\n// Constants\r\nconst OUTPUT_DIR = path.join(__dirname, '../../recordings'); // plugins/whisper-recorder/recordings\r\n\r\nif (!fs.existsSync(OUTPUT_DIR)) {\r\n    fs.mkdirSync(OUTPUT_DIR, { recursive: true });\r\n}\r\n\r\n// Configuration for 16-bit PCM, 16kHz, Mono (Whisper Standard)\r\nconst options = {\r\n    program: 'sox',     // Server-side recording usually works best with SoX\r\n    silence: 0,\r\n    thresholdStart: 0.5,\r\n    thresholdStop: 0.5,\r\n    keepSilence: true,\r\n    device: null,       // Default device\r\n    bits: 16,\r\n    channels: 1,\r\n    encoding: 'signed-integer',\r\n    rate: 16000,\r\n    type: 'wav',\r\n};\r\n\r\n// Initialize\r\nconst audioRecorder = new AudioRecorder(options, console);\r\n\r\nconsole.log('Recording... Press Ctrl+C to stop.');\r\n\r\n// Create file stream\r\nconst timestamp = new Date().toISOString().replace(/[:.]/g, '-');\r\nconst filename = `recording_${timestamp}.wav`;\r\nconst filePath = path.join(OUTPUT_DIR, filename);\r\nconst fileStream = fs.createWriteStream(filePath, { encoding: 'binary' });\r\n\r\n// Start recording\r\naudioRecorder.start().stream().pipe(fileStream);\r\n\r\n// Handle exit\r\nprocess.on('SIGINT', () => {\r\n    console.log('Stopping recording...');\r\n    audioRecorder.stop();\r\n    console.log(`Saved: ${filePath}`);\r\n    process.exit();\r\n});\r\n"
    tokens: 537
    size: 1502
  - path: plugins\whisper-recorder\src\transcriber.ts
    content: "import { pipeline, env } from '@xenova/transformers';\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport wav from 'wav'; // Used to read WAV headers if needed, but transformers handles paths\r\n\r\n// Configure cache to avoid re-downloading to user home\r\nenv.localModelPath = path.join(process.cwd(), 'models');\r\nenv.allowRemoteModels = true;\r\n\r\n/**\r\n * Transcriber (WASM/ONNX Edition)\r\n * Uses @xenova/transformers to run Whisper.\r\n */\r\nexport class Transcriber {\r\n    private p: any = null;\r\n\r\n    constructor(private modelName: string = 'Xenova/whisper-tiny.en') { }\r\n\r\n    async init() {\r\n        console.log(`[Transcriber] Loading Model: ${this.modelName}...`);\r\n        // Define task and model\r\n        this.p = await pipeline('automatic-speech-recognition', this.modelName, {\r\n            quantized: true // Use INT8 quantized model for speed\r\n        });\r\n        console.log(`[Transcriber] Model Loaded.`);\r\n    }\r\n\r\n    async transcribe(wavPath: string): Promise<string> {\r\n        if (!this.p) await this.init();\r\n\r\n        console.log(`[Transcriber] Processing: ${wavPath}`);\r\n\r\n        if (!fs.existsSync(wavPath)) {\r\n            throw new Error(`File not found: ${wavPath}`);\r\n        }\r\n\r\n        try {\r\n            // @xenova/transformers accepts file paths directly in Node.js\r\n            // It uses 'wavefile' internally to parse.\r\n            const result = await this.p(wavPath, {\r\n                chunk_length_s: 30,\r\n                stride_length_s: 5,\r\n                language: 'english',\r\n                task: 'transcribe',\r\n                return_timestamps: true\r\n            });\r\n\r\n            // Result is { text: \"...\", chunks: [...] }\r\n            const text = result.text.trim();\r\n\r\n            // Save transcript\r\n            const txtPath = wavPath.replace('.wav', '.txt');\r\n            fs.writeFileSync(txtPath, text);\r\n            console.log(`[Transcriber] Saved: ${txtPath}`);\r\n\r\n            return text;\r\n\r\n        } catch (e) {\r\n            console.error(`[Transcriber] Error:`, e);\r\n            throw e;\r\n        }\r\n    }\r\n}\r\n"
    tokens: 723
    size: 2071
  - path: plugins\whisper-recorder\tsconfig.json
    content: "{\r\n    \"compilerOptions\": {\r\n        \"target\": \"ES2022\",\r\n        \"module\": \"NodeNext\",\r\n        \"moduleResolution\": \"NodeNext\",\r\n        \"outDir\": \"./dist\",\r\n        \"rootDir\": \"./src\",\r\n        \"strict\": true,\r\n        \"esModuleInterop\": true,\r\n        \"skipLibCheck\": true,\r\n        \"forceConsistentCasingInFileNames\": true\r\n    },\r\n    \"include\": [\r\n        \"src/**/*\"\r\n    ],\r\n    \"exclude\": [\r\n        \"node_modules\"\r\n    ]\r\n}"
    tokens: 131
    size: 432
  - path: pnpm-workspace.yaml
    content: "packages:\r\n  - engine\r\n  - shared\r\n  - frontend\r\n\r\nignoredBuiltDependencies:\r\n  - cozo-node\r\n  - sharp\r\n"
    tokens: 34
    size: 104
  - path: README.md
    content: |
      # ECE_Core - Sovereign Context Engine

      > **Sovereign Context Engine (SCE)** - A local-first, graph-native memory system for cognitive augmentation.

      **Version**: 3.0.0 | **Architecture**: Tag-Walker (Graph-Native) | **Stack**: Node.js Monolith + CozoDB (RocksDB)

      ---

      ##  Overview

      The **ECE_Core** is a sovereign memory engine that transforms your local file system into a structured, queryable knowledge graph. Unlike traditional RAG systems that rely on heavy, probabilistic vector embeddings, ECE uses a **deterministic "Tag-Walker" protocol** to navigate your data.

      It runs 100% locally, protecting your privacy while enabling "Deep Context" retrieval on consumer hardware (low RAM/VRAM requirements).

      ### Key Features

      * ** The Tag-Walker Protocol**: Replaces resource-heavy Vector Search with a lightweight, 3-phase graph traversal (Anchor  Bridge  Walk).
      * ** Sovereign Provenance**: Implements "Trust Hierarchy." Data created by you (Sovereign) receives a 3.0x retrieval boost over external scrapes.
      * ** Mirror Protocol 2.0**: Projects your AI's internal graph onto your filesystem as readable Markdown files organized by `@bucket` and `#tag`.
      * ** Ghost Data Protocol**: "Read-After-Write" verification ensures zero data loss during high-velocity ingestion.
      * ** Atomic Ingestion**: Chemically splits content into "Atoms" (thoughts) rather than arbitrary text chunks, preserving semantic integrity.

      ---

      ##  Architecture

      ### 1. The Core (Node.js Monolith)
      The engine runs as a single, efficient Node.js process managing three distinct layers:

      1.  **Ingestion (The Refiner)**:
          * **Atomizer**: Splits text/code into logical units.
          * **Key Assassin**: Surgically removes JSON artifacts from code (Data Hygiene).
          * **Enricher**: Assigns `source_id`, `sequence`, and `provenance`.
          * **Zero-Vector**: Stubs embedding slots to maintain schema compatibility without VRAM cost.

      2.  **Retrieval (Tag-Walker)**:
          * **Phase 1 (Anchors)**: Uses optimized FTS (Full Text Search) to find direct keyword matches (70% context budget).
          * **Phase 2 (The Walk)**: Pivots via shared tags/buckets to find "Associative Neighbors" that share context but lack keywords (30% context budget).

      3.  **Persistence (CozoDB)**:
          * Backed by **RocksDB** for high-performance local storage.
          * Manages a Datalog graph of `*memory`, `*source`, and `*engrams`.

      ### 2. The Application Layer
      * **API**: RESTful interface at `http://localhost:3000/v1/`.
      * **Frontend**: Modern React + Vite dashboard for managing memories.
      * **Desktop Overlay**: Electron "Thin Client" for Always-on-Top assistance.

      ---

      ##  Quick Start

      ### Prerequisites
      * Node.js >= 18.0.0
      * pnpm (`npm i -g pnpm`)
      * Git

      ### 1. Installation
      ```bash
      git clone https://github.com/External-Context-Engine/ECE_Core.git
      cd ECE_Core
      pnpm install
      ```

      ### 2. Configuration

      Copy the example configuration:

      ```bash
      cp .env.example .env
      ```

      Ensure your `.env` is configured for **Tag-Walker Mode** (Embeddings Disabled):

      ```env
      # Core
      PORT=3000

      # Models (Chat Only)
      LLM_MODEL_PATH=Qwen3-4B-Instruct.gguf
      LLM_CTX_SIZE=4096
      LLM_GPU_LAYERS=33

      # Tech Debt Removal (Disable Embeddings)
      EMBEDDING_GPU_LAYERS=0
      ```

      ### 3. Run Engine

      ```bash
      pnpm start
      ```

      * **Server**: `http://localhost:3000`
      * **Health Check**: `http://localhost:3000/health`

      ### 4. Run Desktop Overlay (Optional)

      ```bash
      cd desktop-overlay
      pnpm install
      pnpm start
      ```

      ---

      ##  Project Structure

      * **`engine/`**: The neural center.
      * `src/core/`: Database (CozoDB) and Batch processors.
      * `src/services/ingest/`: Watchdog, Refiner, and Atomizer.
      * `src/services/search/`: The **Tag-Walker** implementation.
      * `src/services/mirror/`: Filesystem projection logic.


      * **`frontend/`**: React dashboard.
      * **`desktop-overlay/`**: Electron app.
      * **`specs/`**: The **Sovereign Engineering Code (SEC)** - The laws governing this system.

      ---

      ##  Documentation Standards

      This project follows strict engineering standards documented in `specs/standards/`. Key references:

      * **Standard 065**: [Graph-Based Associative Retrieval](./specs/standards/065-graph-associative-retrieval.md)
      * **Standard 059**: [Reliable Ingestion (Ghost Data Protocol)](./specs/standards/059_reliable_ingestion.md)
      * **Standard 058**: [UniversalRAG API](./specs/standards/058_universal_rag_api.md)
      * **Standard 073**: [Data Hygiene Protocol (The Immune System)](./specs/standards/073-data-hygiene-protocol.md)

      ---

      ##  Utility Tools

      ### Codebase Scraper (`read_all.js`)
      **The Official Ingestion Bridge**

      Consolidate an entire project into a digestable corpus for the engine. 

      ```bash
      node read_all.js <path_to_project_root>
      ```

      **Output**: `codebase/combined_context.yaml`
      **Usage**: Drop the result into `notebook/inbox`.
      **Mechanism**: The Refiner detects this YAML format and performs **Virtual Atomization**:
      *   Decomposes the YAML back into virtual files (e.g., `src/index.ts`).
      *   **Hygiene Check**: Filters binaries and circular logs.
      *   Auto-tags them with `#code`, `#doc`, and `#project:{name}`.
      *   Enables precise filtering (Code vs. Docs) even for massive codebases.

      ##  Data Hygiene & Tagging (Standard 073)

      The engine implements a rigorous "Immune System" to ensure your context remains clean and high-signal.

      ### 1. The Dual Inboxes (Provenance)
      *   **`notebook/inbox/`**: For **Sovereign Data** (Your notes, code, thoughts).
          *   **Trust Score**: High (2.0x - 3.0x Boost).
          *   **Use Case**: Manual drops, Obsidian vault sync.
      *   **`notebook/external-inbox/`**: For **External Data** (Web scrapes, news, documentation).
          *   **Trust Score**: Low (supporting evidence only).
          *   **Use Case**: News Agent outputs, documentation dumps.

      ### 2. Sovereign Tags (`context/sovereign_tags.json`)
      You can define specific keywords that should ALWAYS trigger a tag, regardless of context.
      *   **Config**: Edit `context/sovereign_tags.json`.
      *   **Behavior**: If the Refiner sees the word "Sybil" in an atom, it stamps it with `#Sybil`.
      *   **Benefit**: Ensures your core terminology is always indexed for retrieval.

      ### 3. Automatic Project Tagging
      The engine reads your file paths to generate context.
      *   Path: `notebook/inbox/Job-Context/Resume.md`
      *   Resulting Tags: `#project:Job-Context`, `#inbox`, `#Sovereign`
      *   **Tip**: Organize your files into named folders to automatically categorize them.

      ### 4. The Refiner's "Key Assassin"
      The engine automatically strips:
      *   JSON Metadata wrappers (`"response_content": "..."`)
      *   Excessive backslashes (`C:\\\\Users` -> `C:/Users`)
      *   "Thinking" logs from LLM outputs.
      *   **Result**: You can dump raw logs into the inbox, and the engine will distill the actual content.

      ---

      ##  Querying Best Practices

      The **Tag-Walker** engine interprets intents differently than vector databases. Use these patterns for optimal results:

      ### 1. Concept Recall (The Net)
      *   **Pattern**: Single Keyword / Concept
      *   **Example**: `"Memory"` or `"WebGPU"`
      *   **Mechanism**: Triggers **Tag-Walker**. The engine treats the query as a conceptual node and "walks" the graph to find everything related to it, even if the exact word isn't present in the target.
      *   **Use Case**: "Tell me everything about X."

      ### 2. Precision Intersection (The Laser)
      *   **Pattern**: Multi-Keyword Constraint
      *   **Example**: `"WebGPU buffer mapping"`
      *   **Mechanism**: Triggers **FTS Intersection**. The engine demands that *all* terms (or highly correlated concepts) be present. It filters out broad associations to give you the specific implementation details.
      *   **Use Case**: "How do I implement X?" or "Find the specific error log."

      ### 3. Natural Language (The Conversation)
      *   **Pattern**: Questions or Sentences
      *   **Example**: `"How is Coda doing today?"`
      *   **Mechanism**: Uses **Wink-NLP** to extract the core intent (`Coda` + `Status`). It strips stop words ("How", "is", "doing") and executes a graph query on the entities.
      *   **Use Case**: Chatting with the system or asking about high-level status.

      ---

      ## License

      Elastic License 2.0. Copyright (c) 2026 External Context Engine. See [LICENSE](LICENSE) for full terms.
    tokens: 3081
    size: 8185
  - path: read_all.js
    content: "/**\r\n * Universal Context Aggregation Tool\r\n *\r\n * This script recursively scans all text files in a project root,\r\n * aggregates their content into a single YAML file with configurable limits.\r\n * Designed to work in any codebase from the root directory.\r\n */\r\n\r\nimport fs from 'fs';\r\nimport path from 'path';\r\nimport { fileURLToPath } from 'url';\r\nimport yaml from 'js-yaml';\r\n\r\n// Configuration options\r\nconst CONFIG = {\r\n    // Token limits\r\n    tokenLimit: 1000000, // 1M tokens - increased for full codebase analysis\r\n    maxFileSize: 5 * 1024 * 1024, // 5MB max per file to prevent huge files\r\n    maxLinesPerFile: 5000, // Max 5000 lines per file to prevent huge content\r\n\r\n    // Output options\r\n    outputDir: 'codebase',\r\n    outputFile: 'combined_context.yaml',\r\n\r\n    // File inclusion/exclusion patterns\r\n    includeExtensions: [\r\n        // Code files\r\n        '.js', '.ts', '.jsx', '.tsx', '.py', '.java', '.cpp', '.c', '.h', '.cs',\r\n        '.go', '.rs', '.rb', '.php', '.html', '.css', '.scss', '.sass', '.less',\r\n        '.json', '.yaml', '.yml', '.xml', '.sql', '.sh', '.bash', '.zsh',\r\n        '.md', '.txt', '.csv', '.toml', '.ini', '.cfg', '.conf', '.env',\r\n        '.dockerfile', 'dockerfile', '.gitignore', '.npmignore', '.prettierignore',\r\n        // Configuration files\r\n        'makefile', 'cmakelists.txt', 'readme.md', 'readme.txt', 'readme',\r\n        'license', 'license.md', 'changelog', 'changelog.md', 'contributing',\r\n        'contributing.md', 'code_of_conduct', 'code_of_conduct.md'\r\n    ],\r\n\r\n    excludeExtensions: [\r\n        // Binary files\r\n        '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.svg', '.webp',\r\n        '.exe', '.bin', '.dll', '.so', '.dylib', '.zip', '.tar', '.gz', '.rar', '.7z',\r\n        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\r\n        '.mp3', '.mp4', '.avi', '.mov', '.wav', '.flac',\r\n        '.ttf', '.otf', '.woff', '.woff2',\r\n        // Build/cache files\r\n        '.o', '.obj', '.a', '.lib', '.out', '.class', '.jar', '.war', '.swp', '.swo',\r\n        '.lock', '.cache', '.log', '.tmp', '.temp', '.DS_Store', 'Thumbs.db'\r\n    ],\r\n\r\n    excludeDirectories: [\r\n        '.git', 'node_modules', 'archive', 'backups', 'logs', 'context', '.vscode',\r\n        '.idea', '.pytest_cache', '__pycache__', 'dist', 'build', 'target',\r\n        'venv', 'env', '.venv', '.env', 'Pods', 'Carthage', 'CocoaPods',\r\n        '.next', '.nuxt', 'public', 'static', 'assets', 'images', 'img', 'codebase',\r\n    ],\r\n\r\n    excludeFiles: [\r\n        'combined_context.yaml', 'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',\r\n        'Gemfile.lock', 'Pipfile.lock', 'Cargo.lock', 'composer.lock',\r\n        'go.sum', 'go.mod', 'requirements.txt', 'poetry.lock',\r\n        // Database files\r\n        '*.db', '*.sqlite', '*.sqlite3', '*.fdb', '*.mdb', '*.accdb',\r\n        // Temporary files\r\n        '*~', '*.tmp', '*.temp', '*.cache', '*.swp', '*.swo'\r\n    ]\r\n};\r\n\r\n// Simple token counting function\r\nfunction countTokens(text) {\r\n    // A rough approximation: 1 token  4 characters or 1 word\r\n    const words = text.match(/\\b\\w+\\b/g) || [];\r\n    return words.length + Math.ceil(text.length / 4);\r\n}\r\n\r\n// Function to check if a path should be ignored based on configuration\r\nfunction shouldIgnore(filePath, rootDir) {\r\n    const fileName = path.basename(filePath).toLowerCase();\r\n    const dirName = path.dirname(filePath).split(path.sep).pop().toLowerCase();\r\n    const ext = path.extname(filePath).toLowerCase();\r\n\r\n    // Check if directory should be excluded\r\n    for (const excludeDir of CONFIG.excludeDirectories) {\r\n        if (dirName === excludeDir.toLowerCase()) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    // Check if file extension should be excluded\r\n    if (CONFIG.excludeExtensions.includes(ext)) {\r\n        return true;\r\n    }\r\n\r\n    // Check if file should be excluded by name patterns\r\n    for (const excludePattern of CONFIG.excludeFiles) {\r\n        if (matchesPattern(fileName, excludePattern.toLowerCase()) ||\r\n            matchesPattern(path.basename(filePath), excludePattern)) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    // Check if file is too large\r\n    try {\r\n        const stats = fs.statSync(filePath);\r\n        if (stats.size > CONFIG.maxFileSize) {\r\n            return true;\r\n        }\r\n    } catch (e) {\r\n        // If we can't stat the file, skip it\r\n        return true;\r\n    }\r\n\r\n    // Check if file extension should be included (if include list is specified)\r\n    if (CONFIG.includeExtensions.length > 0) {\r\n        const fullName = path.basename(filePath).toLowerCase();\r\n        if (!CONFIG.includeExtensions.includes(ext) && !CONFIG.includeExtensions.includes(fullName)) {\r\n            return true;\r\n        }\r\n    }\r\n\r\n    return false;\r\n}\r\n\r\n// Helper function to check if a filename matches a pattern (supports wildcards)\r\nfunction matchesPattern(fileName, pattern) {\r\n    if (pattern === fileName) return true;\r\n\r\n    if (pattern.startsWith('*') && fileName.endsWith(pattern.substring(1))) {\r\n        return true;\r\n    }\r\n\r\n    if (pattern.endsWith('*') && fileName.startsWith(pattern.substring(0, pattern.length - 1))) {\r\n        return true;\r\n    }\r\n\r\n    return false;\r\n}\r\n\r\n// Function to limit file content based on line count\r\nfunction limitFileContent(content) {\r\n    if (!content) return '';\r\n\r\n    const lines = content.split('\\n');\r\n    if (lines.length <= CONFIG.maxLinesPerFile) {\r\n        return content;\r\n    }\r\n\r\n    // Take the first and last parts of the file to preserve context\r\n    const header = lines.slice(0, CONFIG.maxLinesPerFile / 2).join('\\n');\r\n    const footer = lines.slice(-CONFIG.maxLinesPerFile / 2).join('\\n');\r\n\r\n    return `${header}\\n\\n... [CONTENT TRUNCATED - ${lines.length - CONFIG.maxLinesPerFile} LINES REMOVED] ...\\n\\n${footer}`;\r\n}\r\n\r\n// Function to aggregate all file contents from the project root\r\nexport function createFullCorpusRecursive(rootDir = process.cwd()) {\r\n    // Allow rootDir to be passed as command line argument\r\n    if (process.argv[2] && process.argv[2] !== 'json' && process.argv[2] !== 'yaml') {\r\n        rootDir = path.resolve(process.argv[2]);\r\n    }\r\n\r\n    const outputDir = path.join(rootDir, CONFIG.outputDir);\r\n    console.log(`Scanning project root: ${rootDir}`);\r\n\r\n    if (!fs.existsSync(outputDir)) {\r\n        console.log(`Output directory does not exist, creating: ${outputDir}`);\r\n        fs.mkdirSync(outputDir, { recursive: true });\r\n    }\r\n\r\n    const aggregatedData = {\r\n        project_structure: rootDir,\r\n        scan_config: {\r\n            tokenLimit: CONFIG.tokenLimit,\r\n            maxFileSize: CONFIG.maxFileSize,\r\n            maxLinesPerFile: CONFIG.maxLinesPerFile,\r\n            includeExtensions: CONFIG.includeExtensions,\r\n            excludeExtensions: CONFIG.excludeExtensions,\r\n            excludeDirectories: CONFIG.excludeDirectories,\r\n            excludeFiles: CONFIG.excludeFiles\r\n        },\r\n        files: []\r\n    };\r\n\r\n    let totalTokens = 0;\r\n\r\n    // Walk through all files in the project\r\n    function walkDirectory(currentPath) {\r\n        let items;\r\n        try {\r\n            items = fs.readdirSync(currentPath);\r\n        } catch (e) {\r\n            console.warn(`Could not read directory: ${currentPath} - ${e.message}`);\r\n            return;\r\n        }\r\n\r\n        for (const item of items) {\r\n            const itemPath = path.join(currentPath, item);\r\n            const relativePath = path.relative(rootDir, itemPath);\r\n\r\n            let stat;\r\n            try {\r\n                stat = fs.statSync(itemPath);\r\n            } catch (e) {\r\n                continue;\r\n            }\r\n\r\n            if (stat.isDirectory()) {\r\n                // Skip excluded directories\r\n                const dirName = item.toLowerCase();\r\n                if (CONFIG.excludeDirectories.some(exclude => dirName === exclude.toLowerCase())) {\r\n                    continue;\r\n                }\r\n\r\n                walkDirectory(itemPath);\r\n            } else {\r\n                // Check if file should be ignored\r\n                if (shouldIgnore(itemPath, rootDir)) {\r\n                    continue;\r\n                }\r\n\r\n                try {\r\n                    const rawContent = fs.readFileSync(itemPath, 'utf-8');\r\n                    const content = limitFileContent(rawContent);\r\n                    const fileTokens = countTokens(content);\r\n\r\n                    if (totalTokens + fileTokens > CONFIG.tokenLimit) {\r\n                        console.log(`Token limit reached. Skipping: ${relativePath}`);\r\n                        continue;\r\n                    }\r\n\r\n                    const fileData = {\r\n                        path: relativePath,\r\n                        content: content,\r\n                        tokens: fileTokens,\r\n                        size: Buffer.byteLength(rawContent, 'utf8')\r\n                    };\r\n\r\n                    aggregatedData.files.push(fileData);\r\n                    totalTokens += fileTokens;\r\n                    console.log(`Processed: ${relativePath} (${fileTokens} tokens)`);\r\n                } catch (e) {\r\n                    console.warn(`Could not read file: ${itemPath} - ${e.message}`);\r\n                    // Skip non-text files or files with read errors\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    walkDirectory(rootDir);\r\n\r\n    aggregatedData.metadata = {\r\n        total_files: aggregatedData.files.length,\r\n        total_tokens: totalTokens,\r\n        token_limit: CONFIG.tokenLimit,\r\n        token_limit_reached: totalTokens >= CONFIG.tokenLimit,\r\n        timestamp: new Date().toISOString(),\r\n        root_directory: rootDir,\r\n        config: CONFIG\r\n    };\r\n\r\n    // Write to YAML file in output directory\r\n    const outputFile = path.join(outputDir, CONFIG.outputFile);\r\n    const yamlContent = yaml.dump(aggregatedData, {\r\n        lineWidth: -1,\r\n        noRefs: true,\r\n        quotingType: '\"',\r\n        forceQuotes: false\r\n    });\r\n    fs.writeFileSync(outputFile, yamlContent);\r\n\r\n    console.log(\"Aggregation complete!\");\r\n    console.log(`Output file: ${outputFile}`);\r\n    console.log(`Total files processed: ${aggregatedData.metadata.total_files}`);\r\n    console.log(`Total tokens: ${aggregatedData.metadata.total_tokens}`);\r\n    console.log(`Scan completed at: ${new Date().toISOString()}`);\r\n\r\n    return aggregatedData;\r\n}\r\n\r\n// Alternative function to output JSON format\r\nexport function createFullCorpusRecursiveJSON(rootDir = process.cwd()) {\r\n    const result = createFullCorpusRecursive(rootDir);\r\n    const outputDir = path.join(rootDir, CONFIG.outputDir);\r\n    const outputFile = path.join(outputDir, CONFIG.outputFile.replace('.yaml', '.json'));\r\n\r\n    fs.writeFileSync(outputFile, JSON.stringify(result, null, 2));\r\n    console.log(`JSON output also saved to: ${outputFile}`);\r\n\r\n    return result;\r\n}\r\n\r\nexport { CONFIG };\r\n\r\n// Run if this file is executed directly\r\nconst __filename = fileURLToPath(import.meta.url);\r\nconst entryFile = process.argv[1];\r\n\r\nif (entryFile === __filename) {\r\n    console.log('Starting universal project aggregation...');\r\n\r\n    // Allow format selection via command line argument\r\n    const format = process.argv[3] || 'yaml';\r\n    let rootDir = process.argv[2];\r\n\r\n    // Check if first arg is format instead of dir\r\n    if (rootDir === 'json' || rootDir === 'yaml') {\r\n        rootDir = process.cwd();\r\n    } else {\r\n        rootDir = rootDir || process.cwd();\r\n    }\r\n\r\n    if (process.argv.includes('json') || format.toLowerCase() === 'json') {\r\n        createFullCorpusRecursiveJSON(rootDir);\r\n    } else {\r\n        createFullCorpusRecursive(rootDir);\r\n    }\r\n}\r\n"
    tokens: 4007
    size: 11617
  - path: shared\package.json
    content: "{\r\n    \"name\": \"@ece/shared\",\r\n    \"version\": \"1.0.0\",\r\n    \"private\": true,\r\n    \"main\": \"types/index.ts\"\r\n}"
    tokens: 41
    size: 109
  - path: shared\types\index.ts
    content: "/**\r\n * Core Data Structures for Sovereign Context Engine\r\n * Source of Truth for both Engine and Desktop Overlay\r\n */\r\n\r\n// ------------------------------------------------------------------\r\n// CONFIGURATION TYPES\r\n// ------------------------------------------------------------------\r\n\r\nexport interface ILLMConfig {\r\n    active: boolean;\r\n    path: string;\r\n    context_size: number;\r\n    gpu_layers: number;\r\n    temperature?: number;\r\n    projector_path?: string;\r\n}\r\n\r\nexport interface IAppConfig {\r\n    system_name: string;\r\n    ui: {\r\n        theme: 'dark' | 'light' | 'system';\r\n        transparency: boolean;\r\n        always_on_top: boolean;\r\n        shortcuts: {\r\n            toggle_overlay: string;\r\n        };\r\n    };\r\n    models: {\r\n        orchestrator: ILLMConfig;\r\n        main: ILLMConfig;\r\n        vision: ILLMConfig;\r\n    };\r\n    storage: {\r\n        db_path: string;\r\n        backup_path: string;\r\n    };\r\n    network: {\r\n        api_port: number;\r\n        websocket_port: number;\r\n    };\r\n}\r\n\r\n// ------------------------------------------------------------------\r\n// DATA TYPES\r\n// ------------------------------------------------------------------\r\n\r\nexport type ContextSource = 'file' | 'clipboard' | 'vision' | 'audio' | 'web';\r\n\r\nexport interface IContextItem {\r\n    id: string;            // UUID\r\n    content: string;       // The raw text/content\r\n    source: ContextSource; // Where did it come from?\r\n    timestamp: number;     // Unix Epoch\r\n    metadata: {\r\n        filePath?: string;\r\n        windowTitle?: string;\r\n        url?: string;\r\n        tags?: string[];\r\n    };\r\n    embedding?: number[];  // Vector representation (optional on client)\r\n}\r\n\r\nexport interface IChatMessage {\r\n    role: 'user' | 'assistant' | 'system';\r\n    content: string;\r\n    timestamp: number;\r\n    thoughts?: string;     // Chain of thought (reasoning)\r\n}\r\n"
    tokens: 602
    size: 1873
  - path: specs\context_assembly_findings.md
    content: "# Context Assembly Findings & Optimization Report\r\n\r\n## What Happened?\r\nDuring development and testing of the context assembly system, several important findings emerged regarding how context is retrieved, assembled, and presented to the LLM. This document captures the key findings and optimizations discovered during the process.\r\n\r\n## The Cost\r\n- Initial context assembly was inefficient and slow\r\n- Poor relevance ranking in search results\r\n- Memory budget management issues\r\n- Inconsistent context presentation across different query types\r\n\r\n## The Rule\r\n1. **The 70/30 Split:** When assembling context, allocate 70% of the character budget to Direct Matches (Keyword/Vector) and 30% to Associative Matches (Shared Tags).\r\n\r\n2. **Tag Harvesting:** Extract semantic tags from the Direct Matches to find \"Neighboring\" memories.\r\n\r\n3. **Unified Stream:** Present both Direct and Associative snippets in the same output stream, clearly labeled.\r\n\r\n4. **Memory Budget Management:**\r\n   - Set a maximum character limit for context assembly (default 5000 chars)\r\n   - Implement progressive loading for large context requests\r\n   - Use sliding window approach for temporal context\r\n\r\n5. **Relevance Ranking:**\r\n   - Use BM25 algorithm for keyword-based relevance\r\n   - Implement semantic similarity for vector-based matching\r\n   - Combine both approaches for hybrid search results\r\n\r\n6. **Performance Optimization:**\r\n   - Cache frequent queries to improve response time\r\n   - Implement pagination for large result sets\r\n   - Use asynchronous loading where possible\r\n\r\n## Key Findings\r\n- Direct matches provide the most relevant context for specific queries\r\n- Associative matches help with concept exploration and discovery\r\n- The combination of both approaches provides the most comprehensive context\r\n- Character budget management is crucial for performance and cost efficiency"
    tokens: 711
    size: 1878
  - path: specs\doc_policy.md
    content: "# Documentation Policy (Root Coda)\r\n\r\n**Status:** Active | **Authority:** Human-Locked\r\n\r\n## Core Philosophy\r\n1. **Code is King:** Code is the only source of truth. Documentation is a map, not the territory.\r\n2. **Synchronous Testing:** EVERY feature or data change MUST include a matching update to the Test Suite.\r\n3. **Visuals over Text:** Prefer Mermaid diagrams to paragraphs.\r\n4. **Brevity:** Text sections must be <500 characters.\r\n5. **Pain into Patterns:** Every major bug must become a Standard.\r\n6. **LLM-First Documentation:** Documentation must be structured for LLM consumption and automated processing.\r\n7. **Change Capture:** All significant system improvements and fixes must be documented in new Standard files.\r\n\r\n## User-Facing Documentation\r\n\r\n### `QUICKSTART.md` (Root)  **PRIMARY USER GUIDE**\r\n*   **Role:** First-time user onboarding and daily workflow reference.\r\n*   **Content:** Data ingestion methods, deduplication logic, backup/restore, search patterns.\r\n*   **Audience:** New users, daily reference for workflow.\r\n*   **Authority:** Canonical guide for how users interact with ECE.\r\n\r\n### `README.md` (Root)\r\n*   **Role:** Project overview, installation, and quick start.\r\n*   **Content:** What ECE is, how to install, link to QUICKSTART.md.\r\n\r\n## Data Ingestion Standards\r\n\r\n### Unified Ingestion Flow\r\n```\r\n\r\n  INPUT METHODS (All paths lead to CozoDB)                        \r\n\r\n  1. Drop files  context/           (Watcher auto-ingests)       \r\n  2. Corpus YAML  context/          (read_all.js output)         \r\n  3. API POST  /v1/ingest           (Programmatic)               \r\n  4. Backup restore  backups/       (Session resume)             \r\n\r\n                            \r\n\r\n  DEDUPLICATION LAYER                                             \r\n\r\n   Hash match  Skip (exact duplicate)                           \r\n   >80% Jaccard  Skip (semantic duplicate)                      \r\n   50-80% Jaccard  New version (temporal folding)               \r\n   <50% Jaccard  New document                                   \r\n   >500KB  Reject (Standard 053: FTS poisoning)                \r\n\r\n                            \r\n\r\n  CozoDB GRAPH  Mirror  context/mirrored_brain/                \r\n\r\n```\r\n\r\n### Corpus File Format (read_all.js output)\r\n```yaml\r\nproject_structure: \"C:/path/to/project\"\r\nfiles:\r\n  - path: \"src/index.js\"\r\n    content: \"// file content...\"\r\n  - path: \"README.md\"\r\n    content: \"# Project...\"\r\nmetadata:\r\n  total_files: N\r\n  timestamp: \"ISO-8601\"\r\n```\r\n\r\n### Ingestion Rules\r\n1. **Max Content Size:** 500KB per file (Standard 053: CozoDB Pain Points)\r\n2. **Auto-Bucketing:** Top-level folder name = bucket; root files  `pending`\r\n3. **Corpus Detection:** Files with `project_structure:` + `files:` array are extracted\r\n4. **Temporal Folding:** Search shows latest version, history timestamps collapsed\r\n\r\n## Structure\r\n\r\n### 1. The Blueprint (`specs/spec.md`)\r\n*   **Role:** The single architectural source of truth.\r\n*   **Format:** \"Visual Monolith\".\r\n*   **Content:** High-level diagrams (Kernel, Memory, Logic, Bridge). No deep implementation details.\r\n\r\n### 2. Search Patterns (`specs/search_patterns.md`)\r\n*   **Role:** Document the new semantic search and temporal folding capabilities.\r\n*   **Format:** Examples and usage guidelines.\r\n*   **Content:** How to leverage semantic intent translation and temporal folding for optimal results.\r\n\r\n## CozoDB Integration Challenges & Solutions\r\n\r\n### Issue Description\r\nDuring development, we encountered significant challenges integrating CozoDB with the ECE_Core project, particularly around the native module loading and ES module compatibility.\r\n\r\n### Root Cause\r\nThe `cozo-node` package is a native addon that exports functions directly rather than a class. When importing in an ES module environment, the import syntax needs to be adjusted to handle the CommonJS module correctly.\r\n\r\n### Solution Implemented\r\nInstead of trying to instantiate a `CozoDb` class, we now use the individual functions exported by the module:\r\n- `open_db()` - Creates a database instance and returns a database ID\r\n- `query_db()` - Executes queries against a database using its ID\r\n- `close_db()` - Closes the database connection\r\n\r\n### Key Learnings\r\n1. Native modules in Node.js environments can have compatibility issues between CommonJS and ES modules\r\n2. The `cozo-node` package exports functions directly, not a class\r\n3. Proper error handling is essential when working with native modules\r\n4. The database ID system requires careful management to prevent memory leaks\r\n\r\n### Testing Approach\r\nCreated `test-cozo.js` to verify the native module functionality independently before integrating into the main codebase.\r\n\r\n### Prevention Measures\r\n- Always test native module integrations in isolation first\r\n- Verify the actual export structure of third-party modules\r\n- Implement proper cleanup routines for database connections\r\n- Add comprehensive error handling for native module failures\r\n\r\n## NER Standardization (CPU-First Discovery)\r\n\r\n### Issue Description\r\nThe \"Teacher\" component uses local AI to discover tags without calling the expensive LLM. Initially, we attempted to use GLiNER (Zero-Shot NER), but encountered significant compatibility issues with the `transformers.js` library and ONNX runtime.\r\n\r\n### Root Cause\r\n1.  **Unsupported Architecture:** GLiNER uses a custom architecture not natively supported by standard `transformers.js` pipelines.\r\n2.  **Model Availability:** The ONNX community builds for GLiNER are fragmented and often failed to download or run reliably.\r\n3.  **Dependency Hell:** Attempting to force GLiNER support triggered complex native dependency chains (Sharp/libvips) that are unstable on Windows.\r\n\r\n### Solution Implemented\r\nSwitched the \"Teacher\" to a standard **BERT-based Named Entity Recognition (NER)** model (`Xenova/bert-base-NER`).\r\n\r\n**Benefits:**\r\n*   **Native Support:** Works out-of-the-box with `token-classification` pipeline.\r\n*   **Stability:** No custom inference logic or \"hacky\" dependency overrides needed.\r\n*   **Reliability:** The model is a staple of the Hugging Face ecosystem and extremely unlikely to disappear.\r\n\r\n### Standard 070: Local Discovery\r\n*   **Primary:** `Xenova/bert-base-NER` (Quantized ONNX)\r\n*   **Fallback:** `Xenova/bert-base-multilingual-cased-ner-hrl`\r\n*   **Failsafe:** Main LLM (Orchestrator) via \"Tag Infection\" prompts.\r\n\r\n## Native Module Best Practices\r\n\r\n### Core Principles\r\n1. **Graceful Degradation**: Services should continue to function when native modules are unavailable\r\n2. **Platform Compatibility**: Always test on target platforms before deployment\r\n3. **Error Handling**: Implement fallback mechanisms for missing dependencies\r\n4. **Documentation**: Record integration challenges and solutions for future reference\r\n\r\n### Implementation Guidelines\r\n- Use WASM/JavaScript alternatives when possible to avoid native compilation issues\r\n- Implement try/catch blocks around native module operations\r\n- Provide meaningful error messages and fallback behaviors\r\n- Document platform-specific installation requirements\r\n- Test error conditions and fallback paths regularly\r\n\r\n### Key Learnings from Recent Issues\r\n- Native modules can cause platform-specific issues that impact system stability\r\n- Graceful error handling is essential for robust systems\r\n- Proper documentation of integration challenges helps future development\r\n- Fallback mechanisms ensure core functionality remains available\r\n- Always verify the actual export structure of third-party modules before integration"
    tokens: 2956
    size: 9305
  - path: specs\findings_2026_01_19_cozodb_parser_instability.md
    content: "# Finding: CozoDB Query Parser Instability in Hybrid Search\r\n\r\n**Date:** 2026-01-19\r\n**Status:** Open\r\n**Severity:** High\r\n**Component:** Engine / Search Service / CozoDB Driver\r\n\r\n## Description\r\nDuring the implementation of \"Sovereign Bias\" and \"UniversalRAG\", persistent `coercion_failed` and `query parser unexpected input` errors were encountered when executing complex Datalog queries via the Node.js CozoDB driver (`cozo-node`).\r\n\r\nSpecifically, the FTS (Full-Text Search) query combined with Vector Search logic fails with:\r\n```\r\nError: \"The query parser has encountered unexpected input / end of input at 20..20\"\r\n```\r\nThis occurs even when the query syntax appears valid and identical queries pass in isolated test scripts (`test_fts_simple.ts`).\r\n\r\n## Symptoms\r\n- `runTraditionalSearch` fails consistently when imported into the full engine context.\r\n- `vectorSearch` triggers `coercion_failed` or similar opaque errors.\r\n- The error `20..20` suggests the parser chokes on the projection variables (e.g., `?[id, score, content...]`), possibly due to:\r\n    1. Invisible character encoding issues in TypeScript template literals.\r\n    2. Conflict with reserved keywords (though `content` worked in isolation).\r\n    3. Memory corruption or uninitialized state in the `db` instance when running multiple heavy queries.\r\n\r\n## Workaround / Resolution\r\nTo restore stable system functionality, we have implemented the following temporary measures:\r\n1.  **Disabled Vector Search**: The `vectorSearch` call in `executeSearch` matches has been replaced with a `Promise.resolve([])` stub.\r\n2.  **Simplified FTS Queries**: Search queries are restricted to single-line strings to minimize parser ambiguity.\r\n3.  **Fallback Mechanism**: The system relies heavily on the `runTraditionalSearch` (FTS) and Engram (Lexical) layers until the driver instability is resolved.\r\n\r\n## Impact\r\n- Semantic retrieval (embedding-based) is currently inactive. Use `provenance` or `buckets` for filtering.\r\n- \"Dreamer\" and \"Recall\" features relying on purely semantic matches may see reduced accuracy.\r\n- \"Sovereign Bias\" logic remains implemented but operates only on FTS/Lexical results.\r\n\r\n## Next Steps\r\n- Investigate `cozo-node` binary compatibility with the current Node.js version.\r\n- Re-enable Vector Search incrementally using simplified, isolated queries.\r\n- Audit all Datalog queries for template literal normalization.\r\n"
    tokens: 914
    size: 2412
  - path: specs\llama_servers.md
    content: "# Starting Llama Servers with ECE_Core\r\n\r\nYou can start llama.cpp-based LLM servers for both inference and embeddings via the scripts in the ECE_Core project.\r\n\r\nThe `start-llama-server.bat` and `start-embed-server.bat` scripts live in the repo root (`./start-llama-server.bat` and `./start-embed-server.bat`).\r\n\r\nHow it works:\r\n- `scripts/generate_llama_env.py` reads `src.config.settings` and prints environment variables.\r\n- The batch scripts call the helper to load configuration values from `.env` or environment, and use them to start `llama-server.exe` with appropriate flags (context, threads, GPU layers, etc.).\r\n- For interactive model selection, the scripts call `select_model.py`; if the helper supplied `MODEL`, it will use this directly.\r\n\r\nInstructions:\r\n1. Set configuration values in `.env` (for example `LLM_MODEL_PATH`, `LLM_CONTEXT_SIZE`, `LLM_THREADS`, `LLM_GPU_LAYERS`, `LLM_EMBEDDINGS_*`, `LLAMA_SERVER_EXE_PATH`, etc.).\r\n2. Start the API server:\r\n   - Open a PowerShell window in the `ECE_Core` folder and run:\r\n\r\n```powershell\r\n.\\start-llama-server.bat\r\n```\r\n\r\n3. Start the Embeddings server:\r\n\r\n```powershell\r\n.\\start-embed-server.bat\r\n```\r\n\r\nTip: You can also specify a different set of values using environment variables directly or via a custom `.env` file, and you can override the model selection interactively with `select_model.py` if needed.\r\n\r\nBatching guidance (GPU/UBATCH) \r\n--------------------------------\r\nIf you serve many small requests concurrently, we recommend keeping continuous batching enabled (it improves throughput). However, ensure that the `UBATCH` (physical batch size) is large enough to fit typical requests, and also small enough to avoid OOM on your GPU.\r\n\r\nFor NVIDIA RTX 4090 (16 GB VRAM) laptops, a reasonable starting point is to set:\r\n\r\n```env\r\nLLAMA_SERVER_UBATCH_MAX=8192\r\nLLAMA_BATCH=2048\r\nLLAMA_PARALLEL=1\r\nLLAMA_CONT_BATCHING=True\r\n```\r\n\r\nAdjust `LLAMA_SERVER_UBATCH_MAX` up or down depending on model size:\r\n- Small embedding models (e.g., 300M): you can often set a higher UBATCH.\r\n- Larger models (4B+): start conservative (4096 or 2048) and raise if the load stays stable.\r\n\r\nUse `python scripts/generate_llama_env.py` to dump settings and confirm the final `LLAMA_UBATCH` value prior to starting the server. This helper respects `LLAMA_SERVER_UBATCH_MAX` and will cap the computed UBATCH accordingly.\r\n\r\nPre-flight token validation \r\n---------------------------------\r\nECE_Core includes a pre-flight validation in the API layer that checks the size of the assembled prompt (in tokens) against the configured `LLAMA_SERVER_UBATCH_SIZE` micro-batch. If the prompt tokens exceed the UBATCH the service returns an HTTP 400 response advising the user to reduce the context size or increase `LLAMA_SERVER_UBATCH_SIZE`. This prevents a llama.cpp GGML assertion (encoder requires n_ubatch >= n_tokens) and reduces 500 Internal Server Errors under heavy load.\r\n\r\nDebugging:\r\n\r\nAuto-tuning helper \r\n---------------------\r\nECE_Core ships with `scripts/auto_tune_llama.py` which can recommend `LLM_CONTEXT_SIZE`, `LLAMA_SERVER_UBATCH_SIZE`, and `LLAMA_SERVER_BATCH_SIZE` based on detected GPU VRAM and model file size. Run it with `python scripts/auto_tune_llama.py` to print recommended settings or `python scripts/auto_tune_llama.py --apply` to append conservative recommendations to `ece-core/.env` (backing up the original). This can be helpful when swapping models on limited VRAM machines like the RTX 4090.\r\n"
    tokens: 1344
    size: 3490
  - path: specs\plan.md
    content: "# Anchor Core Roadmap (V2.4)\r\n\r\n**Status:** Markovian Reasoning Deployed & Context Assembly Experiments Added\r\n**Focus:** Production Polish & Verification.\r\n\r\n## Phase 1: Foundation (Completed)\r\n- [x] Pivot to WebLLM/WebGPU stack.\r\n- [x] Implement CozoDB (WASM) for memory.\r\n- [x] Create core HTML tools (`model-server-chat`, `sovereign-db-builder`, `log-viewer`).\r\n\r\n## Phase 2: Stabilization (Completed)\r\n- [x] Fix Model Loading (Quota/VRAM config).\r\n- [x] Add 14B Model Support (Qwen2.5, DeepSeek-R1).\r\n- [x] **Snapdragon Optimization**: Implemented Buffer Override (256MB).\r\n\r\n## Phase 2.5: Root Refactor (Completed)\r\n- [x] **Kernel Implementation**: Created `sovereign.js` (Unified Logger, State, Hardware).\r\n- [x] **The Ears**: Refactored `root-mic.html` to Root Architecture.\r\n- [x] **The Stomach**: Refactored `sovereign-db-builder.html` to Root Architecture.\r\n\r\n## Phase 3: Markovian Reasoning & Context Optimization (Completed)\r\n- [x] **Scribe Service**: Created `engine/src/services/scribe.js` for rolling state\r\n- [x] **Context Weaving**: Upgraded `inference.js` to auto-inject session state\r\n- [x] **Dreamer Service**: Enhanced `dreamer.js` with batch processing to prevent OOM errors\r\n- [x] **Semantic Translation**: Added intent translation via local SLM\r\n- [x] **Context Experiments**: Created `engine/tests/context_experiments.js` for optimal context window sizing\r\n- [x] **The Brain**: Refactored `model-server-chat.html` to Root Architecture (Graph-R1 preservation).\r\n\r\n## Phase 3-8: [Archived] (Completed)\r\n*See `specs/tasks.md` for detailed historical phases.*\r\n\r\n## Phase 9: Node.js Monolith & Snapshot Portability (Completed)\r\n- [x] **Migration**: Move from Python/Browser Bridge to Node.js Monolith (Standard 034).\r\n- [x] **FTS Optimization**: Implement native CozoDB BM25 search.\r\n- [x] **Operational Safety**: Implement detached execution and logging protocols (Standard 035/036).\r\n- [x] **Snapshot Portability**: Create \"Eject\" (Backup) and \"Hydrate\" (Restore) workflow (Standard 037).\r\n\r\n## Phase 10: Cortex Upgrade (Completed)\r\n- [x] **Local Inference**: Integrate `node-llama-cpp` for GGUF support (Standard 038).\r\n- [x] **Multi-Bucket Schema**: Migrate from single `bucket` to `buckets: [String]` (Standard 039).\r\n- [x] **Dreamer Service**: Implement background self-organization via local LLM.\r\n- [x] **Cozo Hardening**: Resolve list-handling and `unnest` syntax errors (Standard 040).\r\n- [x] **ESM Interop**: Fix dynamic import issues for native modules in CJS.\r\n\r\n## Phase 11: Markovian Reasoning Engine (Completed)\r\n- [x] **Scribe Service**: Implement rolling session state compression (Standard 041).\r\n- [x] **Context Weaving**: Auto-inject Markovian state into inference.\r\n- [x] **Test Suite**: Create `engine/tests/suite.js` for API verification.\r\n- [x] **Benchmark Tool**: Create `engine/tests/benchmark.js` for accuracy testing.\r\n- [x] **Configuration Hardening**: Externalize paths, fix package.json, add validation.\r\n\r\n## Phase 12: Production Polish (In Progress)\r\n- [ ] **UI/UX Overhaul**: Implement \"Flight Recorder\" aesthetic for the dashboard.\r\n- [ ] **Chat Cockpit**: Enhance `interface/chat.html` with conversation history.\r\n- [ ] **Streaming Responses**: Implement SSE for real-time token streaming.\r\n- [ ] **Android Compatibility**: Ensure Node.js monolith runs in Termux.\r\n- [ ] **Clean Install Script**: Create one-click setup for new users.\r\n\r\n## Phase 13: Enterprise & Advanced RAG (Up Next)\r\n- [ ] **Backup System**: Robust snapshotting/restore (Feature 7).\r\n- [ ] **Smart Context**: Middle-Out \"Rolling Slicer\" logic (Feature 8).\r\n- [ ] **RAG IDE**: Live Context Visualization in UI (Feature 9).\r\n- [ ] **Provenance**: Trust Hierarchy switching (Feature 10).\r\n\r\n## Phase 14: Federation (Future)\r\n- [ ] **Device Sync**: Sync snapshots across devices (P2P or cloud).\r\n- [ ] **Local-First Cloud**: Optional encrypted backup.\r\n- [ ] **Multi-Model**: Support multiple models loaded simultaneously."
    tokens: 1480
    size: 3948
  - path: specs\search_patterns.md
    content: "# Search Patterns & Query Syntax for ECE\r\n\r\n## What Happened?\r\nThe system needed a standardized approach for search queries to ensure consistent behavior across all search operations. This document defines the search patterns, query syntax, and optimization strategies for the ECE system.\r\n\r\n## The Cost\r\n- Inconsistent search behavior across different components\r\n- Users experiencing different search results depending on which interface they used\r\n- Difficulty in optimizing search queries for performance\r\n- Lack of clear guidance on how to structure search queries for best results\r\n\r\n## The Rule\r\n1. **Standardized Query Structure:** All search queries should follow the same basic structure:\r\n   - Simple keyword search: Just enter the keywords you're looking for\r\n   - Bucket filtering: Use `bucket:name` to filter results by bucket\r\n   - Phrase matching: Use `\"exact phrase\"` to match exact phrases\r\n   - Complex queries: Combine keywords, buckets, and phrases as needed\r\n\r\n2. **Search Optimization Strategies:**\r\n   - **Broad Strategy:** For concept exploration and general information retrieval\r\n   - **Precise Strategy:** For specific information and exact matches\r\n   - **Hybrid Strategy:** For complex queries that need both concepts and specifics\r\n\r\n3. **Bucket-Based Organization:**\r\n   - Use buckets to organize and filter search results\r\n   - Common buckets include: `core`, `development`, `research`, `personal`, `codebase`\r\n   - Create new buckets as needed for specific contexts or projects\r\n\r\n4. **Character Limit Considerations:**\r\n   - Default character limit for search results is 5000 characters\r\n   - This can be adjusted based on the specific needs of the search\r\n   - Larger limits may impact performance but provide more context\r\n\r\n5. **Semantic Intent Translation:**\r\n   - The system will automatically translate natural language queries to optimized search parameters\r\n   - This includes identifying relevant buckets and search strategies\r\n   - Users can override automatic classification if needed"
    tokens: 768
    size: 2030
  - path: specs\spec.md
    content: "# ECE_Core - Technical Specification\r\n\r\n## Mission\r\n\r\nBuild a **personal external memory system** as an assistive cognitive tool using:\r\n- **CozoDB (RocksDB)**: Unified graph-relational-vector-fts engine (Replacing Neo4j/Redis).\r\n- **Tag-Walker Protocol**: Graph-based associative retrieval (Replacing legacy Vector Search).\r\n- **Mirror 2.0**: Tangible Knowledge Graph filesystem projections.\r\n- **Local-first LLM integration**: `node-llama-cpp` for GGUF support.\r\n\r\n**Current**: Node.js Monolith with CozoDB backend.\r\n**Tools**: Loaded via `PluginManager` from `plugins/` directory.\r\n\r\n## Architecture Overview\r\n\r\n### System Architecture (Tag-Walker)\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph \"Interface Layer\"\r\n        UI[Frontend (React)] -->|API| Server[Express Server]\r\n        Overlay[Desktop Overlay] -->|Loads| UI\r\n        Inbox[Inbox Directory] -.->|File Watch| Watcher[Watchdog Service]\r\n    end\r\n\r\n    subgraph \"Core Engine (Node.js)\"\r\n        Server --> Provider[LLM Provider]\r\n        Watcher --> Refiner[Refiner Service]\r\n        \r\n        subgraph \"Ingestion Pipeline\"\r\n            Refiner -->|Sanitize| Assassin[Key Assassin]\r\n            Assassin -->|Clean| Atomizer[Atomizer]\r\n            Atomizer -->|Zero-Vector Stub| Cozo\r\n        end\r\n        \r\n        subgraph \"Inference System\"\r\n            Provider -->|Routing| ChatWorker[ChatWorker (Chat Model)]\r\n            Provider -->|Tasks| Orch[OrchestratorWorker]\r\n        end\r\n        \r\n        subgraph \"Context Manager\"\r\n            Search[Tag-Walker Search] -->|FTS + Pivot| Slicer[Context Slicer]\r\n            Slicer -->|Assembly| Provider\r\n        end\r\n    end\r\n\r\n    subgraph \"Persistence Layer\"\r\n        Cozo[CozoDB (RocksDB)] -->|Mirror 2.0| Mirror[mirrored_brain/ @bucket/#tag]\r\n    end\r\n\r\n    style ChatWorker fill:#f9f,stroke:#333\r\n    style Orch fill:#bbf,stroke:#333\r\n    style Cozo fill:#dfd,stroke:#333\r\n```\r\n\r\n### Context Assembly Flow (Tag-Walker)\r\n\r\n```mermaid\r\ngraph LR\r\n    Query[User Query] --> FTS[Phase 1: Anchor FTS]\r\n    FTS --> Harvest[Phase 2: Tag Harvest]\r\n    Harvest --> Walk[Phase 3: Neighbor Walk]\r\n    \r\n    subgraph \"Retrieval Layer\"\r\n        FTS -->|70% Budget| Ranking\r\n        Walk -->|30% Budget| Ranking\r\n    end\r\n    \r\n    Ranking[Score-Based Mix] --> Budget{Token Budget?}\r\n    \r\n    Budget -->|Fit| FinalContext[Final Context Prompt]\r\n    Budget -->|Overflow| Slicing[Smart Slicing]\r\n    \r\n    subgraph \"Smart Slicing\"\r\n        Slicing --> Punctuation{Find Sentence End}\r\n        Punctuation -->|Found| Cut[Truncate]\r\n        Punctuation -->|Not Found| HardCut[Hard Cut]\r\n    end\r\n    \r\n    Cut --> FinalContext\r\n    HardCut --> FinalContext\r\n```\r\n\r\n## Graph Architecture: CozoDB\r\n\r\n**Verifier Agent** - Truth Verification\r\n- **Role**: Fact-checking via Empirical Distrust\r\n- **Method**: Provenance-aware scoring (primary sources > summaries)\r\n- **Goal**: Reduce hallucinations, increase factual accuracy\r\n\r\n**Distiller Agent** - Memory Compression\r\n- **Role**: Memory summarization and compression\r\n- **Method**: LLM-assisted distillation with salience scoring\r\n- **Goal**: Maintain high-value context, prune noise\r\n\r\n**Archivist Agent** - Memory Maintenance\r\n- **Role**: Knowledge base maintenance, freshness checks\r\n- **Method**: Scheduled verification, stale node detection\r\n- **Goal**: Keep memory graph current and trustworthy\r\n\r\n**Memory Weaver** - Automated Relationship Repair\r\n- **Role**: Automated graph relationship repair and optimization\r\n- **Method**: Tag-based similarity with audit trail (`auto_commit_run_id`)\r\n- **Goal**: Maintain graph integrity without vectors\r\n\r\n### Reasoning Architecture: Tag-Walker + CozoDB\r\n\r\n**Tag-Walker Reasoning Pattern**:\r\n1. **Anchor** - FTS match for direct entities\r\n2. **Pivot** - Extract tags from anchors\r\n3. **Walk** - Retrieve neighbors via shared tags\r\n4. **Boost** - Apply Sovereign provenance boosting\r\n5. **Synthesize** - Present multi-node context to LLM\r\n\r\n**Unified Memory**: Consolidated context management\r\n- **Hot Context**: Active session memories in CozoDB\r\n- **Mirrored Brain**: Filesystem projection via @bucket/#tag structure\r\n\r\n### Tool Architecture: UTCP Plugin System\r\n\r\n**Current Implementation**: Plugin-based UTCP (Simple Tool Mode)\r\n- Discovery via `plugins/` directory\r\n- Safety layers with whitelist/blacklist\r\n- Human confirmation flows for dangerous operations\r\n\r\n**Available Tools**:\r\n- `web_search` - DuckDuckGo with result limits\r\n- `filesystem_read` - File operations with path restrictions\r\n- `shell_execute` - Command execution with safety checks\r\n- `mgrep` - Semantic code search with context\r\n\r\n## Infinite Context Pipeline\r\n\r\n### Phase 1: Hardware Foundation\r\n- **64k Context Windows**: All LLM servers boot with 65,536 token capacity\r\n- **GPU Optimization**: Full layer offload with Q8 quantized KV cache\r\n- **Flash Attention**: Enabled when available for optimal long-context performance\r\n\r\n### Phase 2: Context Rotation Protocol\r\n- **Monitoring**: ContextManager monitors total context length\r\n- **Trigger**: When context approaches 55k tokens (safety buffer for 64k window)\r\n- **Compression**: Distiller compresses old segments into \"Narrative Gists\"\r\n- **Storage**: Gists stored in Neo4j as `(:ContextGist)` nodes with `[:NEXT_GIST]` relationships\r\n- **Rewriting**: New context = `[System Prompt] + [Historical Gists Summary] + [Recent Context] + [New Input]`\r\n\r\n### Phase 3: Graph-R1 Enhancement\r\n- **Historical Retrieval**: GraphReasoner includes `:ContextGist` nodes in retrieval\r\n- **Continuity Maintenance**: Reasoning flow maintained across context rotations\r\n- **Temporal Awareness**: Reasoning considers chronological relationships in gists\r\n\r\n## API Specification\r\n\r\n### Core Endpoints (Port 8000)\r\n\r\n**Chat Interface**:\r\n- `POST /chat/stream` - Streaming conversation with full memory context\r\n- Request: `{\"session_id\": str, \"message\": str, \"stream\": bool}`\r\n- Response: Streaming SSE with full context injection\r\n\r\n**Memory Operations**:\r\n- `POST /v1/memory/search` - Tag-Walker search\r\n- `GET /v1/buckets` - List active buckets\r\n- `POST /v1/ingest` - Manual atom ingestion\r\n- `GET /v1/backup` - Snapshot RocksDB state\r\n\r\n**Health & Info**:\r\n- `GET /health` - Server health check\r\n- `GET /v1/models` - Available models\r\n- `GET /health/memory` - Memory system status\r\n\r\n**MCP Integration** (when enabled):\r\n- `GET /mcp/tools` - Available memory tools\r\n- `POST /mcp/call` - Execute memory tools\r\n\r\n## Configuration\r\n\r\n### Required Parameters (in `.env` or sovereign.yaml)\r\n- `MODELS_DIR` - Path to GGUF model storage\r\n- `NOTEBOOK_DIR` - Root of the target brain for mirroring\r\n- `LLM_GPU_LAYERS` - GPU offload settings\r\n- `PORT` - Engine API port (default: 3000)\r\n\r\n### Optional Parameters\r\n- `ECE_REQUIRE_AUTH` - Enable API token authentication (default: false)\r\n- `ECE_API_KEY` - Static API key when auth enabled\r\n- `MCP_ENABLED` - Enable Model Context Protocol integration (default: true)\r\n- `VERIFIER_AGENT_ENABLED` - Enable truth-checking agent (default: true)\r\n- `ARCHIVIST_AGENT_ENABLED` - Enable memory maintenance agent (default: true)\r\n- `DISTILLER_AGENT_ENABLED` - Enable summarization agent (default: true)\r\n\r\n## Security\r\n\r\n### Authentication\r\n- Optional API token authentication (controlled by `ECE_REQUIRE_AUTH`)\r\n- Session isolation with UUID-based session IDs\r\n- Memory access limited to owner's session\r\n\r\n### Authorization\r\n- Path restrictions on filesystem operations\r\n- Command whitelisting for shell execution\r\n- Rate limiting on all endpoints\r\n- Input validation on all parameters\r\n\r\n### Data Protection\r\n- All data stored locally by default\r\n- End-to-end encryption for sensitive memories (optional)\r\n- Audit logging for all memory operations\r\n- Traceability for automated repairs and context rotations\r\n\r\n## Performance Optimization\r\n\r\n### Hardware Recommendations\r\n- **Minimum**: 16GB RAM, CUDA-capable GPU (RTX series)\r\n- **Recommended**: 32GB+ RAM, RTX 4090 or similar\r\n- **Context Windows**: 64k requires ~8GB VRAM for KV cache with 7B-14B models\r\n\r\n### Memory Management\r\n- **Hot Cache**: rocksdb backed CozoDB for all active session and entity memories.\r\n- **Mirrored Brain**: File-based projection in `@bucket/#tag` format for human audit.\r\n- **Cleanup Strategy**: Periodic vacuuming of FTS indices in CozoDB.\r\n- **Caching Strategy**: RocksDB block cache handles L1 performance; Mirror 2.0 provides L2 observability.\r\n\r\n## Integration Points\r\n\r\n### With Anchor CLI\r\n- HTTP API communication on configured port (default: 8000)\r\n- Streaming responses via Server-Sent Events\r\n- Memory operations through dedicated endpoints\r\n\r\n### With Browser Extension\r\n- HTTP API communication for context injection and memory saving\r\n- Streaming chat interface via Side Panel\r\n- Page content reading and memory ingestion\r\n\r\n### With LLM Servers\r\n- OpenAI-compatible API for LLM communication\r\n- Streaming response handling via SSE\r\n- Context window management with rotation protocol"
    tokens: 3231
    size: 8911
  - path: specs\standards\00-CORE\002-cache-api-security-policy.md
    content: "# Standard 002: Cache API Security Policy\r\n\r\n## What Happened?\r\nBrowser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as \"data in RAM\" rather than \"persistent storage\", causing initialization failures.\r\n\r\n## The Cost\r\n- Multiple failed model loading attempts\r\n- Browser security errors preventing WebGPU initialization\r\n- \"Stealth Mode\" required to bypass strict policies\r\n- Significant debugging time to understand browser security model\r\n\r\n## The Rule\r\n1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:\r\n   ```python\r\n   class NoCacheStaticFiles(StaticFiles):\r\n       async def __call__(self, scope, receive, send):\r\n           async def send_wrapper(message):\r\n               if message['type'] == 'http.response.start':\r\n                   headers = message.get('headers', [])\r\n                   headers.extend([\r\n                       (b\"Cache-Control\", b\"no-store, no-cache, must-revalidate, proxy-revalidate\"),\r\n                       (b\"Pragma\", b\"no-cache\"),\r\n                       (b\"Expires\", b\"0\"),\r\n                   ])\r\n                   message['headers'] = headers\r\n               await send(message)\r\n           await super().__call__(scope, receive, send_wrapper)\r\n   ```\r\n\r\n2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.\r\n\r\n3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components."
    tokens: 586
    size: 1610
  - path: specs\standards\00-CORE\012-context-utility-manifest.md
    content: "<<<<<<< HEAD\r\n# Standard 012: Context Utility Manifest\r\n\r\n**Authority:** Active | **Philosophy:** Invisible Infrastructure\r\n\r\n## The Principle\r\nThe Anchor Core is not a \"Chat App\". It is a **Context Utility** (like electricity or WiFi).\r\n1.  **Headless First**: The system must provide value without a visible UI window.\r\n2.  **Passive Observation**: Data ingestion should happen automatically (Daemon Eyes) rather than requiring manual user input.\r\n3.  **Universal Availability**: Context must be accessible via standard HTTP endpoints (`/v1/memory/search`) to any client (Terminal, VS Code, Browser).\r\n\r\n## The Rules\r\n1.  **No UI Blocking**: Long-running tasks (like VLM analysis) MUST run in background threads/processes.\r\n2.  **Zero-Touch Ingestion**: Screen/Audio capture must require zero clicks after initial activation.\r\n3.  **Ground Truth**: All ingested context is immutable \"Ground Truth\" until proven otherwise.\r\n=======\r\n# Standard 012: Context Utility Manifest - The Invisible Infrastructure\r\n\r\n## What Happened?\r\nThe Anchor Core system was originally conceived as a chat application, but has evolved into a unified cognitive infrastructure. The system now needs to transition from \"active user input\" to \"passive observation\" to function as truly invisible infrastructure like electricity - always present but never demanding attention.\r\n\r\n## The Cost\r\n- UI bloat with multiple chat interfaces competing for user attention\r\n- Manual data entry required to populate context\r\n- Users having to copy/paste information instead of automatic capture\r\n- Architecture treating UI as primary rather than as debugging tool\r\n- Missing opportunity to create true \"ambient intelligence\"\r\n\r\n## The Rule\r\n1. **Headless by Default**: All core functionality must operate without user interface interaction\r\n   ```python\r\n   # Core services run as background daemons\r\n   daemon_services = [\r\n       \"memory_graph\",      # CozoDB persistence\r\n       \"gpu_engine\",        # WebLLM inference\r\n       \"context_capture\",   # Screen/Audio observation\r\n       \"data_ingestion\"     # Memory writing\r\n   ]\r\n   ```\r\n\r\n2. **Passive Observation**: System captures context automatically rather than waiting for user input\r\n   - **Eyes**: Automated screen sampling and OCR\r\n   - **Ears**: Continuous audio transcription (when enabled)\r\n   - **Memory**: Automatic ingestion without user intervention\r\n\r\n3. **Architecture Priority**: `webgpu_bridge.py` is the nervous system; UIs are merely debugging/interaction tools\r\n   - UIs are temporary visualization layers\r\n   - Core logic exists independently of any UI\r\n   - Background services operate without UI presence\r\n\r\n4. **Invisible Utility**: The system should function like electricity - always available, rarely noticed, essential infrastructure\r\n   - Zero user interaction required for core functions\r\n   - Automatic context capture and storage\r\n   - Seamless integration with user's workflow\r\n\r\n5. **Context First**: Prioritize capturing and understanding user context over responding to queries\r\n   - Short-term context populated automatically\r\n   - Long-term memory built passively\r\n   - Responses based on observed reality rather than explicit input\r\n\r\n## Implementation Requirements\r\n\r\n### Core Daemon Services\r\n- **Memory Daemon**: Continuous CozoDB operations in background\r\n- **Vision Daemon**: Automated screen capture and OCR (daemon_eyes.py)\r\n- **Audio Daemon**: Optional background audio processing\r\n- **Ingestion Daemon**: Automatic data flow to memory graph\r\n\r\n### API-First Design\r\n- All functionality accessible via API endpoints\r\n- UIs as thin clients consuming API services\r\n- Background services operating independently\r\n\r\n### Error Handling\r\n- Daemons must handle errors gracefully without user intervention\r\n- Automatic recovery from common failures\r\n- Silent operation with optional logging for debugging\r\n\r\n## Transition Protocol\r\n\r\nWhen implementing new features:\r\n1. Design for headless operation first\r\n2. Add UI as optional visualization layer\r\n3. Ensure all functionality available via API\r\n4. Test daemon operation independently of UI\r\n\r\nThis standard ensures that Anchor Core evolves into true invisible infrastructure rather than remaining a traditional application.\r\n>>>>>>> 3cd511631b7eaf7d033a1bacccff36325545fc78\r\n"
    tokens: 1592
    size: 4285
  - path: specs\standards\00-CORE\027-no-resurrection-mode.md
    content: "# Standard 027: No Resurrection Mode for Manual Ghost Engine Control\r\n\r\n## What Happened?\r\nThe system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues for users who wanted to use an existing browser window or manually control when the Ghost Engine connects. Users needed an option to disable the automatic resurrection protocol and connect the Ghost Engine manually when needed.\r\n\r\n## The Cost\r\n- Unnecessary browser processes launched automatically\r\n- Resource usage when Ghost Engine not needed\r\n- Inability to use existing browser windows for Ghost Engine operations\r\n- Confusion when multiple browser instances were running\r\n- Users wanting more control over when the Ghost Engine connects\r\n\r\n## The Rule\r\n1. **Environment Variable Control**: The system must support a `NO_RESURRECTION_MODE=true` environment variable to disable automatic Ghost Engine launching.\r\n\r\n2. **Conditional Launch**: When `NO_RESURECTION_MODE=true`, the system shall NOT automatically launch the Ghost Engine during startup.\r\n\r\n3. **Manual Connection**: In no resurrection mode, users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.\r\n\r\n4. **Clear Messaging**: The system shall provide clear instructions to users when no resurrection mode is enabled, indicating they need to open ghost.html manually.\r\n\r\n5. **Resource Management**: When no resurrection mode is enabled, the system shall not attempt to kill browser processes during shutdown.\r\n\r\n6. **API Behavior**: API endpoints that require the Ghost Engine shall return appropriate 503 errors with clear messaging when the Ghost Engine is disconnected, regardless of resurrection mode setting.\r\n\r\n## Implementation\r\n- Set environment variable: `set NO_RESURRECTION_MODE=true` before running `start-anchor.bat`\r\n- The Bridge will log a message indicating manual connection is required\r\n- Users open `http://localhost:8000/ghost.html` in their browser to connect the Ghost Engine\r\n- All functionality remains the same, just with manual control over Ghost Engine connection"
    tokens: 825
    size: 2118
  - path: specs\standards\00-CORE\028-default-no-resurrection-mode.md
    content: "# Standard 028: Configuration-Driven System with Default No Resurrection Mode\r\n\r\n## What Happened?\r\nThe system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues with resource usage and prevented users from controlling when the Ghost Engine connects. The system now defaults to \"No Resurrection Mode\" where the Ghost Engine must be manually started by opening ghost.html in the browser. Additionally, ALL system variables are now abstracted to a central configuration file (config.json) to support future settings menu implementation.\r\n\r\n## The Cost\r\n- Excessive resource usage from automatically launching headless browser\r\n- Browser processes that couldn't be controlled by the user\r\n- Confusion when multiple browser instances were running\r\n- Unnecessary complexity in the startup process\r\n- Users wanting more control over when the Ghost Engine connects\r\n- Hard-coded values throughout the codebase that made customization difficult\r\n\r\n## The Rule\r\n1. **Default Behavior**: The system shall default to `NO_RESURRECTION_MODE=true`, meaning the Ghost Engine is not automatically launched.\r\n\r\n2. **Manual Connection**: Users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.\r\n\r\n3. **Environment Override**: Users can set `NO_RESURRECTION_MODE=false` to return to auto-launching behavior.\r\n\r\n4. **Queued Operations**: When Ghost Engine is disconnected, operations shall be queued and processed when connection is established.\r\n\r\n5. **Clear Messaging**: The system shall provide clear instructions when Ghost Engine is not connected, indicating how to establish the connection.\r\n\r\n6. **Configurable Values**: All system parameters shall be configurable via the config.json file, including:\r\n   - Server settings (port, host, CORS origins)\r\n   - Ghost Engine settings (auto resurrection, browser paths, flags)\r\n   - Logging configuration (max lines, directory, format)\r\n   - Memory settings (max ingest size, default limits, char limits)\r\n   - GPU management (enabled, concurrent ops, timeout)\r\n   - Model loading (timeout, default model, base URL)\r\n   - Watchdog settings (enabled, watch directory, allowed extensions, debounce time)\r\n\r\n7. **Detached Operation**: All scripts shall run in detached mode with logging to the logs/ directory as per Standard 025.\r\n\r\n## Implementation\r\n- Default configuration sets `\"ghost_engine.auto_resurrection_enabled\": false`\r\n- The start-anchor.bat script defaults to NO_RESURRECTION_MODE=true\r\n- All system variables abstracted to config.json with config_manager.py\r\n- Watchdog logs appropriate messages when Ghost Engine is disconnected\r\n- API endpoints return 503 with clear messaging when Ghost Engine is disconnected\r\n- Files are queued for ingestion when Ghost Engine is not available\r\n- Created start_anchor_detached.py for proper detached operation with logging"
    tokens: 1115
    size: 2915
  - path: specs\standards\001-windows-console-encoding.md
    content: "# Standard 001: Windows Console Encoding\r\n\r\n## What Happened?\r\nThe Python Bridge (`webgpu_bridge.py`) crashed immediately upon launch on Windows 11. The error was `UnicodeEncodeError: 'charmap' codec can't encode character...`.\r\n\r\n## The Cost\r\n- 3 failed integration attempts.\r\n- \"Integration Hell\" state requiring full manual intervention.\r\n- Bridge stability compromised during demos.\r\n\r\n## The Rule\r\n1. **Explicit Encoding:** All Python scripts outputting to stdout must explicitly handle encoding.\r\n2. **The Fix:** Include this snippet at the top of all entry points:\r\n   ```python\r\n   import sys\r\n   if sys.platform == \"win32\":\r\n       sys.stdout.reconfigure(encoding='utf-8')\r\n   ```\r\n\r\n3. **Validation:** CI/CD or startup scripts must verify the bridge launches without encoding errors."
    tokens: 302
    size: 793
  - path: specs\standards\002-cache-api-security-policy.md
    content: "# Standard 002: Cache API Security Policy\r\n\r\n## What Happened?\r\nBrowser AI engines were blocked by strict security policies when trying to load models. The Cache API was preventing models from being treated as \"data in RAM\" rather than \"persistent storage\", causing initialization failures.\r\n\r\n## The Cost\r\n- Multiple failed model loading attempts\r\n- Browser security errors preventing WebGPU initialization\r\n- \"Stealth Mode\" required to bypass strict policies\r\n- Significant debugging time to understand browser security model\r\n\r\n## The Rule\r\n1. **NoCacheStaticFiles Class:** Always implement custom StaticFiles class with proper headers:\r\n   ```python\r\n   class NoCacheStaticFiles(StaticFiles):\r\n       async def __call__(self, scope, receive, send):\r\n           async def send_wrapper(message):\r\n               if message['type'] == 'http.response.start':\r\n                   headers = message.get('headers', [])\r\n                   headers.extend([\r\n                       (b\"Cache-Control\", b\"no-store, no-cache, must-revalidate, proxy-revalidate\"),\r\n                       (b\"Pragma\", b\"no-cache\"),\r\n                       (b\"Expires\", b\"0\"),\r\n                   ])\r\n                   message['headers'] = headers\r\n               await send(message)\r\n           await super().__call__(scope, receive, send_wrapper)\r\n   ```\r\n\r\n2. **Force RAM Storage:** Configure headers to force browser to treat models as in-memory data, bypassing strict security policies.\r\n\r\n3. **Security Bypass Protocol:** When browser AI engines fail to initialize, first check Cache API policies before debugging other components."
    tokens: 586
    size: 1610
  - path: specs\standards\003-webgpu-initialization-stability.md
    content: "# Standard 003: WebGPU Initialization Stability\r\n\r\n## What Happened?\r\nWebGPU failed to initialize properly in headless browsers, causing GPU access failures and preventing AI model execution. This occurred because browsers require visible windows for GPU access in some configurations.\r\n\r\n## The Cost\r\n- Failed AI model execution in headless environments\r\n- Hours of debugging GPU initialization issues\r\n- Unreliable AI processing in automated systems\r\n- Need for complex workarounds to achieve stable GPU access\r\n\r\n## The Rule\r\n1. **Minimized Window Approach:** Always use `--start-minimized` flag when launching headless browsers that require GPU access:\r\n   ```bash\r\n   start \"Ghost Engine\" /min msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222\r\n   ```\r\n\r\n2. **GPU Buffer Configuration:** Implement 256MB override for Adreno GPUs and other constrained hardware:\r\n   ```javascript\r\n   // In WebGPU configuration\r\n   const adapter = await navigator.gpu.requestAdapter({\r\n       powerPreference: 'high-performance',\r\n       forceFallbackAdapter: false\r\n   });\r\n   ```\r\n\r\n3. **Hardware Abstraction Layer:** Use clamp buffer techniques for Snapdragon/Mobile stability to prevent VRAM crashes.\r\n\r\n4. **Consciousness Semaphore:** Ensure resource arbitration between different components to prevent GPU memory conflicts."
    tokens: 508
    size: 1372
  - path: specs\standards\004-wasm-memory-management.md
    content: "# Standard 004: WASM Memory Management\r\n\r\n## What Happened?\r\nWASM applications experienced \"memory access out of bounds\" errors and crashes when handling large JSON payloads or complex database operations. This was particularly problematic in `sovereign-db-builder.html` and `unified-coda.html` where JSON parameters were passed to `db.run()`.\r\n\r\n## The Cost\r\n- Crashes during database operations in browser-based CozoDB\r\n- \"Maximum call stack size exceeded\" errors with large JSON payloads\r\n- Unreliable memory operations in browser-based systems\r\n- Hours of debugging memory access violations in WASM\r\n\r\n## The Rule\r\n1. **JSON Stringification:** Always properly stringify JSON parameters before passing to WASM functions:\r\n   ```javascript\r\n   // Before calling db.run() or similar WASM functions\r\n   const jsonString = JSON.stringify(data);\r\n   db.run(query, jsonString);\r\n   ```\r\n\r\n2. **Payload Size Limits:** Implement size checks before processing large JSON payloads in browser workers:\r\n   ```javascript\r\n   if (JSON.stringify(payload).length > MAX_SAFE_SIZE) {\r\n       // Handle large payloads differently or chunk them\r\n   }\r\n   ```\r\n\r\n3. **Error Handling:** Add timeout protection and fallback mechanisms for hanging WASM calls:\r\n   ```javascript\r\n   try {\r\n       const result = await Promise.race([\r\n           db.run(query),\r\n           new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))\r\n       ]);\r\n   } catch (error) {\r\n       // Handle timeout or memory errors gracefully\r\n   }\r\n   ```"
    tokens: 570
    size: 1534
  - path: specs\standards\005-model-loading-configuration.md
    content: "# Standard 005: Model Loading Configuration & Endpoint Verification\r\n\r\n## What Happened?\r\nModel loading failed due to various configuration issues including \"Cannot find model record\" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.\r\n\r\n## The Cost\r\n- Failed model initialization preventing AI functionality\r\n- Multiple 404 errors for specific model types\r\n- 503 and 405 errors during embedding and model download requests\r\n- Hours spent debugging model configuration issues\r\n- Unreliable model loading across different model types\r\n- Significant time wasted discovering that documented endpoints didn't exist in the backend\r\n- Frontend-backend integration failures due to missing API endpoints\r\n\r\n## The Rule\r\n1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:\r\n   ```python\r\n   # Example mapping for problematic models\r\n   MODEL_MAPPINGS = {\r\n       'OpenHermes': 'Mistral-v0.3',\r\n       'NeuralHermes': 'Mistral-v0.3',\r\n       # Add other mappings as needed\r\n   }\r\n   ```\r\n\r\n2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:\r\n   ```python\r\n   # In webgpu_bridge.py - ensure flexible model name handling\r\n   # Don't validate model names strictly on the bridge side\r\n   ```\r\n\r\n3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:\r\n   ```javascript\r\n   // In frontend code\r\n   const internalModelId = getModelInternalId(userModelName);\r\n   const modelUrl = getModelUrl(internalModelId);\r\n   ```"
    tokens: 727
    size: 1939
  - path: specs\standards\006-model-url-construction.md
    content: "# Standard 006: Model URL Construction for MLC-LLM Compatibility\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) failed to load models with the error \"TypeError: Failed to construct 'URL': Invalid URL\", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.\r\n\r\n## The Cost\r\n- 4+ hours debugging model loading failures\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Multiple failed attempts with different URL construction approaches\r\n\r\n## The Rule\r\n1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path:path}` endpoint to redirect MLC-LLM requests to local model files:\r\n   ```python\r\n   @app.get(\"/models/{model_name}/resolve/main/{file_path:path}\")\r\n   async def model_resolve_redirect(model_name: str, file_path: str):\r\n       import os\r\n       from fastapi.responses import FileResponse, JSONResponse\r\n\r\n       models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\r\n       actual_path = os.path.join(models_base, model_name, file_path)\r\n\r\n       if os.path.exists(actual_path) and os.path.isfile(actual_path):\r\n           return FileResponse(actual_path)\r\n       else:\r\n           return JSONResponse(status_code=404, content={\r\n               \"error\": f\"File {file_path} not found for model {model_name}\"\r\n           })\r\n   ```\r\n\r\n2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.\r\n\r\n3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.\r\n\r\n4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.\r\n\r\n5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly."
    tokens: 850
    size: 2259
  - path: specs\standards\007-model-loading-transition.md
    content: "# Standard 007: Model Loading Transition - Online-Only Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.\r\n\r\nThe old implementation in `chat.html` was trying to:\r\n1. Check for local model files using the `/models/{model}/resolve/main/` pattern\r\n2. Download models through the bridge if not found locally\r\n3. Use a complex configuration with multiple model entries and local file resolution\r\n\r\nThis approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.\r\n\r\n## The Cost\r\n- Hours spent debugging model loading failures in `chat.html`\r\n- Confusion between working and failing components (anchor-mic.html vs chat.html)\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:"
    tokens: 859
    size: 2245
  - path: specs\standards\008-model-loading-online-only.md
    content: "# Standard 008: Model Loading - Online-Only Approach for Browser Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.\r\n\r\nThe issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.\r\n\r\n## The Cost\r\n- All models showing as unavailable in API tests\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with limited model availability\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: window.location.origin + \"/models/\" + selectedModelId, // This will redirect to online source\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Bridge Redirect Endpoint**: Ensure the `/models/{model_name}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist."
    tokens: 918
    size: 2406
  - path: specs\standards\009-model-loading-configuration.md
    content: "# Standard 009: Model Loading Configuration - Bridge vs Direct Online\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:\r\n\r\n1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources\r\n2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser\r\n\r\nThe inconsistency occurred because:\r\n- Some components (like `anchor-mic.html`) work with direct online URLs\r\n- Other components (like `chat.html`) were configured for local file resolution\r\n- The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist\r\n\r\n## The Cost\r\n- Confusion about which model loading approach to use\r\n- Inconsistent behavior across different UI components\r\n- Models working in some components but not others\r\n- Debugging time spent on understanding different loading mechanisms\r\n- Users experiencing different model availability depending on which UI they use\r\n\r\n## The Rule\r\n1. **Consistent Model Configuration**: All UI components should use the same model loading approach:\r\n   ```javascript\r\n   // Recommended configuration pattern\r\n   const modelConfig = {\r\n       model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect\r\n       model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID\r\n       model_lib: modelLib,                                  // WASM library URL\r\n   };\r\n   ```\r\n\r\n2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:\r\n   - First check for local files in the models directory\r\n   - If local file doesn't exist, redirect to the corresponding HuggingFace URL:\r\n     `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`\r\n\r\n3. **Fallback Handling**: Implement proper fallback when local files are not available:\r\n   ```javascript"
    tokens: 756
    size: 2009
  - path: specs\standards\012-context-utility-manifest.md
    content: "# Standard 012: Context Utility Manifest - The Invisible Infrastructure\r\n\r\n## What Happened?\r\nThe Anchor Core system was originally conceived as a chat application, but has evolved into a unified cognitive infrastructure. The system now needs to transition from \"active user input\" to \"passive observation\" to function as truly invisible infrastructure like electricity - always present but never demanding attention.\r\n\r\n## The Cost\r\n- UI bloat with multiple chat interfaces competing for user attention\r\n- Manual data entry required to populate context\r\n- Users having to copy/paste information instead of automatic capture\r\n- Architecture treating UI as primary rather than as debugging tool\r\n- Missing opportunity to create true \"ambient intelligence\"\r\n\r\n## The Rule\r\n1. **Headless by Default**: All core functionality must operate without user interface interaction\r\n   ```python\r\n   # Core services run as background daemons\r\n   daemon_services = [\r\n       \"memory_graph\",      # CozoDB persistence\r\n       \"gpu_engine\",        # WebLLM inference\r\n       \"context_capture\",   # Screen/Audio observation\r\n       \"data_ingestion\"     # Memory writing\r\n   ]\r\n   ```\r\n\r\n2. **Passive Observation**: System captures context automatically rather than waiting for user input\r\n   - **Eyes**: Automated screen sampling and OCR\r\n   - **Ears**: Continuous audio transcription (when enabled)\r\n   - **Memory**: Automatic ingestion without user intervention\r\n\r\n3. **Architecture Priority**: `webgpu_bridge.py` is the nervous system; UIs are merely debugging/interaction tools\r\n   - UIs are temporary visualization layers\r\n   - Core logic exists independently of any UI\r\n   - Background services operate without UI presence\r\n\r\n4. **Invisible Utility**: The system should function like electricity - always available, rarely noticed, essential infrastructure\r\n   - Zero user interaction required for core functions\r\n   - Automatic context capture and storage\r\n   - Seamless integration with user's workflow"
    tokens: 734
    size: 1994
  - path: specs\standards\014-async-await-best-practices.md
    content: "# Standard 014: Async/Await Best Practices for FastAPI\r\n\r\n## What Happened?\r\nThe system had multiple \"coroutine was never awaited\" warnings due to improper async/await usage in the webgpu_bridge.py. These warnings occurred when async functions were called without being properly awaited or when they weren't integrated correctly with FastAPI's event loop system.\r\n\r\n## The Cost\r\n- Runtime warnings cluttering the console output\r\n- Potential resource leaks from improperly handled async operations\r\n- Unpredictable behavior in WebSocket connections and API endpoints\r\n- Difficulty debugging real issues due to noise from async warnings\r\n\r\n## The Rule\r\n1. **Proper Await Usage**: All async functions must be awaited when called within async contexts\r\n   ```python\r\n   # Correct\r\n   await add_log_entry(\"source\", \"type\", \"message\")\r\n\r\n   # Incorrect\r\n   add_log_entry(\"source\", \"type\", \"message\")  # Creates unawaited coroutine\r\n   ```\r\n\r\n2. **Event Loop Integration**: When scheduling tasks at module level, ensure they run within an active event loop:\r\n   ```python\r\n   # Correct - in startup event\r\n   async def startup_event():\r\n       await add_log_entry(\"System\", \"info\", \"Service started\")\r\n\r\n   # Incorrect - at module level before event loop starts\r\n   # asyncio.create_task(add_log_entry(...))  # Will cause warning\r\n   ```\r\n\r\n3. **FastAPI Event Handlers**: Use FastAPI's event system (`@app.on_event(\"startup\")`) for initialization tasks that require async operations\r\n\r\n4. **Background Tasks**: For fire-and-forget async operations, use FastAPI's BackgroundTasks or properly scheduled asyncio tasks within request handlers\r\n\r\n5. **WebSocket Cleanup**: Always ensure proper cleanup of async resources in WebSocket exception handlers to prevent resource leaks\r\n\r\n6. **Exception Handling**: Wrap async operations in try/catch blocks that properly handle async exceptions and clean up resources"
    tokens: 718
    size: 1899
  - path: specs\standards\017-file-ingestion-debounce-hash-checking.md
    content: "# Standard 017: File Ingestion Debounce and Hash Checking\r\n\r\n## What Happened?\r\nThe Watchdog service was triggering excessive memory ingestion when modern editors (VS Code, Obsidian) would autosave files frequently. This caused \"Memory Churn\" in CozoDB with duplicate content being ingested repeatedly, fragmenting the database and spiking CPU usage.\r\n\r\n## The Cost\r\n- High CPU usage from repeated ingestion of unchanged content\r\n- Database fragmentation from duplicate entries\r\n- Poor performance during active editing sessions\r\n- 2+ hours spent implementing debounce and hash checking to prevent \"Autosave Flood\"\r\n\r\n## The Rule\r\n1. **Debounce File Events**: Implement a debounce mechanism that waits for a period of silence before processing file changes:\r\n   ```python\r\n   # Wait for debounce period before processing\r\n   debounce_time = 2.0  # seconds\r\n   ```\r\n\r\n2. **Content Hash Verification**: Calculate MD5 hash of file content before ingestion and compare with previously ingested version:\r\n   ```python\r\n   import hashlib\r\n   current_hash = hashlib.md5(content).hexdigest()\r\n   if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:\r\n       # Skip ingestion - content hasn't changed\r\n       return\r\n   ```\r\n\r\n3. **Cancel Pending Operations**: Cancel any existing debounce timer when a new file event occurs for the same file:\r\n   ```python\r\n   if file_path in self.debounce_timers:\r\n       self.debounce_timers[file_path].cancel()\r\n   ```\r\n\r\n4. **Proper Cleanup**: Clean up debounce timer references after processing:\r\n   ```python\r\n   if file_path in self.debounce_timers:\r\n       del self.debounce_timers[file_path]\r\n   ```"
    tokens: 621
    size: 1662
  - path: specs\standards\019-code-file-ingestion-comprehensive-context.md
    content: "# Standard 019: Code File Ingestion for Comprehensive Context\r\n\r\n## What Happened?\r\nThe Watchdog service was only monitoring text files (.txt, .md, .markdown) but ignoring code files which represent a significant portion of developer context. This created an \"Ingestion Blind Spot\" where the system was blind to codebase context.\r\n\r\n## The Cost\r\n- Limited context ingestion for developers\r\n- Missing important code-related information\r\n- 30 minutes spent updating watchdog.py to include code extensions\r\n\r\n## The Rule\r\n1. **Expand File Extensions**: Include common programming language extensions in file monitoring:\r\n   ```python\r\n   enabled_extensions = {\".txt\", \".md\", \".markdown\", \".py\", \".js\", \".html\", \".css\",\r\n                         \".json\", \".yaml\", \".yml\", \".sh\", \".bat\", \".ts\", \".tsx\",\r\n                         \".jsx\", \".xml\", \".sql\", \".rs\", \".go\", \".cpp\", \".c\", \".h\", \".hpp\"}\r\n   ```\r\n\r\n2. **Comprehensive Coverage**: Monitor all relevant text-based file types that contain context\r\n\r\n3. **Maintain Performance**: Ensure file size limits still apply to prevent performance issues with large code files\r\n\r\nThis standard ensures that developer context is fully captured by including code files in passive ingestion."
    tokens: 461
    size: 1227
  - path: specs\standards\021-chat-session-persistence-context-continuity.md
    content: "# Standard 021: Chat Session Persistence for Context Continuity\r\n\r\n## What Happened?\r\nThe anchor.py CLI client maintained conversation history only in memory. If the terminal was closed or the CLI crashed, the entire conversation history was lost. This created a \"Lost Context\" risk where valuable conversation history was not preserved.\r\n\r\n## The Cost\r\n- Loss of conversation history on CLI crashes or termination\r\n- Broken loop between active chatting and long-term memory\r\n- 45 minutes spent implementing chat session persistence to context folder\r\n\r\n## The Rule\r\n1. **Auto-Save Sessions**: Automatically save each chat message to a session file:\r\n   ```python\r\n   def save_message_to_session(role, content):\r\n       # Create timestamped session file in context/sessions/\r\n       # Append each message as it occurs\r\n   ```\r\n\r\n2. **Session Directory**: Create a dedicated `context/sessions/` directory for chat logs:\r\n   ```python\r\n   SESSIONS_DIR = os.path.join(CONTEXT_DIR, \"sessions\")\r\n   os.makedirs(SESSIONS_DIR, exist_ok=True)\r\n   ```\r\n\r\n3. **Markdown Format**: Save conversations in markdown format for easy reading and processing:\r\n   ```python\r\n   # Format: ## Role\\nContent\\n\\n for each message\r\n   ```\r\n\r\n4. **Loop Closure**: Ensure that chat sessions become text files that the Watchdog service can monitor and ingest into long-term memory automatically.\r\n\r\nThis standard ensures conversation history survives CLI crashes and becomes part of the persistent context."
    tokens: 566
    size: 1479
  - path: specs\standards\022-text-file-source-of-truth-cross-machine-sync.md
    content: "# Standard 022: Text-File Source of Truth for Cross-Machine Sync\r\n\r\n## What Happened?\r\nThe CozoDB database lives in IndexedDB inside the headless browser profile, making it impossible to sync between machines. Chat history and learned connections were trapped in the browser instance and lost when switching laptops. The system needed a \"Text-File Source of Truth\" approach where the database is treated as a cache and all important data is stored in text files.\r\n\r\n## The Cost\r\n- Lost conversation history when switching between machines\r\n- Inability to sync learned connections and context across devices\r\n- 1 hour spent implementing daily session files and text-file persistence\r\n\r\n## The Rule\r\n1. **Database is Cache**: Treat CozoDB as a cache, not the source of truth:\r\n   ```python\r\n   # All important data must exist in text files first\r\n   # Database is rebuilt from text files on each machine\r\n   ```\r\n\r\n2. **Daily Session Files**: Create daily markdown files for chat persistence:\r\n   ```python\r\n   def ensure_session_file():\r\n       date_str = datetime.now().strftime(\"%Y-%m-%d\")\r\n       filename = f\"chat_{date_str}.md\"\r\n       # Creates daily consolidated session files\r\n   ```\r\n\r\n3. **Text-File First**: All important information must be written to text files:\r\n   ```python\r\n   # Every chat message gets saved to markdown file\r\n   # Files are automatically ingested by watchdog service\r\n   # Creates infinite loop: Chat -> File -> Ingestion -> Memory -> Next Chat\r\n   ```\r\n\r\n4. **Cross-Machine Sync**: Use Dropbox/Git for file synchronization:\r\n   ```python\r\n   # Text files sync automatically via Dropbox/Git\r\n   # Database rebuilds from text files on each machine\r\n   # Ensures consistent context across all devices\r\n   ```"
    tokens: 674
    size: 1740
  - path: specs\standards\024-context-ingestion-pipeline-fix.md
    content: "# Standard 024: Context Ingestion Pipeline - Field Name Alignment Protocol\r\n\r\n## What Happened?\r\nThe context ingestion pipeline was failing silently due to field name mismatches between the watchdog service and the ghost engine. The watchdog was sending `filetype` but the memory ingestion endpoint expected `file_type`, and the ghost engine was looking for `msg.filetype` instead of `msg.file_type`. This caused the database to appear empty even though files were being processed, resulting in failed context searches.\r\n\r\n## The Cost\r\n- 2+ hours spent debugging why context files weren't appearing in the database\r\n- Confusion from \"Database appears empty!\" messages in ghost engine logs\r\n- Failed context searches returning no results despite files existing in context directory\r\n- Misleading \"Ingested\" messages in watchdog logs that masked the actual field name mismatch\r\n- Users experiencing broken context retrieval functionality\r\n\r\n## The Rule\r\n1. **Field Name Consistency**: All components in the ingestion pipeline must use consistent field names:\r\n   - Watchdog sends: `file_type`, `source`, `content`, `filename`\r\n   - Bridge expects: `file_type`, `source`, `content`, `filename`\r\n   - Ghost engine receives: `file_type`, `source`, `content`, `filename`\r\n\r\n2. **Payload Validation**: Always validate that field names match across the entire pipeline:\r\n   ```javascript\r\n   // In ghost.html handleIngest function\r\n   await runQuery(query, {\r\n       data: [[id, ts, msg.content, msg.source || msg.filename, msg.file_type || \"text\"]]\r\n   });\r\n   ```\r\n\r\n3. **Source Identification**: The watchdog must send a proper source identifier instead of relying on default \"unknown\" values\r\n\r\n4. **Error Reporting**: Include detailed error messages when ingestion fails to help with debugging"
    tokens: 680
    size: 1790
  - path: specs\standards\027-no-resurrection-mode.md
    content: "# Standard 027: No Resurrection Mode for Manual Ghost Engine Control\r\n\r\n## What Happened?\r\nThe system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues for users who wanted to use an existing browser window or manually control when the Ghost Engine connects. Users needed an option to disable the automatic resurrection protocol and connect the Ghost Engine manually when needed.\r\n\r\n## The Cost\r\n- Unnecessary browser processes launched automatically\r\n- Resource usage when Ghost Engine not needed\r\n- Inability to use existing browser windows for Ghost Engine operations\r\n- Confusion when multiple browser instances were running\r\n- Users wanting more control over when the Ghost Engine connects\r\n\r\n## The Rule\r\n1. **Environment Variable Control**: The system must support a `NO_RESURRECTION_MODE=true` environment variable to disable automatic Ghost Engine launching.\r\n\r\n2. **Conditional Launch**: When `NO_RESURECTION_MODE=true`, the system shall NOT automatically launch the Ghost Engine during startup.\r\n\r\n3. **Manual Connection**: In no resurrection mode, users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.\r\n\r\n4. **Clear Messaging**: The system shall provide clear instructions to users when no resurrection mode is enabled, indicating they need to open ghost.html manually.\r\n\r\n5. **Resource Management**: When no resurrection mode is enabled, the system shall not attempt to kill browser processes during shutdown.\r\n\r\n6. **API Behavior**: API endpoints that require the Ghost Engine shall return appropriate 503 errors with clear messaging when the Ghost Engine is disconnected, regardless of resurrection mode setting.\r\n\r\n## Implementation\r\n- Set environment variable: `set NO_RESURRECTION_MODE=true` before running `start-anchor.bat`\r\n- The Bridge will log a message indicating manual connection is required\r\n- Users open `http://localhost:8000/ghost.html` in their browser to connect the Ghost Engine\r\n- All functionality remains the same, just with manual control over Ghost Engine connection"
    tokens: 825
    size: 2118
  - path: specs\standards\028-default-no-resurrection-mode.md
    content: "# Standard 028: Configuration-Driven System with Default No Resurrection Mode\r\n\r\n## What Happened?\r\nThe system was automatically launching the Ghost Engine (headless browser) every time the Anchor Core started, which caused issues with resource usage and prevented users from controlling when the Ghost Engine connects. The system now defaults to \"No Resurrection Mode\" where the Ghost Engine must be manually started by opening ghost.html in the browser. Additionally, ALL system variables are now abstracted to a central configuration file (config.json) to support future settings menu implementation.\r\n\r\n## The Cost\r\n- Excessive resource usage from automatically launching headless browser\r\n- Browser processes that couldn't be controlled by the user\r\n- Confusion when multiple browser instances were running\r\n- Unnecessary complexity in the startup process\r\n- Users wanting more control over when the Ghost Engine connects\r\n- Hard-coded values throughout the codebase that made customization difficult\r\n\r\n## The Rule\r\n1. **Default Behavior**: The system shall default to `NO_RESURRECTION_MODE=true`, meaning the Ghost Engine is not automatically launched.\r\n\r\n2. **Manual Connection**: Users must manually open `ghost.html` in their browser to connect the Ghost Engine to the Bridge.\r\n\r\n3. **Environment Override**: Users can set `NO_RESURRECTION_MODE=false` to return to auto-launching behavior.\r\n\r\n4. **Queued Operations**: When Ghost Engine is disconnected, operations shall be queued and processed when connection is established.\r\n\r\n5. **Clear Messaging**: The system shall provide clear instructions when Ghost Engine is not connected, indicating how to establish the connection.\r\n\r\n6. **Configurable Values**: All system parameters shall be configurable via the config.json file, including:\r\n   - Server settings (port, host, CORS origins)\r\n   - Ghost Engine settings (auto resurrection, browser paths, flags)\r\n   - Logging configuration (max lines, directory, format)\r\n   - Memory settings (max ingest size, default limits, char limits)\r\n   - GPU management (enabled, concurrent ops, timeout)\r\n   - Model loading (timeout, default model, base URL)\r\n   - Watchdog settings (enabled, watch directory, allowed extensions, debounce time)\r\n\r\n7. **Detached Operation**: All scripts shall run in detached mode with logging to the logs/ directory as per Standard 025.\r\n\r\n## Implementation\r\n- Default configuration sets `\"ghost_engine.auto_resurrection_enabled\": false`\r\n- The start-anchor.bat script defaults to NO_RESURRECTION_MODE=true"
    tokens: 970
    size: 2539
  - path: specs\standards\029-consolidated-data-aggregation.md
    content: "# Standard 029: Consolidated Data Aggregation with YAML Support\r\n\r\n## What Happened?\r\nThe system had multiple scripts performing similar functions for data aggregation and migration:\r\n- `migrate_history.py` - Legacy session migration to YAML\r\n- `read_all.py` in context directory - Data aggregation to JSON\r\n- Multiple overlapping data processing scripts\r\n\r\nThis created redundancy and confusion about which script to use for data aggregation. The functionality has been consolidated into a single authoritative script: `context/Coding-Notes/Notebook/read_all.py` which now supports all three output formats (text, JSON, YAML).\r\n\r\n## The Cost\r\n- Multiple scripts with overlapping functionality\r\n- Confusion about which script to use for data aggregation\r\n- Maintenance burden of multiple similar scripts\r\n- Inconsistent output formats across scripts\r\n- Redundant code that needed to be updated in multiple places\r\n\r\n## The Rule\r\n1. **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation from the context directory.\r\n\r\n2. **Multi-Format Output**: The script must generate three output formats:\r\n   - `combined_text.txt` - Human-readable text corpus\r\n   - `combined_memory.json` - Structured JSON for database ingestion\r\n   - `combined_memory.yaml` - Structured YAML for easier processing and migration\r\n\r\n3. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.\r\n\r\n4. **Encoding Handling**: The script must handle various file encodings using chardet for reliable processing.\r\n\r\n5. **Recursive Processing**: The script must process all subdirectories while respecting exclusion rules.\r\n\r\n6. **Metadata Preservation**: File metadata (path, timestamp) must be preserved in structured outputs.\r\n\r\n## Implementation\r\n- Consolidated migrate_history.py functionality into read_all.py\r\n- Moved migrate_history.py to archive/tools/\r\n- Updated read_all.py to generate YAML output with proper multiline formatting\r\n- Used yaml.dump() with custom representer for multiline strings\r\n- Maintained all existing functionality while adding YAML support\r\n- Preserved the same exclusion rules and file type filtering"
    tokens: 858
    size: 2268
  - path: specs\standards\030-multi-format-output.md
    content: "# Standard 030: Multi-Format Output for Project Aggregation\r\n\r\n## What Happened?\r\nThe `read_all.py` script in the root directory was only generating text and JSON outputs for project aggregation. To improve compatibility with various processing tools and follow the documentation policy of supporting YAML format, the script was updated to also generate a YAML version of the memory records.\r\n\r\n## The Cost\r\n- Limited output format options for downstream processing\r\n- Inconsistency with the documentation policy that prefers YAML for configuration and data exchange\r\n- Missing opportunity to provide easily readable structured data in YAML format\r\n- Users had to convert JSON to YAML if they needed that format\r\n\r\n## The Rule\r\n1. **Multi-Format Output**: The `read_all.py` script must generate both JSON and YAML versions of memory records.\r\n\r\n2. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.\r\n\r\n3. **Consistent Naming**: Output files should follow consistent naming patterns:\r\n   - `combined_text.txt` - Aggregated text content\r\n   - `combined_memory.json` - Structured JSON memory records\r\n   - `combined_text.yaml` - Structured YAML memory records\r\n\r\n4. **Custom Representers**: Use custom YAML representers to handle multiline strings appropriately with the `|` indicator.\r\n\r\n5. **Encoding Handling**: Ensure proper UTF-8 encoding for both input and output operations.\r\n\r\n## Implementation\r\n- Updated `read_all.py` to import and use the `yaml` module\r\n- Added custom string representer for multiline content\r\n- Created separate YAML output file with proper formatting\r\n- Maintained all existing functionality while adding YAML support\r\n- Used `yaml.dump()` with appropriate parameters for clean output"
    tokens: 698
    size: 1814
  - path: specs\standards\032-ghost-engine-initialization-flow.md
    content: "# Standard 032: Ghost Engine Initialization and Ingestion Flow\r\n\r\n## What Happened?\r\nThe Ghost Engine was experiencing race conditions where memory ingestion requests were being processed before the database was fully initialized. This caused errors like \"Cannot read properties of null (reading 'run')\" and inconsistent ingestion behavior between the Bridge API logs and the Ghost Engine logs.\r\n\r\n## The Cost\r\n- Database ingestion failures when Ghost Engine connected to Bridge before database initialization completed\r\n- Inconsistent logging between Bridge and Ghost Engine (Bridge showing success, Ghost Engine showing failures)\r\n- Race conditions where ingestion requests arrived before database was ready\r\n- Poor user experience with failed memory operations\r\n- Confusing error messages in the UI\r\n\r\n## The Rule\r\n1. **Sequential Initialization**: The Ghost Engine must initialize the database completely before signaling readiness to the Bridge.\r\n\r\n2. **Database Readiness Checks**: All ingestion and search operations must verify that the database object is properly initialized before attempting operations.\r\n\r\n3. **Proper Error Handling**: When database is not ready, the Ghost Engine must return appropriate error messages to the Bridge instead of failing silently.\r\n\r\n4. **Synchronous Connection Flow**: WebSocket connection must follow: Connect  Initialize Database  Signal Ready  Process Requests.\r\n\r\n5. **Graceful Degradation**: If database initialization fails, the Ghost Engine must report the error to the Bridge and not attempt to process requests.\r\n\r\n6. **Message Type Handling**: The system must properly handle all message types including `engine_error` responses.\r\n\r\n## Implementation\r\n- Modified WebSocket connection flow to initialize database before signaling readiness\r\n- Added database readiness checks in `handleIngest` and `handleSearch` functions\r\n- Implemented proper error responses when database is not ready\r\n- Added support for `engine_error` message type handling\r\n- Enhanced error logging with fallbacks to prevent \"undefined\" messages\r\n- Ensured sequential processing: Connect  DB Init  Ready Signal  Process Requests"
    tokens: 819
    size: 2172
  - path: specs\standards\053-cozodb-pain-points-reference.md
    content: "# Standard 053: CozoDB Pain Points & OS Compatibility\r\n\r\n**Status:** Active | **Category:** Architecture / Database\r\n\r\n## 1. Native Binary Desync (Critical)\r\nThe `cozo-node` package relies on a native C++ binding (`.node` file). On Windows systems, `pnpm install` may fail to correctly link or place the prebuilt binary in the expected path, especially when moving between different Node.js context (e.g., global vs workspace).\r\n\r\n### Symptoms\r\n- `Error: Cannot find module '.../cozo_node_prebuilt.node'`\r\n- `MODULE_NOT_FOUND` during engine startup.\r\n\r\n### Resolution\r\nThe prebuilt binary for Windows (`napi-v6`) must be manually verified. In case of failure:\r\n1. Locate the correct binary (typically found in `.ignored` or a previous build's `node_modules`).\r\n2. Map it to: `node_modules/cozo-node/native/6/cozo_node_prebuilt.node`.\r\n\r\n## 2. API Inconsistency (v0.7.6+)\r\nThe official `cozo-node` library exports a `CozoDb` class, but the ECE_Core architecture (Standard 058/064) expects individual function exports (`open_db`, `query_db`, etc.) to maintain a functional, stateless-style interface.\r\n\r\n### The Patch\r\nWe maintain a manual patch in `node_modules/cozo-node/index.js` to expose native methods directly:\r\n```javascript\r\nmodule.exports = {\r\n    CozoDb: CozoDb,\r\n    open_db: (engine, path, options) => native.open_db(engine, path, JSON.stringify(options)),\r\n    query_db: (id, script, params) => {\r\n        return new Promise((resolve, reject) => {\r\n            native.query_db(id, script, params, (err, res) => { ... });\r\n        });\r\n    },\r\n    // ...\r\n}\r\n```\r\n\r\n## 3. CozoDB Parser Fragility\r\nThe CozoDB Datalog parser is sensitive to:\r\n- **Multiline Strings**: Newlines in template literals can cause desync.\r\n- **Empty Params**: Always pass `{}` if no params are used.\r\n- **Type Downcasting**: Passing `null` to `path` in `open_db` can cause `failed to downcast any to string`. Use a string path or a descriptive constant.\r\n\r\n## 4. Hardware/OS Constraints\r\n- **Windows**: Requires VS Build Tools for native compilation if prebuilts fail.\r\n- **VRAM**: CozoDB is disk-native (RocksDB); it does not compete for VRAM, but large FTS indices can bloat RAM. Limit file sizes to <500KB (Standard 053: FTS Poisoning).\r\n\r\n> [!IMPORTANT]\r\n> When updating dependencies, ALWAYS verify the `cozo-node/index.js` patch is still active. Automated builds may overwrite this file.\r\n"
    tokens: 912
    size: 2380
  - path: specs\standards\058-universal-rag-api.md
    content: "# Standard 058: UniversalRAG API & Modality-Aware Search\r\n\r\n**Status:** Active | **Type:** Architectural Constraint | **Created:** 2026-01-16\r\n\r\n## The Triangle of Pain\r\n\r\n1.  **What Happened:** The initial search API (`GET /v1/memory/search`) relied on URL parameters, making it impossible to support complex RAG queries involving modality routing (Buckets) and provenance filtering. Additionally, native database exports proved opaque and brittle, risking data lock-in.\r\n2.  **The Cost:** Agent confusion (\"wobbling\") due to ambiguous \"legacy support\" directives, inability to implement \"Deep Research\" features, and risk of losing historical context if the database engine changes.\r\n3.  **The Rule:** \r\n    *   **Strict POST:** All semantic search operations MUST use `POST /v1/memory/search` with a structured JSON body conforming to the `SearchRequest` interface.\r\n    *   **Universal Context Routing:** \"Buckets\" are strictly mapped to \"Modalities\" (e.g., `@code`, `@memory`, `@visual`).\r\n    *   **Sovereign Dump:** Backups MUST be human-readable JSON streams (`GET /v1/backup`), never binary database exports.\r\n\r\n## The Standard\r\n\r\n### 1. UniversalRAG Interface\r\nThe search endpoint is the \"Central Nervous System\" of the engine. It does not just \"look up keywords\"; it routes intent.\r\n\r\n```typescript\r\nexport interface SearchRequest {\r\n  query: string;           // Natural language intent\r\n  limit?: number;          // Default: 20\r\n  deep?: boolean;          // True = Trigger Dreamer/Epochal layers\r\n  buckets?: string[];      // Modalities: [\"@code\", \"@visual\", \"@memory\"]\r\n  provenance?: 'sovereign' | 'external' | 'all';\r\n}\r\n```\r\n\r\n### 2. Modality Mapping\r\nBuckets are not arbitrary folders. They define the *Type of Mind* required:\r\n*   `@code`  Source code focus (`.ts`, `.py`, `.rs`). Prioritizes structural understanding.\r\n*   `@memory`  Chat logs, Dreamer epochs, and episodic history. Prioritizes temporal continuity.\r\n*   `@visual`  Image descriptions and spatial data.\r\n\r\n### 3. Sovereign Backup Strategy\r\nData sovereignty means the user owns the format.\r\n*   **Format:** Single JSON object.\r\n*   **Structure:**\r\n    ```json\r\n    {\r\n      \"timestamp\": \"ISO-8601\",\r\n      \"stats\": { \"memory_count\": N, \"engram_count\": N },\r\n      \"memories\": [ ... ],\r\n      \"engrams\": [ ... ]\r\n    }\r\n    ```\r\n*   **Portability:** This format is database-agnostic. It can be re-ingested into SQLite, Postgres, or a new CozoDB instance.\r\n\r\n## Implementation Requirements\r\n*   **Routes:** `POST /v1/memory/search`, `GET /v1/backup`\r\n*   **Legacy Support:** `GET` search endpoints should redirect or instruct users to use `POST`.\r\n*   **Streaming:** Chat interfaces (`/v1/chat/completions`) must support SSE (Server-Sent Events) for real-time feedback.\r\n"
    tokens: 1020
    size: 2766
  - path: specs\standards\059-reliable-ingestion.md
    content: "# Standard 059: Reliable Ingestion (The \"Ghost Data\" Protocol)\r\n\r\n**Status:** Active\r\n**Trigger:** Ingestion API returning 200 OK while failing to persist data to CozoDB.\r\n\r\n## 1. The Pain (Ghost Data & Silent Failures)\r\n*   **symptom:** The `POST /v1/ingest` endpoint returned `200 OK` with a valid ID, but the data was never written to the database.\r\n*   **Cost:** 6 hours of debugging search logic logic when ingestion was the root cause.\r\n*   **Risk:** Silent data loss. Users believe memories are saved when they are discarded.\r\n\r\n## 2. The Solution (Trust but Verify)\r\n1.  **Read-After-Write (RAW):** Every ingestion operation MUST perform a read query immediately after the write operation, *within the same request scope*, to verify persistence.\r\n    *   *Implementation:* insert `?[count] := *memory{id}, count(id)` or `?[id] := *memory{id}, id = $id`\r\n2.  **Count Validation:** The API MUST NOT return `200 OK` unless the Verification Count > 0 (or specifically matches expected count).\r\n3.  **Explicit Failure:** If verification fails, the API MUST return `500 Internal Server Error` with a standard error code (`INGEST_VERIFY_FAILED`).\r\n4.  **Logging:** The Verification Count must be logged to the critical path log (Console or File) with the prefix `[INGEST_VERIFY]`.\r\n\r\n## 4. Schema Alignment\r\n*   **Strict Column Order:** CozoDB's `<- $data` insertion is positional. The API array order MUST match the `::columns memory` order exactly.\r\n*   **Migration Integrity:** Any schema change (adding columns) requires a corresponding update to the `ingest.ts` data array *and* a verified migration of existing data using the Safe Restart Protocol.\r\n*   **Nuclear Fallback:** If automated migration fails persistently (e.g. index locks) and data volume is zero or recoverable (inbox-based), the system MAY auto-reset the database (delete/recreate) to ensure service availability.\r\n\r\n## 5. Metadata Mandatory\r\n*   **Source ID:** `source_id` is mandatory for all atoms.\r\n*   **Sequence:** `sequence` is mandatory (default 0).\r\n## 6. The Cleanup Protocol (Encoding & Sanitization)\r\n*   **Null Byte Stripping:** Ingested content MUST be scrubbed of null bytes (`\\x00`) and replacement characters (`\\uFFFD`). These cause `node-llama-cpp` tokenizer to bloat text significantly (1 char -> multiple tokens), leading to context overflows.\r\n*   **BOM Detection:** The system MUST detect UTF-16 LE/BE Byte Order Marks (BOM) and decode buffers accordingly before processing.\r\n*   **Strict Truncation:** To preserve system stability, embedding workers MUST truncate inputs to a safe factor of the context window (Recommended: `1.2 * ContextSize` characters) to prevent OOM or logic crashes on dense inputs (e.g., minified code).\r\n\r\n## 7. The Inbox Zero Protocol (Recursive Ingestion)\r\n*   **Recursive Scanning:** The Ingestion Engine MUST scan subdirectories within the `inbox/` folder.\r\n*   **Smart Bucketing:**\r\n    *   Files at `inbox/root.md` -> Bucket: `inbox`.\r\n    *   Files at `inbox/project-a/note.md` -> Bucket: `project-a`.\r\n    *   *Purpose:* This allows users to pre-organize content without it getting lost in a generic \"inbox\" tag.\r\n*   **Transient Tag Cleanup:** The \"inbox\" tag is considered transient. The Dreamer/Organization Agents MUST remove the `inbox` tag after processing/tagging, but MUST preserve specific subfolder tags (e.g. `project-a`) to respect user intent.\r\n"
    tokens: 1320
    size: 3386
  - path: specs\standards\060-worker-system.md
    content: "\r\n# Standard 060: Worker System Architecture\r\n\r\n**Supersedes**: N/A (New Standard)\r\n**Effective Date**: 2026-01-16\r\n**Status**: Active\r\n\r\n## 1. Dual-Worker Model\r\nTo resolve concurrency issues (blocking during ingestion), ECE_Core uses a dedicated worker model.\r\n\r\n### 1.1 ChatWorker\r\n- **File**: `src/core/inference/ChatWorker.ts`\r\n- **Role**: Handles conversational inference only.\r\n- **Model**: Loaded from `LLM_MODEL_PATH`.\r\n- **Context**: Managed via `LlamaChatSession`.\r\n\r\n### 1.2 EmbeddingWorker\r\n- **File**: `src/core/inference/EmbeddingWorker.ts`\r\n- **Role**: Handles vector generation only.\r\n- **Model**: Loaded from `LLM_EMBEDDING_MODEL_PATH`.\r\n- **Context**: Managed via `LlamaEmbeddingContext`.\r\n- **Note**: If `LLM_EMBEDDING_MODEL_PATH` is unset, the system falls back to `HybridWorker` (shared model).\r\n\r\n## 2. Provider Routing\r\n- `src/services/llm/provider.ts` is the orchestrator.\r\n- It detects the configuration state and spawns the appropriate workers.\r\n- **Dedicated Mode**: Spawns both workers. Routes `chat` -> ChatWorker, `embed` -> EmbeddingWorker.\r\n- **Shared Mode**: Spawns `HybridWorker`. Routes all traffic to it.\r\n\r\n## 3. Communication Protocol\r\n- Workers communicate via `parentPort` messages.\r\n- **Types**: `loadModel`, `chat`, `getEmbeddings`.\r\n- **Error Handling**: Workers must wrap main logic in `try/catch` and send `type: 'error'` on failure.\r\n"
    tokens: 505
    size: 1381
  - path: specs\standards\061-context-logic.md
    content: "\r\n# Standard 061: Context Management Logic\r\n\r\n**Supersedes**: N/A (New Standard)\r\n**Effective Date**: 2026-01-16\r\n**Status**: Active\r\n\r\n## 1. Rolling Context Assembly\r\nECE_Core uses a \"Middle-Out\" budgeting strategy to maximize context relevance while preserving narrative flow.\r\n\r\n### 1.1 Selection Pipeline\r\n1.  **Temporal Analysis**:\r\n    *   If query contains `[\"recent\", \"latest\", \"today\", \"now\", \"current\"]`:\r\n        *   **Recency Weight**: 60%\r\n        *   **Relevance Weight**: 40%\r\n    *   Otherwise:\r\n        *   **Recency Weight**: 30%\r\n        *   **Relevance Weight**: 70%\r\n2.  **Scoring**: Atoms are ranked by `MixedScore` (Relevance * W1 + Recency * W2).\r\n3.  **Budgeting**: Atoms fill the `TokenBudget` starting from highest score.\r\n\r\n### 1.2 Safety Constraints\r\n- **Token Buffer**: The target budget is effectively `min(ConfiguredBudget, 3800)` to provide a ~300 token safety margin against CJK/multibyte inflation and tokenizer mismatches.\r\n- **Smart Slicing**:\r\n    *   Atoms are NOT cut mid-sentence.\r\n    *   The slicer looks for punctuation (`.`, `!`, `?`, `\\n`) within the last 50-100 characters of the remaining budget.\r\n    *   If no punctuation is found, it falls back to a hard cut with `...`\r\n\r\n### 1.3 Assembly\r\n- **Re-Sorting**: After selection, atoms are re-sorted **Chronologically** to present a linear narrative to the LLM.\r\n- **Formatting**: Each atom is prefixed with `[Source: <filename>] (<ISO-Date>)`.\r\n"
    tokens: 541
    size: 1443
  - path: specs\standards\062-inference-stability.md
    content: "\r\n# Standard 062: Inference Worker Stability\r\n\r\n**Status:** Active\r\n**Context:** Local LLM/Embedding inference via `node-llama-cpp` or similar bindings.\r\n\r\n## 1. The Pain (Context Explosions)\r\n*   **Symptom:** Worker threads crashing with `Input is longer than context size` errors during background embedding.\r\n*   **Cause:** \"Dense Text\" (Minified code, base64, foreign languages) can have a 1:1 Character-to-Token ratio. A 6000-char chunk becomes 6000 tokens, overflowing a 2048-token context.\r\n*   **Risk:** System instability, lost data, and endless retry loops.\r\n\r\n## 2. The Solution (Dynamic Safety)\r\n### A. Context Awareness\r\n*   **Dynamic Configuration:** Workers MUST read the actual `CTX_SIZE` from load options, not assume 4096.\r\n\r\n### B. The \"Safe Ratio\" Rule\r\n*   **Logic:** Truncate input text *before* tokenization using a conservative safety factor.\r\n*   **Formula:** `SafeLength = floor(ContextSize * 1.2)`\r\n    *   Example: 2048 tokens * 1.2 = 2457 chars.\r\n*   **Blob Strategy:** For detected dense content (avg line len > 300), use an even stricter hard limit (e.g. 1500 chars) to guarantee safety.\r\n\r\n## 3. Worker Isolation\r\n*   **Error Containment:** A crash in a worker (e.g., CUDA error) MUST NOT crash the main process.\r\n*   **Queue Resilience:** If a batch fails, the worker should attempt to recover or return a partial result (e.g., empty embeddings for failed items) rather than hanging the queue indefinitely.\r\n\r\n## 4. The \"Ghost CUDA\" Patch\r\n*   **Symptom:** Setting `GPU_LAYERS=0` for a worker still results in CUDA initialization and VRAM usage (leading to OOM).\r\n*   **Cause:** `node-llama-cpp` by default eagerly initializes the best available backend (CUDA) even if `gpuLayers` is 0.\r\n*   **Fix:** Workers MUST explicitly check for `GPU_LAYERS=0` in their `init()` sequence and pass `gpu: { exclude: ['cuda'] }` to the loading configuration.\r\n*   **Rule:** \"Zero means Zero\". If the user requests 0 GPU layers, the CUDA backend should not even be loaded.\r\n"
    tokens: 780
    size: 1992
  - path: specs\standards\062_inference_stability.md
    content: "\r\n# Standard 062: Inference Worker Stability\r\n\r\n**Status:** Active\r\n**Context:** Local LLM/Embedding inference via `node-llama-cpp` or similar bindings.\r\n\r\n## 1. The Pain (Context Explosions)\r\n*   **Symptom:** Worker threads crashing with `Input is longer than context size` errors during background embedding.\r\n*   **Cause:** \"Dense Text\" (Minified code, base64, foreign languages) can have a 1:1 Character-to-Token ratio. A 6000-char chunk becomes 6000 tokens, overflowing a 2048-token context.\r\n*   **Risk:** System instability, lost data, and endless retry loops.\r\n\r\n## 2. The Solution (Dynamic Safety)\r\n### A. Context Awareness\r\n*   **Dynamic Configuration:** Workers MUST read the actual `CTX_SIZE` from load options, not assume 4096.\r\n\r\n### B. The \"Safe Ratio\" Rule\r\n*   **Logic:** Truncate input text *before* tokenization using a conservative safety factor.\r\n*   **Formula:** `SafeLength = floor(ContextSize * 1.2)`\r\n    *   Example: 2048 tokens * 1.2 = 2457 chars.\r\n*   **Blob Strategy:** For detected dense content (avg line len > 300), use an even stricter hard limit (e.g. 1500 chars) to guarantee safety.\r\n\r\n## 3. Worker Isolation\r\n*   **Error Containment:** A crash in a worker (e.g., CUDA error) MUST NOT crash the main process.\r\n*   **Queue Resilience:** If a batch fails, the worker should attempt to recover or return a partial result (e.g., empty embeddings for failed items) rather than hanging the queue indefinitely.\r\n\r\n## 4. The \"Ghost CUDA\" Patch\r\n*   **Symptom:** Setting `GPU_LAYERS=0` for a worker still results in CUDA initialization and VRAM usage (leading to OOM).\r\n*   **Cause:** `node-llama-cpp` by default eagerly initializes the best available backend (CUDA) even if `gpuLayers` is 0.\r\n*   **Fix:** Workers MUST explicitly check for `GPU_LAYERS=0` in their `init()` sequence and pass `gpu: { exclude: ['cuda'] }` to the loading configuration.\r\n*   **Rule:** \"Zero means Zero\". If the user requests 0 GPU layers, the CUDA backend should not even be loaded."
    tokens: 780
    size: 1990
  - path: specs\standards\063-cozo-db-syntax.md
    content: "\r\n# Standard 063: CozoDB Syntax & Schema Patterns\r\n\r\n**Status:** Active\r\n**Context:** CozoDB (RocksDB Backend) via `cozo-node` binding.\r\n\r\n## 1. Syntax Criticals (The \"Parser Traps\")\r\nThe `cozo-node` parser is stricter/different than some Rust documentation implies.\r\n1.  **Vector Columns:** MUST use angle brackets with dimensions.\r\n    *    Correct: `embedding: <F32; 384>`\r\n    *    Incorrect: `embedding: [F32; 384]`, `embedding: Float32Array`\r\n2.  **Assignment Operator:** MUST be `<-` (no spaces).\r\n    *    Correct: `... <- $data`\r\n    *    Incorrect: `... < - $data` (Causes `eval::named_field_not_found`)\r\n3.  **Insertion Verb:** Use `:put`.\r\n    *    Correct: `:put memory { ... }`\r\n    *    Risky: `:insert`, `:replace` (Behavior varies by version/context)\r\n\r\n## 2. HNSW Index Creation\r\nThe `::index create` command is insufficient for HNSW. Use the dedicated `::hnsw` command.\r\n\r\n```cozoql\r\n::hnsw create memory:knn {\r\n    dim: 384,\r\n    m: 50,\r\n    ef_construction: 200,\r\n    fields: [embedding],\r\n    dtype: F32,\r\n    distance: L2\r\n}\r\n```\r\n\r\n## 3. Schema Evolution (The \"Safe Restart\")\r\nCozoDB does not support `ALTER TABLE` easily.\r\n*   **Protocol:** If the schema changes (e.g. adding `hash` column):\r\n    1.  Detect mismatch (Column count check).\r\n    2.  **Explicitly Drop Indices:** `::index drop memory:idxname` (Failure to do this locks the table drop).\r\n    3.  Drop Table: `:drop memory`.\r\n    4.  Recreate Table with new Schema.\r\n    5.  Recreate Indices.\r\n\r\n## 4. Query Reliability\r\n*   **Parameter Binding:** Always use `$var` binding.\r\n    *   `?[id] := *memory{id}, id = $id`\r\n*   **Read-After-Write:** See [Standard 059](059_reliable_ingestion.md).\r\n\r\n## 5. HNSW Vector Search (Verified Protocol)\r\nVector search via `cozo-node` has strict, non-obvious requirements that differ from CLI usage.\r\n\r\n### A. Explicit Index Query\r\nDo NOT use the `:vec_nearest` algorithm directly on the table (it forces a full table scan and has obscure syntax binding issues). Always query the Index.\r\n\r\n*    **Clean & Fast (O(log n)):**\r\n    ```typescript\r\n    // Use the ~table:index format\r\n    ?[id, dist] := ~memory:knn{id | query: vec($q), k: 100, ef: 200, bind_distance: d}, \r\n                   dist = d\r\n    ```\r\n*    **Slow & Error Prone (O(n)):**\r\n    ```typescript\r\n    ?[id, dist] := *memory{id, embedding}, :vec_nearest(embedding, $q, 100, dist)\r\n    ```\r\n\r\n### B. Type Casting (The \"List vs Vector\" Trap)\r\nJavaScript arrays (e.g. `[0.1, 0.2]`) passed as parameters (`$q`) are treated as *Lists* by Cozo. The HNSW index demands a *Vector*.\r\nYou MUST explicitly cast the input using `vec()` inside the query.\r\n\r\n*    Correct: `query: vec($queryVec)`\r\n*    Error (`Expected vector, got List`): `query: $queryVec`\r\n\r\n### C. Mandatory Parameters\r\n*   **`ef` (Expansion Factor):** This parameter is **REQUIRED** for HNSW index queries. Omitting it causes `Field 'ef' is required`.\r\n    *   *Recommendation:* Set `ef` to `2 * k` (e.g., if k=100, ef=200).\r\n*   **`k` (Limit):** Should be a literal integer or bound variable.\r\n\r\n### D. Output Variable Binding\r\nWhen binding the calculated distance, use a **Logic Variable** (no `$`), not a Parameter (`$`).\r\n*    Correct: `bind_distance: d` (where `d` is then used in projection)\r\n*    Error (`Unexpected input`): `bind_distance: $d`\r\n"
    tokens: 1250
    size: 3335
  - path: specs\standards\064-cozodb-query-stability.md
    content: "# Standard 064: CozoDB Query Structure & Stability\r\n\r\n**Category:** Engineering / Database\r\n**Status:** Draft\r\n**Date:** 2026-01-19\r\n\r\n## Context\r\nComplex Datalog queries, especially those involving `~memory:content_fts` (Full Text Search) and `~memory:knn` (Vector Search), have demonstrated instability in the Node.js environment. This manifests as opaque parser errors (`unexpected input`, `coercion_failed`).\r\n\r\n## Guidelines\r\n\r\n### 1. Query Simplicity\r\n- **Avoid Multiline Literals**: Where possible, keep queries single-line or strictly sanitized. Invisible newline characters in template literals can cause parser desync.\r\n  - **Bad**:\r\n    ```typescript\r\n    const q = `?[a, b] :=\r\n       *table{a, b}`;\r\n    ```\r\n  - **Good**:\r\n    ```typescript\r\n    const q = `?[a, b] := *table{a, b}`;\r\n    ```\r\n\r\n### 2. Variable Naming\r\n- Avoid variable names that collide with column names in complex projections if not strictly necessary. \r\n- Use distinct logic variables (e.g., `cont` vs `content`) during `bind` operations to prevent ambiguity.\r\n\r\n### 3. Vector & FTS Isolation\r\n- Do not assume `Promise.all` parallel execution of FTS and Vector queries is safe on the single `db` instance lock. \r\n- **Sequential Execution**: If instability persists, run queries sequentially rather than in parallel.\r\n- **Graceful degradation**: Always wrap vector/FTS queries in independent `try/catch` blocks. If one fails, the other should still return results.\r\n\r\n### 4. Parameter Binding\r\n- Always use `$param` binding for user input to prevent injection and parser errors.\r\n- **Sanitization**: Violently sanitize inputs for FTS. FTS parsers are fragile with symbols like `:`, `*`, `-`.\r\n\r\n## Implemented Workarounds (Current Codebase)\r\n- Vector Search is currently **DISABLED** in `services/search/search.ts` via `Promise.resolve([])`.\r\n- FTS Queries are **Single-Line**.\r\n"
    tokens: 700
    size: 1863
  - path: specs\standards\065-graph-associative-retrieval.md
    content: "# Standard 065: Graph-Based Associative Retrieval (Semantic-Lite)\r\n\r\n**Category:** Architecture / Search\r\n**Status:** Approved\r\n**Date:** 2026-01-19\r\n\r\n## Context\r\nTraditional Vector Search (HNSW) poses significant resource overhead (RAM/CPU) and can be unstable in local environments (CozoDB driver issues). Furthermore, for personal knowledge bases, \"fuzzy\" vector neighbors often hallucinate connections that lack explicit structural relevance.\r\n\r\n## The Strategy: \"Tag-Walker\"\r\nWe replace the Vector Layer with a **Graph-Based Associative Retrieval** protocol. This trades geometric distance for explicit graph traversals using `tags` and `buckets`.\r\n\r\n### Architecture\r\n| Feature | Vector Architecture | Tag-Walker Architecture |\r\n| --- | --- | --- |\r\n| **Storage** | Atoms + 768d Vectors (Float32) | Atoms + Strings |\r\n| **Index** | HNSW Index (Heavy) | Inverted Index (Light) |\r\n| **Logic** | \"Find nearest neighbors in embedding space\" | \"Traverse edges: Atom -> Tag -> Atom\" |\r\n\r\n### The Algorithm (70/30 Split)\r\n\r\n#### Phase 1: Anchor Search (70% Budget)\r\n**Goal:** Find \"Direct Hits\" using Weighted Keyword Search (BM25).\r\n1.  **Execute FTS**: Search for atoms matching the user query.\r\n    *   **Rule**: All FTS queries MUST be passed through `sanitizeFtsQuery` to prevent parser crashes on special characters (e.g., `.org`).\r\n2.  **Boosting**: Boost results that contain query terms in `tags` or `buckets` (2x boost).\r\n3.  **Selection**: Allocate **70%** of the context character budget to these results.\r\n\r\n#### Phase 2: Tag Harvest\r\n**Goal:** Identify \"Bridge Tags\" to find hidden context.\r\n1.  **Extract**: Collect all unique `tags` and `buckets` from the top X results of Phase 1.\r\n2.  **Filter**: Exclude generic system tags if necessary (though strict filtering is often not needed).\r\n3.  **Bridge**: These tags represent the *structural* context of the query.\r\n\r\n#### Phase 3: Neighbor Walk (30% Budget)\r\n**Goal:** Find \"Associative Hits\" (Hidden connections).\r\n1.  **Query**: Find atoms that share the **Harvested Tags** but *do not* contain the original query keywords (or are duplicates of Phase 1).\r\n    *   *Logic*: `atom -> has_tag -> tag -> has_tag -> neighbor_atom`\r\n2.  **Selection**: Allocate the remaining **30%** of the budget to these associative neighbors.\r\n\r\n### Implementation Guidelines (CozoDB)\r\n\r\n**Anchor Search Query (Simplified)**\r\n```cozo\r\n?[id, score, content, tags] := *memory{id, content, tags},\r\n                               ~memory:content_fts{id | query: $query, bind_score: score}\r\n```\r\n\r\n**Neighbor Walk Query**\r\n```cozo\r\n?[neighbor_id, neighbor_content] := *memory{id, tags},\r\n                                    member($tag, tags),        # Explode tags from source\r\n                                    *memory{id: neighbor_id, tags: n_tags},\r\n                                    member($tag, n_tags),      # Match neighbor tags\r\n                                    id != neighbor_id          # Exclude self\r\n```\r\n\r\n### The \"Lazy Tax\" Mitigation\r\nTo ensure graph connectivity even when users fail to tag notes manually, the **Dreamer Service** should be employed to auto-tag atoms during idle cycles, ensuring a dense node-edge-node graph.\r\n\r\n## The Projection: Mirror 2.0 (The Tangible Graph)\r\nTo audit and browse the graph without database tools, the engine projects its internal state onto the filesystem.\r\n\r\n1. **Structure**: `@bucket/#tag/[Source_Name]_[PathHash].md`\r\n2. **Bundling**: Atoms are bundled by source and tag to prevent file explosion. \r\n3. **Pagination**: Each bundle is limited to **100 atoms** (approx. 150-300KB) to ensure high readability and fast loading.\r\n4. **Sync Trigger**: Mirror synchronizes immediately after every successful ingestion and during every Dreamer cycle.\r\n5. **Wipe Policy**: The `mirrored_brain` directory is explicitly wiped before synchronization.\r\n6. **Navigation**: Uses `## [ID] Snippet` headers and horizontal rules to separate atoms within a bundle.\r\n"
    tokens: 1485
    size: 3958
  - path: specs\standards\066-human-readable-mirror.md
    content: "# Standard 066: Human-Readable Mirror Protocol (Re-Hydration)\r\n\r\n## 1. The Core Philosophy\r\n**\"Atomize for the Graph, Bundle for the Human.\"**\r\n\r\nThe Database (`CozoDB`) requires granularity (Atoms) for effective retrieval.\r\nThe User (`The Architect`) requires coherence (Documents) for effective reading and editing.\r\nThe Mirror must bridge this gap by **Re-Hydrating** atoms into document-like structures.\r\n\r\n## 2. The Problem of Fragmentation\r\nRaw mirroring of atoms (1:1 mapping) results in:\r\n- **Inode Exhaustion:** 100k+ small files degrade filesystem performance.\r\n- **Cognitive Load:** Users cannot \"read\" a directory of UUIDs.\r\n- **Editor Crash:** IDEs like VS Code consume GBs of RAM indexing the file tree.\r\n\r\n## 3. The Re-Hydration Protocol\r\nWhen syncing the `mirrored_brain/` directory, the Engine MUST:\r\n\r\n### A. Group by Source\r\nInstead of iterating atoms, iterate **Sources**.\r\n```sql\r\n?[source_path, atom_content, sequence] := *atoms{source_path, content, sequence}\r\n:order source_path, sequence\r\n```\r\n\r\n### B. Concatenation Strategy\r\nAtoms belonging to the same `source_path` are written to a single file.\r\n**Format:**\r\n```markdown\r\n# Source: relative/path/to/original.md\r\n\r\n[Content of Atom 1]\r\n\r\n---\r\n[Content of Atom 2]\r\n```\r\n\r\n### C. The \"Orphan\" Handling\r\nIf an atom lacks a clear `source_path` (e.g., generated insights, chat logs), it should be bundled by **Time** (Daily/Weekly Logs) or **Topic** (Tag Buckets) to prevent directory pollution.\r\n\r\n## 4. Performance Target\r\n- **Max Files in Root:** < 50\r\n- **Max Files per Subdir:** < 100\r\n- **Sync Speed:** < 2s for 10k atoms (via bulk write).\r\n"
    tokens: 626
    size: 1620
  - path: specs\standards\067-cozodb-query-sanitization.md
    content: "# Standard 066: CozoDB Query Sanitization\r\n\r\n**Category:** Database / Reliability\r\n**Status:** Approved\r\n**Date:** 2026-01-19\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened?\r\nThe CozoDB FTS (Full-Text Search) parser is highly sensitive to special characters. Queries containing periods (e.g., `arXiv.org`), hyphens, or brackets would trigger opaque `query parser unexpected input` errors, crashing the search service and returning zero results.\r\n\r\n### 2. The Cost\r\n- 2 hours of debugging silent search failures.\r\n- Application instability when users entered technical terms.\r\n- Difficulty in retrieving memories related to specific file extensions or domain names.\r\n\r\n### 3. The Rule\r\n**All user-provided search terms MUST be passed through the `sanitizeFtsQuery` helper before being embedded into a Datalog query.**\r\n\r\n#### The Implementation\r\n```typescript\r\nfunction sanitizeFtsQuery(query: string): string {\r\n  return query\r\n    .replace(/[^a-zA-Z0-9\\s]/g, ' ') // Replace all non-alphanumeric chars with spaces\r\n    .replace(/\\s+/g, ' ')            // Collapse multiple spaces\r\n    .trim()\r\n    .toLowerCase();\r\n}\r\n```\r\n\r\n#### Guidelines\r\n- **FTS-Only**: This sanitization applies specifically to terms destined for `~memory:content_fts`.\r\n- **Preserve Projection**: Sanitization should ONLY affect the value of the `$query` parameter, not the projection variables (e.g., `?[id, content...]`).\r\n- **Reserved Keywords**: Be cautious of reserved words in Datalog; ensuring the query is a string literal passed as a parameter (using `$`) is the safest approach.\r\n"
    tokens: 593
    size: 1568
  - path: specs\standards\068-tag-infection-protocol.md
    content: "# Standard 068: Tag Infection Protocol (Weak Supervision)\r\n\r\n**Status:** Active\r\n**Context:** High-Volume Data Tagging (1M+ Atoms) on Consumer Hardware.\r\n\r\n## 1. The Problem: The \"GPU Bottleneck\"\r\nRunning a Large Language Model (LLM) or even a BERT embedding model on 1 million atoms takes days of GPU time and terabytes of VRAM/Compute.\r\n* **Cost:** ~100ms per atom * 1,000,000 = 27 hours.\r\n* **Result:** The system is too slow to react to real-time data ingestion (e.g., live chat logs).\r\n\r\n## 2. The Solution: \"Teacher-Student\" Weak Supervision\r\nWe decouple **Understanding** (The Teacher) from **Application** (The Student).\r\n\r\n### Phase A: Discovery (The Teacher)\r\n* **Default Agent:** [GLiNER](file:///c:/Users/rsbiiw/Projects/ECE_Core/engine/src/services/tags/gliner.ts) (Zero-Shot Entity Recognition via ONNX).\r\n* **High-Latency Fallback:** GLM-Edge LLM (side-channel).\r\n* **Input:** A random sample of data (e.g., 20-50 atoms).\r\n* **Registry:** `context/master_tags.json` (The \"Virus Definition\").\r\n* **Output:** A list of **Viral Patterns** (e.g., `[\"Dory\", \"Jade\", \"ECE_Core\"]`).\r\n* **Frequency:** Low (Once per Dream Cycle).\r\n\r\n### Phase B: Infection (The Student)\r\n* **Agent:** `wink-nlp` (Statistical/Regex Engine).\r\n* **Input:** The remaining **99.9%** of the dataset.\r\n* **Task:** \"Scan for these exact strings. If found, apply the tag.\"\r\n* **Speed:** < 1ms per atom (CPU only).\r\n* **Output:** Enriched Graph.\r\n\r\n## 3. Implementation Rules\r\n1.  **Master Tag List:** The system must maintain a `sovereign_tags.json` file. This is the persistent memory of the \"Virus.\"\r\n2.  **No Hallucinations:** The Student (Wink) performs **Hard Matching** only. It does not guess.\r\n3.  **Feedback Loop:** If the Teacher finds a new entity (e.g., \"DeepSeek\"), it adds it to the Master List. The next Infection Cycle will automatically tag all historical mentions of \"DeepSeek\" in the entire database.\r\n\r\n## 4. Architecture\r\n```typescript\r\nasync function runInfectionCycle() {\r\n  // 1. Teacher learns new names from recent data\r\n  const newTags = await DiscoveryService.learn(sampleAtoms);\r\n  await TagRegistry.register(newTags);\r\n\r\n  // 2. Student applies names to ALL data (fast)\r\n  await InfectorService.processBatch(allAtoms);\r\n}\r\n```\r\n"
    tokens: 871
    size: 2239
  - path: specs\standards\068_tag_infection_protocol.md
    content: "# Standard 068: Tag Infection Protocol (Weak Supervision)\r\n\r\n**Status:** Active\r\n**Context:** High-Volume Data Tagging (1M+ Atoms) on Consumer Hardware.\r\n\r\n## 1. The Problem: The \"GPU Bottleneck\"\r\nRunning a Large Language Model (LLM) or even a BERT embedding model on 1 million atoms takes days of GPU time and terabytes of VRAM/Compute.\r\n*   **Cost:** ~100ms per atom * 1,000,000 = 27 hours.\r\n*   **Result:** The system is too slow to react to real-time data ingestion (e.g., live chat logs).\r\n\r\n## 2. The Solution: \"Teacher-Student\" Weak Supervision\r\nWe decouple **Understanding** (The Teacher) from **Application** (The Student).\r\n\r\n### Phase A: Discovery (The Teacher)\r\n*   **Agent:** GLiNER / BERT NER (Generalist Entity Recognition).\r\n*   **Input:** A random sample of **0.1%** of the dataset (e.g., 50 atoms).\r\n*   **Task:** \"Read these texts deeply. Identify new Entities (Names, Projects, Locations) that matter.\"\r\n*   **Output:** A list of **Viral Patterns** (e.g., `[\"Dory\", \"Jade\", \"ECE_Core\", \"Bernalillo\"]`).\r\n*   **Frequency:** Low (Once per Dream Cycle).\r\n\r\n### Phase B: Infection (The Student)\r\n*   **Agent:** `wink-nlp` (Statistical/Regex Engine).\r\n*   **Input:** The remaining **99.9%** of the dataset.\r\n*   **Task:** \"Scan for these exact strings. If found, apply the tag.\"\r\n*   **Speed:** < 1ms per atom (CPU only).\r\n*   **Output:** Enriched Graph.\r\n\r\n## 3. Implementation Rules\r\n1.  **Master Tag List:** The system must maintain a `sovereign_tags.json` file. This is the persistent memory of the \"Virus.\"\r\n2.  **No Hallucinations:** The Student (Wink) performs **Hard Matching** only. It does not guess.\r\n3.  **Feedback Loop:** If the Teacher finds a new entity (e.g., \"DeepSeek\"), it adds it to the Master List. The next Infection Cycle will automatically tag all historical mentions of \"DeepSeek\" in the entire database.\r\n\r\n## 4. Architecture\r\n```typescript\r\nasync function runInfectionCycle() {\r\n    // 1. Teacher learns new names from recent data\r\n    const newTags = await DiscoveryService.learn(sampleAtoms);\r\n    await TagRegistry.register(newTags);\r\n\r\n    // 2. Student applies names to ALL data (fast)\r\n    await InfectorService.processBatch(allAtoms);\r\n}\r\n```\r\n"
    tokens: 848
    size: 2193
  - path: specs\standards\069-intelligent-query-expansion.md
    content: "# Standard 069: Intelligent Query Expansion Protocol\r\n\r\n## 1. The Core Philosophy\r\n**\"Semantics for the Human, Tags for the Graph.\"**\r\n\r\nWhile humans provide natural language queries, the Tag-Walker engine is most efficient when seeded with precise tags and entities. This protocol defines the use of an LLM (GLM 1.5B) to bridge the gap between human \"intent\" and graph \"indices\".\r\n\r\n## 2. Expansion Workflow\r\nWhen a query is marked as \"Complex\" or by default in high-precision modes:\r\n\r\n1. **Tag Grounding**: The engine retrieves the **Top 50 most frequent tags** from the database.\r\n2. **LLM Prompting**: The user query and the tag list are passed to the GLM.\r\n3. **Decomposition**: The GLM is instructed to:\r\n   - Identify literal entities (Names, Places, Technical Terms).\r\n   - map abstract concepts to the most semantically similar tags from the provided list.\r\n   - Output a list of \"Expansion Tags\".\r\n4. **Weighted Execution**: The original query (FTS) is combined with the Expansion Tags (Tag Search) to find the Anchors for the Tag-Walker.\r\n\r\n## 3. The Directive\r\nThe GLM must follow a strict persona:\r\n- **Role**: Search Specialist for an Associative Graph.\r\n- **Goal**: Convert natural language into a boolean-style set of high-recall tags.\r\n- **Constraints**: Prefer existing tags from the system list; only invent new tags if absolutely necessary for the query's core meaning.\r\n\r\n## 4. Performance Targets\r\n- **Latency**: Expansion should add < 500ms to the search cycle.\r\n- **Precision Improvement**: Increase \"Anchor Hit Rate\" for multi-intent queries by >40% compared to raw FTS.\r\n"
    tokens: 641
    size: 1598
  - path: specs\standards\069_functional_flow.md
    content: "# Standard 069: Functional Flow (Generators over Loops)\r\n\r\n**Status:** Active\r\n**Context:** Node.js (V8) Stack Limitations vs. Large Datasets (1M+ Atoms).\r\n\r\n## 1. The Recursion Ban\r\n**Recursive functions are STRICTLY PROHIBITED** for data processing pipelines (Ingest, Refiner, Search).\r\n*   **Reason:** Node.js lacks Tail Call Optimization (TCO). Recursion depth > 10k triggers `RangeError` and crashes the process.\r\n*   **Exception:** Recursion is permitted ONLY for hierarchical structures with strictly bounded depth < 100 (e.g., Folder walking, JSON parsing, DOM trees).\r\n\r\n## 2. The Iterator Pattern\r\nReplace `for` loops with **Async Generators** (`async function*`) when processing datasets.\r\n*   **Memory Efficiency:** Generators process items lazily (one by one), preventing RAM spikes (O(1) memory vs O(n)).\r\n*   **Cleanliness:** Eliminates index variables (`i`, `j`), accumulators, and boundary checks.\r\n\r\n## 3. Implementation Guide\r\n\r\n**Bad (Loop Bloat):**\r\n```typescript\r\nconst batches = [/*...*/];\r\nconst results = [];\r\nfor (let i = 0; i < batches.length; i++) {\r\n   results.push(await process(batches[i]));\r\n}\r\n```\r\n\r\n**Good (Pipeline):**\r\n```typescript\r\nimport { pipeline } from 'stream/promises';\r\n\r\nawait pipeline(\r\n    sourceStream,     // Generator\r\n    transformFunction, // Processor\r\n    writeStream       // Database\r\n);\r\n```\r\n\r\n**Sovereign Standard (Manual Consumption):**\r\n```typescript\r\nasync function* itemGenerator() {\r\n    // fetch batch...\r\n    for (const item of batch) yield item;\r\n}\r\n\r\nfor await (const item of itemGenerator()) {\r\n    await process(item);\r\n}\r\n```\r\n"
    tokens: 584
    size: 1600
  - path: specs\standards\070-local-discovery.md
    content: "# Standard 070: Local Discovery (NER Standardization)\r\n\r\n**Status:** Active | **Domain:** 20: DATA | **Relevant Services:** Tags, Dreamer\r\n\r\n## 1. Core Philosophy (The \"Teacher\" Role)\r\nThe system uses a \"Teacher-Student\" architecture for entity discovery:\r\n*   **The Teacher (CPU-Local)**: A fast, free, local model scans all incoming content to find potential entities (tags).\r\n*   **The Student (LLM)**: Takes these rough tags, refines them, and \"infects\" the graph via weak supervision.\r\n\r\nTo ensure the \"Teacher\" is always available and free, it MUST run on the CPU without blocking the main event loop.\r\n\r\n## 2. Model Selection (Transformers.js)\r\nWe use `transformers.js` with the `token-classification` pipeline.\r\n\r\n### Primary Model: `Xenova/bert-base-NER`\r\n*   **Type:** Quantized ONNX\r\n*   **Size:** ~100MB\r\n*   **Performance:** ~20-50ms per sentence on modern CPUs.\r\n*   **Output:** Standard CONLL labels (`B-PER`, `I-ORG`, `B-LOC`, `I-MISC`).\r\n\r\n### Fallback Model: `Xenova/bert-base-multilingual-cased-ner-hrl`\r\n*   **Use Case:** If the primary model fails to download or load.\r\n*   **Capabilities:** Better multilingual support, slightly larger/slower.\r\n\r\n## 3. Supported Entity Types\r\nThe BERT NER models standardizes on 4 core entity types:\r\n1.  **PER (Person)**: Names of people (e.g., \"Sam Altman\", \"Elon\").\r\n2.  **ORG (Organization)**: Companies, institutions (e.g., \"OpenAI\", \"Google\").\r\n3.  **LOC (Location)**: Cities, countries, landmarks (e.g., \"San Francisco\", \"Mars\").\r\n4.  **MISC (Miscellaneous)**: Events, products, works of art.\r\n\r\n## 4. Implementation Rules\r\n1.  **No Native Dependencies:** You MUST disable `sharp` and native ONNX bindings in the `env` config to prevent Windows build failures.\r\n2.  **Quantization:** Always use `quantized: true` to minimize memory usage (~400MB RAM -> ~100MB RAM).\r\n3.  **Graceful Fallback:** If *both* local models fail, the system MUST return an empty list `[]`. The \"Dreamer\" will then automatically default to the main LLM (The \"Failsafe\") for that cycle. It must never crash the engine.\r\n"
    tokens: 800
    size: 2057
  - path: specs\standards\072_epochal_historian.md
    content: "# Standard 072: Epochal Historian Architecture (Hybrid)\r\n\r\n## 1. Core Philosophy\r\nThe Dreamer acts as a **Historian**, organizing memory into a searchable timeline.\r\nTo maintain speed and search relevance ($70/30$), we use a **Hybrid Approach**:\r\n*   **Granular (Episodes)**: Deterministic, Metadata-Rich, Fast.\r\n*   **High-Level (Epochs)**: Narrative, Reflective, Slow.\r\n\r\n## 2. Context-Aware Sorting\r\n**Rule**: Trust the content, then the file.\r\n1.  **Extraction**: Scan first 500 chars for `[YYYY-MM-DD]` or `January 1st, 2024`.\r\n2.  **Sort**: Process memories by `narrative_timestamp` ASC.\r\n\r\n## 3. The Abstraction Hierarchy\r\n\r\n### Level 1: Episodes (Search Anchors)\r\n*   **Technique**: **Deterministic Extraction** (No LLM).\r\n*   **Tool**: `op-layer` (wink-nlp).\r\n*   **Input**: Cluster of 5-25 atoms.\r\n*   **Output**: `Episode` Node.\r\n*   **Schema**:\r\n    *   `content`: \"Entities: [X, Y]. Topics: [A, B]. Range: [Start - End].\"\r\n    *   **Goal**: Provide high-density metadata for the Search weights.\r\n*   **Cost**: O(1) ~0ms.\r\n\r\n### Level 2: Epochs (The Narrative)\r\n*   **Technique**: **Abstractive Summarization** (LLM).\r\n*   **Input**: A sequential lists of `Episode` nodes filling context window.\r\n*   **Output**: `Epoch` Node.\r\n*   **Schema**:\r\n    *   `content`: \"A narrative summary of the week's events...\"\r\n    *   `meta`: Key URLs, Project Milestones.\r\n*   **Goal**: Provide the \"Big Picture\" view for the User.\r\n*   **Cost**: Once per Epoch (High latency acceptable).\r\n\r\n## 4. Performance Constraints\r\n*   **No LLM in Level 1**: Episodes MUST be generated via text analysis.\r\n*   **Generators**: Use async iterators for all processing.\r\n"
    tokens: 619
    size: 1655
  - path: specs\standards\073-data-hygiene-protocol.md
    content: "# Standard 073: Data Hygiene Protocol (The Immune System)\r\n\r\n## Status: ACTIVE\r\n**Related Standards**: [059-reliable-ingestion](./059-reliable-ingestion.md), [065-graph-retrieval](./065-graph-associative-retrieval.md)\r\n\r\n---\r\n\r\n## 1. Core Philosophy: \"Garbage In, Garbage Out\"\r\n\r\nThe sophistication of the ECE Architecture (Graph-Walkers, RAG, Vector Search) is irrelevant if the input signal is noisy. The **Data Hygiene Protocol** is the system's \"Immune System,\" ensuring that only high-quality, sanitized, and correctly tagged data enters the Knowledge Graph.\r\n\r\nThe protocol operates on three layers:\r\n1.  **Aggregation Layer**: Rejection of binary and circular trash.\r\n2.  **Refinement Layer**: Surgical removal of artifacts and auto-classification.\r\n3.  **Ingestion Layer**: Merge-based tagging strategies.\r\n\r\n---\r\n\r\n## 2. Layer 1: Aggregation (The Shield)\r\n**Component**: `read_all.js` (Universal Scraper)\r\n\r\nBefore data allows enters the pipeline, it must pass a \"Allow List\" approach:\r\n\r\n### 2.1 Binary Inspection\r\n*   **Rule**: Files must be inspected at the buffer level.\r\n*   **Mechanism**: If the first 8000 bytes contain `null` bytes, the file is rejected.\r\n*   **Why**: Extensions like `.ts` or `.txt` can sometimes mask binary dumps.\r\n\r\n### 2.2 Feedback Loop Prevention\r\n*   **Rule**: The Aggregator must ignore its own output artifacts.\r\n*   **Mechanism**: Explicit exclusion of `combined_context.yaml`, `combined_memory.yaml`, `combined_context.json`, etc.\r\n*   **Why**: Prevents \"infinite mirror\" bugs where the system reads its own logs about reading its own logs, exponentially increasing token count.\r\n\r\n---\r\n\r\n## 3. Layer 2: Refinement (The Surgeon)\r\n**Component**: `refiner.ts`\r\n\r\nThe Refiner transforms raw text into \"Atoms\" (Thoughts). It is responsible for **Sanitization** and **Classification**.\r\n\r\n### 3.1 The Key Assassin\r\nA heuristic algorithm designed to strip JSON wrappers and execution logs from \"Code\" files while preserving the actual code structure.\r\n\r\n*   **Problem**: When LLMs generate code, they often wrap it in JSON logs (`\"response_content\": \"def foo()...\"`). Ingesting this as \"code\" corrupts the search index with meta-noise.\r\n*   **Solution**:\r\n    1.  **Mask Code Blocks**: Preserve ` ``` ` content.\r\n    2.  **Purge Keys**: Remove `response_content`, `timestamp`: `...`, `type`: `...`.\r\n    3.  **Unescape**: Fix `\\n` and `\\\"` artifacts.\r\n    4.  **Unmask**: Restore the code blocks.\r\n\r\n### 3.2 Project Root Extraction (Auto-Tagging)\r\nContext is derived from the filesystem path, not just the content.\r\n\r\n*   **Logic**: `path/to/project/src/file.ts` ->\r\n    *   `#project:{project_name}`\r\n    *   `#src` (Structural Tag)\r\n    *   `#code` (Type Tag)\r\n\r\n*   **Mapping**:\r\n    *   `/src/` -> `#src`\r\n    *   `/docs/` -> `#docs`\r\n    *   `/tests/` -> `#test`\r\n    *   `/specs/` -> `#specs`\r\n\r\n---\r\n\r\n## 4. Layer 3: Ingestion (The Librarian)\r\n**Component**: `ingest.ts`\r\n\r\nThe Ingestion Service is the final checkpoint. Its job is **Integration**.\r\n\r\n### 4.1 Merged Tagging Strategy\r\nWhen atoms arrive with `AutoTags` (from Refiner) and `BatchTags` (from the user/watchdog), they must be merged without duplication.\r\n\r\n*   **Formula**: `FinalTags = Unique(BatchTags + AtomTags)`\r\n*   **Example**:\r\n    *   Batch: `[\"inbox\"]`\r\n    *   Atom: `[\"#src\", \"#project:ECE_Core\"]`\r\n    *   **Result**: `[\"inbox\", \"#src\", \"#project:ECE_Core\"]`\r\n\r\n### 4.2 Deduplication\r\n*   **Rule**: Idempotency is key.\r\n*   **Mechanism**: Atoms use a deterministic hash of `(FilePath + Content + Sequence Index)`. Re-ingesting the same file generates the same IDs.\r\n*   **Action**: `UPSERT` (Update if exists, Insert if new).\r\n\r\n---\r\n\r\n## 5. Verification\r\nEvery ingestion run must be verified by a \"Read-After-Write\" check (Standard 059).\r\n\r\n*   **Query**: Select ID from DB immediately after write.\r\n*   **Failure**: If ID is missing, throw CRITICAL error and rollback batch (if transaction supported) or alert Watchdog.\r\n"
    tokens: 1483
    size: 3945
  - path: specs\standards\10-ARCH\003-webgpu-initialization-stability.md
    content: "# Standard 003: WebGPU Initialization Stability\r\n\r\n## What Happened?\r\nWebGPU failed to initialize properly in headless browsers, causing GPU access failures and preventing AI model execution. This occurred because browsers require visible windows for GPU access in some configurations.\r\n\r\n## The Cost\r\n- Failed AI model execution in headless environments\r\n- Hours of debugging GPU initialization issues\r\n- Unreliable AI processing in automated systems\r\n- Need for complex workarounds to achieve stable GPU access\r\n\r\n## The Rule\r\n1. **Minimized Window Approach:** Always use `--start-minimized` flag when launching headless browsers that require GPU access:\r\n   ```bash\r\n   start \"Ghost Engine\" /min msedge --app=http://localhost:8000/chat.html?headless=true --start-minimized --remote-debugging-port=9222\r\n   ```\r\n\r\n2. **GPU Buffer Configuration:** Implement 256MB override for Adreno GPUs and other constrained hardware:\r\n   ```javascript\r\n   // In WebGPU configuration\r\n   const adapter = await navigator.gpu.requestAdapter({\r\n       powerPreference: 'high-performance',\r\n       forceFallbackAdapter: false\r\n   });\r\n   ```\r\n\r\n3. **Hardware Abstraction Layer:** Use clamp buffer techniques for Snapdragon/Mobile stability to prevent VRAM crashes.\r\n\r\n4. **Consciousness Semaphore:** Ensure resource arbitration between different components to prevent GPU memory conflicts."
    tokens: 508
    size: 1372
  - path: specs\standards\10-ARCH\004-wasm-memory-management.md
    content: "# Standard 004: WASM Memory Management\r\n\r\n## What Happened?\r\nWASM applications experienced \"memory access out of bounds\" errors and crashes when handling large JSON payloads or complex database operations. This was particularly problematic in `sovereign-db-builder.html` and `unified-coda.html` where JSON parameters were passed to `db.run()`.\r\n\r\n## The Cost\r\n- Crashes during database operations in browser-based CozoDB\r\n- \"Maximum call stack size exceeded\" errors with large JSON payloads\r\n- Unreliable memory operations in browser-based systems\r\n- Hours of debugging memory access violations in WASM\r\n\r\n## The Rule\r\n1. **JSON Stringification:** Always properly stringify JSON parameters before passing to WASM functions:\r\n   ```javascript\r\n   // Before calling db.run() or similar WASM functions\r\n   const jsonString = JSON.stringify(data);\r\n   db.run(query, jsonString);\r\n   ```\r\n\r\n2. **Payload Size Limits:** Implement size checks before processing large JSON payloads in browser workers:\r\n   ```javascript\r\n   if (JSON.stringify(payload).length > MAX_SAFE_SIZE) {\r\n       // Handle large payloads differently or chunk them\r\n   }\r\n   ```\r\n\r\n3. **Error Handling:** Add timeout protection and fallback mechanisms for hanging WASM calls:\r\n   ```javascript\r\n   try {\r\n       const result = await Promise.race([\r\n           db.run(query),\r\n           new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 10000))\r\n       ]);\r\n   } catch (error) {\r\n       // Handle timeout or memory errors gracefully\r\n   }\r\n   ```\r\n\r\n4. **IndexedDB Fallback:** Use `CozoDb.new_from_indexed_db` instead of `new_from_path` for persistent browser storage to avoid filesystem access issues."
    tokens: 627
    size: 1690
  - path: specs\standards\10-ARCH\005-model-loading-configuration.md
    content: "# Standard 005: Model Loading Configuration & Endpoint Verification\r\n\r\n## What Happened?\r\nModel loading failed due to various configuration issues including \"Cannot find model record\" errors, 404 errors for specific model types (OpenHermes, NeuralHermes), and improper model ID to URL mapping. The bridge also had issues accepting model names during embedding requests, causing 503 errors. Additionally, critical endpoints like `/v1/models/pull`, `/v1/models/pull/status`, and GPU management endpoints (`/v1/gpu/lock`, `/v1/gpu/status`, etc.) were documented but missing from the actual bridge implementation, causing 405 errors.\r\n\r\n## The Cost\r\n- Failed model initialization preventing AI functionality\r\n- Multiple 404 errors for specific model types\r\n- 503 and 405 errors during embedding and model download requests\r\n- Hours spent debugging model configuration issues\r\n- Unreliable model loading across different model types\r\n- Significant time wasted discovering that documented endpoints didn't exist in the backend\r\n- Frontend-backend integration failures due to missing API endpoints\r\n\r\n## The Rule\r\n1. **Model ID Mapping:** Always map alternative model names to verified WASM libraries:\r\n   ```python\r\n   # Example mapping for problematic models\r\n   MODEL_MAPPINGS = {\r\n       'OpenHermes': 'Mistral-v0.3',\r\n       'NeuralHermes': 'Mistral-v0.3',\r\n       # Add other mappings as needed\r\n   }\r\n   ```\r\n\r\n2. **Bridge Configuration:** Configure the bridge to accept any model name to prevent 503 errors:\r\n   ```python\r\n   # In webgpu_bridge.py - ensure flexible model name handling\r\n   # Don't validate model names strictly on the bridge side\r\n   ```\r\n\r\n3. **Decouple Internal IDs:** Separate internal model IDs from HuggingFace URLs to prevent configuration mismatches:\r\n   ```javascript\r\n   // In frontend code\r\n   const internalModelId = getModelInternalId(userModelName);\r\n   const modelUrl = getModelUrl(internalModelId);\r\n   ```\r\n\r\n4. **Verification Registry:** Maintain `specs/mlc-urls.md` as a registry for verified WASM binaries to ensure compatibility.\r\n\r\n5. **Bridge-Based URLs:** Use bridge-based model URLs (`http://localhost:8000/models/`) with comprehensive cache-disabling configuration to prevent Cache API errors.\r\n\r\n6. **Endpoint Verification Protocol:** Always verify that documented endpoints exist in the backend implementation before deploying frontend code that depends on them:\r\n   ```python\r\n   # Example: Required endpoints for model management\r\n   REQUIRED_ENDPOINTS = [\r\n       \"/v1/models/pull\",\r\n       \"/v1/models/pull/status\",\r\n       \"/v1/gpu/lock\",\r\n       \"/v1/gpu/unlock\",\r\n       \"/v1/gpu/status\",\r\n       \"/v1/gpu/reset\",\r\n       \"/v1/gpu/force-release-all\",\r\n       \"/v1/system/spawn_shell\",\r\n       \"/v1/shell/exec\"\r\n   ]\r\n   ```\r\n\r\n7. **Documentation-Implementation Synchronization:** When documenting an endpoint, immediately implement it in the backend to prevent documentation-code drift.\r\n\r\n8. **Server Startup Verification:** After adding new endpoints, always verify that the server starts properly and doesn't hang due to problematic async operations or path parameter syntax:\r\n   - Test import functionality: `python -c \"import webgpu_bridge; print('Import successful')\"`\r\n   - Verify server startup and response to requests\r\n   - Avoid problematic syntax like `:path` in route definitions that can cause server hangs\r\n   - Use simple synchronous operations when possible to avoid blocking the event loop"
    tokens: 1297
    size: 3459
  - path: specs\standards\10-ARCH\006-model-url-construction-fix.md
    content: "# Standard 006: Model URL Construction for MLC-LLM Compatibility\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) failed to load models with the error \"TypeError: Failed to construct 'URL': Invalid URL\", while the Anchor Mic (`anchor-mic.html`) loaded models successfully. The issue was that MLC-LLM library expects to access local models using the HuggingFace URL pattern (`/models/{model}/resolve/main/{file}`) but the actual model files are stored in local directories with different structure.\r\n\r\n## The Cost\r\n- 4+ hours debugging model loading failures\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Multiple failed attempts with different URL construction approaches\r\n\r\n## The Rule\r\n1. **URL Redirect Endpoint**: Implement `/models/{model_name}/resolve/main/{file_path}` endpoint to redirect MLC-LLM requests to local model files:\r\n   ```python\r\n   @app.get(\"/models/{model_name}/resolve/main/{file_path:path}\")\r\n   async def model_resolve_redirect(model_name: str, file_path: str):\r\n       import os\r\n       from fastapi.responses import FileResponse, JSONResponse\r\n\r\n       models_base = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"models\")\r\n       actual_path = os.path.join(models_base, model_name, file_path)\r\n\r\n       if os.path.exists(actual_path) and os.path.isfile(actual_path):\r\n           return FileResponse(actual_path)\r\n       else:\r\n           return JSONResponse(status_code=404, content={\r\n               \"error\": f\"File {file_path} not found for model {model_name}\"\r\n           })\r\n   ```\r\n\r\n2. **Path Parameter Safety**: Avoid using problematic syntax like `:path` in route definitions that can cause server hangs; use `{param_name:path}` instead.\r\n\r\n3. **Model File Recognition**: Recognize that MLC-LLM models use sharded parameter files (`params_shard_*.bin`) instead of single `params.json` files.\r\n\r\n4. **Server Startup Verification**: Always verify server starts properly after adding new endpoints by testing import and basic functionality.\r\n\r\n5. **Endpoint Precedence**: Place specific redirect endpoints before general static file mounts to ensure they're processed correctly."
    tokens: 848
    size: 2254
  - path: specs\standards\10-ARCH\007-model-loading-transition-standard.md
    content: "# Standard 007: Model Loading Transition - Online-Only Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing hangs during model loading after GPU configuration, while the Anchor Mic (`anchor-mic.html`) worked perfectly with the same models. The issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to stall after GPU initialization.\r\n\r\nThe old implementation in `chat.html` was trying to:\r\n1. Check for local model files using the `/models/{model}/resolve/main/` pattern\r\n2. Download models through the bridge if not found locally\r\n3. Use a complex configuration with multiple model entries and local file resolution\r\n\r\nThis approach was causing the loading process to hang after the GPU configuration step, preventing models from loading properly.\r\n\r\n## The Cost\r\n- Hours spent debugging model loading failures in `chat.html`\r\n- Confusion between working and failing components (anchor-mic.html vs chat.html)\r\n- Inconsistent model loading across different UI components\r\n- User frustration with non-functional chat interface\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **Archive Complex Logic**: When a complex model loading approach fails, archive it for future reference while implementing a working solution:\r\n   ```javascript\r\n   // Archive the old function with a descriptive name\r\n   async function loadModel_archived() {\r\n       // Original complex implementation\r\n   }\r\n   \r\n   // Implement the working online-only approach\r\n   async function loadModel() {\r\n       // Simplified online-only implementation\r\n   }\r\n   ```\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Progressive Enhancement**: Start with a working online-only solution, then add local model loading capabilities in a separate iteration after the basic functionality is stable.\r\n\r\n6. **Model Loading Verification**: Always test model loading with the same models across different UI components to ensure consistency.\r\n\r\n## Implementation Pattern\r\n\r\n### Working Online-Only Format (Recommended):\r\n```javascript\r\n// Based on the working anchor-mic.html implementation\r\nasync function loadModel() {\r\n    // ... setup code ...\r\n    \r\n    const appConfig = {\r\n        model_list: [{\r\n            model: \"https://huggingface.co/\" + selectedModelId + \"/resolve/main/\",\r\n            model_id: selectedModelId,\r\n            model_lib: modelLib,  // WASM library URL from mapper\r\n            vram_required_MB: 2000,\r\n            low_resource_required: true,\r\n            buffer_size_required_bytes: gpuConfig.maxBufferSize,\r\n            overrides: {\r\n                context_window_size: gpuConfig.isConstrained ? 2048 : 4096\r\n            }\r\n        }],\r\n        useIndexedDBCache: false, // Disable caching to prevent issues\r\n    };\r\n\r\n    engine = await CreateWebWorkerMLCEngine(\r\n        new Worker('./modules/llm-worker.js', { type: 'module' }),\r\n        selectedModelId,\r\n        {\r\n            initProgressCallback: (report) => {\r\n                // Progress reporting\r\n            },\r\n            appConfig: appConfig,\r\n            logLevel: \"INFO\",\r\n            useIndexedDBCache: false, // Force disable cache\r\n        }\r\n    );\r\n}\r\n```\r\n\r\n### Complex Local Resolution (Problematic - Avoid):\r\n```javascript\r\n// DO NOT USE - This causes hangs after GPU configuration\r\n// Complex local file checking and bridge download logic\r\nconst localModelUrl = `${window.location.origin}/models/${safeStrippedId}/ndarray-cache.json`;\r\nconst check = await fetch(localModelUrl, { method: 'HEAD' });\r\n// ... complex download and resolution logic that causes hangs\r\n```\r\n\r\n## Transition Protocol\r\n\r\nWhen transitioning model loading implementations:\r\n\r\n1. **Identify Working Component**: Find a UI component that successfully loads models (e.g., `anchor-mic.html`)\r\n2. **Analyze Working Pattern**: Study the model loading approach in the working component\r\n3. **Archive Complex Logic**: Preserve the old implementation for future reference\r\n4. **Implement Simple Approach**: Adopt the working pattern from the successful component\r\n5. **Test Thoroughly**: Verify the new implementation works with multiple models\r\n6. **Document Changes**: Record the transition in standards documentation\r\n\r\n## Future Considerations\r\n\r\nThe archived local model loading approach should be revisited after further prototyping and debugging. The online-only approach provides immediate functionality while the more complex local approach can be refined separately without blocking development progress."
    tokens: 2044
    size: 5602
  - path: specs\standards\10-ARCH\008-model-loading-online-only-approach.md
    content: "# Standard 008: Model Loading - Online-Only Approach for Browser Implementation\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) was experiencing failures when attempting to load models using a complex local file resolution approach that tried to check for local model files using the `/models/{model}/resolve/main/` pattern, download models through the bridge if not found locally, and use complex multi-model configurations. Meanwhile, `anchor-mic.html` worked perfectly with the same models using a direct online URL approach.\r\n\r\nThe issue was in the complex model loading approach in `chat.html` that attempted to use local model files with bridge downloads, creating a problematic flow that caused the loading process to fail for most models.\r\n\r\n## The Cost\r\n- All models showing as unavailable in API tests\r\n- Confusion between working and failing components\r\n- Inconsistent model loading across different UI components\r\n- User frustration with limited model availability\r\n- Time wasted on attempting to fix complex local model resolution logic\r\n- Delayed development due to complex debugging of the local file + bridge download approach\r\n\r\n## The Rule\r\n1. **Online-Only Model Loading**: For reliable model loading in browser implementations, use direct online URLs instead of complex local file resolution:\r\n   ```javascript\r\n   // Use direct HuggingFace URLs like anchor-mic.html\r\n   const appConfig = {\r\n       model_list: [{\r\n           model: window.location.origin + \"/models/\" + selectedModelId, // This will redirect to online source\r\n           model_id: selectedModelId,\r\n           model_lib: modelLib,  // WASM library URL\r\n           // ... other config\r\n       }],\r\n       useIndexedDBCache: false, // Disable caching to prevent issues\r\n   };\r\n   ```\r\n\r\n2. **Simplified Configuration**: Use the same straightforward approach as `anchor-mic.html` instead of complex multi-model configurations with local file resolution.\r\n\r\n3. **URL Format Consistency**: Ensure all models use the same URL format pattern to avoid configuration mismatches.\r\n\r\n4. **Fallback to Working Patterns**: When debugging model loading issues, compare with known working implementations (like `anchor-mic.html`) and adopt their patterns.\r\n\r\n5. **Bridge Redirect Endpoint**: Ensure the `/models/{model}/resolve/main/{file_path}` endpoint properly redirects to online sources when local files don't exist."
    tokens: 917
    size: 2401
  - path: specs\standards\10-ARCH\009-model-loading-configuration-bridge-vs-direct.md
    content: "# Standard 009: Model Loading Configuration - Bridge vs Direct Online\r\n\r\n## What Happened?\r\nThe Anchor Console (`chat.html`) and other UI components were experiencing inconsistent model loading behavior. The system has two different model loading pathways:\r\n\r\n1. **Bridge-based loading**: Uses `/models/{model_name}` endpoint which should redirect to local files or online sources\r\n2. **Direct online loading**: Uses full HuggingFace URLs directly in the browser\r\n\r\nThe inconsistency occurred because:\r\n- Some components (like `anchor-mic.html`) work with direct online URLs\r\n- Other components (like `chat.html`) were configured for local file resolution\r\n- The bridge redirect endpoint `/models/{model}/resolve/main/{file}` exists but may not be properly redirecting when local files don't exist\r\n\r\n## The Cost\r\n- Confusion about which model loading approach to use\r\n- Inconsistent behavior across different UI components\r\n- Models working in some components but not others\r\n- Debugging time spent on understanding different loading mechanisms\r\n- Users experiencing different model availability depending on which UI they use\r\n\r\n## The Rule\r\n1. **Consistent Model Configuration**: All UI components should use the same model loading approach:\r\n   ```javascript\r\n   // Recommended configuration pattern\r\n   const modelConfig = {\r\n       model: window.location.origin + `/models/${modelId}`,  // Will use bridge redirect\r\n       model_id: `mlc-ai/${modelId}`,                        // Full HuggingFace ID\r\n       model_lib: modelLib,                                  // WASM library URL\r\n   };\r\n   ```\r\n\r\n2. **Bridge Redirect Logic**: The `/models/{model}/resolve/main/{file}` endpoint must:\r\n   - First check for local files in the models directory\r\n   - If local file doesn't exist, redirect to the corresponding HuggingFace URL:\r\n     `https://huggingface.co/mlc-ai/{modelId}/resolve/main/{file}`\r\n\r\n3. **Fallback Handling**: Implement proper fallback when local files are not available:\r\n   ```javascript\r\n   // In the UI, handle both local and online availability\r\n   async function loadModel(modelId) {\r\n       try {\r\n           // Try bridge-based loading first\r\n           await initializeEngine(bridgeConfig(modelId));\r\n       } catch (error) {\r\n           // Fallback to direct online loading if bridge fails\r\n           await initializeEngine(onlineConfig(modelId));\r\n       }\r\n   }\r\n   ```\r\n\r\n4. **Testing Protocol**: Test model loading through both pathways:\r\n   - Verify local file resolution works when files exist\r\n   - Verify online fallback works when local files don't exist\r\n   - Test both the redirect endpoint and direct access patterns\r\n\r\n5. **Documentation Consistency**: All UI components should follow the same documented approach to avoid confusion."
    tokens: 1036
    size: 2777
  - path: specs\standards\10-ARCH\014-async-best-practices.md
    content: "# Standard 014: Async/Await Best Practices for FastAPI\r\n\r\n## What Happened?\r\nThe system had multiple \"coroutine was never awaited\" warnings due to improper async/await usage in the webgpu_bridge.py. These warnings occurred when async functions were called without being properly awaited or when they weren't integrated correctly with FastAPI's event loop system.\r\n\r\n## The Cost\r\n- Runtime warnings cluttering the console output\r\n- Potential resource leaks from improperly handled async operations\r\n- Unpredictable behavior in WebSocket connections and API endpoints\r\n- Difficulty debugging real issues due to noise from async warnings\r\n\r\n## The Rule\r\n1. **Proper Await Usage**: All async functions must be awaited when called within async contexts\r\n   ```python\r\n   # Correct\r\n   await add_log_entry(\"source\", \"type\", \"message\")\r\n   \r\n   # Incorrect\r\n   add_log_entry(\"source\", \"type\", \"message\")  # Creates unawaited coroutine\r\n   ```\r\n\r\n2. **Event Loop Integration**: When scheduling tasks at module level, ensure they run within an active event loop:\r\n   ```python\r\n   # Correct - in startup event\r\n   async def startup_event():\r\n       await add_log_entry(\"System\", \"info\", \"Service started\")\r\n   \r\n   # Incorrect - at module level before event loop starts\r\n   # asyncio.create_task(add_log_entry(...))  # Will cause warning\r\n   ```\r\n\r\n3. **FastAPI Event Handlers**: Use FastAPI's event system (`@app.on_event(\"startup\")`) for initialization tasks that require async operations\r\n\r\n4. **Background Tasks**: For fire-and-forget async operations, use FastAPI's BackgroundTasks or properly scheduled asyncio tasks within request handlers\r\n\r\n5. **WebSocket Cleanup**: Always ensure proper cleanup of async resources in WebSocket exception handlers to prevent resource leaks\r\n\r\n6. **Exception Handling**: Wrap async operations in try/catch blocks that properly handle async exceptions and clean up resources"
    tokens: 720
    size: 1905
  - path: specs\standards\10-ARCH\014-gpu-resource-availability.md
    content: "# Standard 014: GPU Resource Availability Management\r\n\r\n## What Happened?\r\nThe system was returning 503 (Service Unavailable) errors for `/v1/memory/search` and other endpoints when GPU resources were busy or locked by another process. Users encountered errors like \"Ghost Engine Disconnected\" even when the Ghost Engine was running but GPU resources were temporarily unavailable.\r\n\r\n## The Cost\r\n- 2+ hours spent troubleshooting \"disconnected\" errors that were actually GPU resource contention issues\r\n- Misleading error messages suggesting connection problems when the real issue was resource availability\r\n- Poor user experience with hard failures instead of graceful queuing\r\n- Multiple failed requests during peak GPU usage periods\r\n\r\n## The Rule\r\n1. **GPU Queuing Protocol**: All GPU-dependent operations must use the `/v1/gpu/lock` and `/v1/gpu/unlock` endpoints to acquire/release resources safely\r\n2. **Graceful Degradation**: When GPU resources are unavailable, return informative messages about queue status rather than 503 errors\r\n3. **Resource Status Checks**: Use `/v1/gpu/status` to check availability before attempting GPU operations\r\n4. **Timeout Handling**: Implement proper timeouts (60s+) for GPU resource acquisition with clear user feedback\r\n5. **Queue Position Reporting**: Inform users of their position in the GPU queue when applicable\r\n6. **Fallback Strategies**: For non-critical operations, implement CPU-based fallbacks when GPU is heavily queued"
    tokens: 573
    size: 1475
  - path: specs\standards\10-ARCH\023-anchor-lite-simplification.md
    content: "# Standard 023: Anchor Lite Architecture\r\n\r\n## 1. What Happened\r\nThe system became overly complex with multiple database views (DB Builder, Memory Builder) and experimental chat interfaces, causing data synchronization issues and user confusion.\r\n\r\n## 2. The Cost\r\n- Loss of trust in retrieval (\"Jade\" not found).\r\n- High maintenance overhead.\r\n\r\n## 3. The Rule\r\n**Single Pipeline Architecture:**\r\n```mermaid\r\ngraph LR\r\n    BS[File System] -->|Watchdog| DB[Ghost Engine]\r\n    DB -->|WebSocket| UI[Context Console]\r\n    note[Single Source of Truth] -.-> BS\r\n```\r\n1. **Source:** File System (`context/` folder) is the Single Source of Truth.\r\n2. **Ingest:** `watchdog.py` monitors files and pushes to the Engine.\r\n3. **Index:** `ghost.html` (Headless CozoDB) maintains the index.\r\n4. **Retrieve:** `context.html` is the sole interface for search.\r\n"
    tokens: 327
    size: 846
  - path: specs\standards\10-ARCH\031-ghost-engine-stability-fix.md
    content: "# Fix for Ghost Engine CozoDB Schema Issues\r\n\r\n## Problem\r\nThe Ghost Engine in ghost.html is experiencing crashes due to schema creation failures:\r\n- \"Schema creation failed: undefined\"\r\n- \"Test query failed: undefined\"\r\n- Browser crashes when reloading database\r\n\r\n## Root Cause\r\nThe issue is in the `ensureSchema()` function in ghost.html. The schema creation query attempts to create both the main table and the FTS (Full Text Search) index in a single operation:\r\n\r\n```javascript\r\nconst schemaQuery = `\r\n:create memory {\r\n    id: String =>\r\n    timestamp: Int,\r\n    content: String,\r\n    source: String,\r\n    type: String\r\n} if not exists;\r\n\r\n::fts create memory:content_fts {\r\n    extractor: content,\r\n    tokenizer: Simple,\r\n    filters: [Lowercase]\r\n} if not exists;\r\n`;\r\n```\r\n\r\nIf the FTS creation fails (which can happen with certain CozoDB WASM builds), the entire schema creation fails.\r\n\r\n## Solution\r\nModify the `ensureSchema()` function to separate schema and FTS creation:\r\n\r\n```javascript\r\n// First, create the basic schema\r\nasync function ensureSchema() {\r\n    // Create basic table first\r\n    const basicSchemaQuery = `\r\n    :create memory {\r\n        id: String =>\r\n        timestamp: Int,\r\n        content: String,\r\n        source: String,\r\n        type: String\r\n    } if not exists;\r\n    `;\r\n\r\n    try {\r\n        const result = await db.run(basicSchemaQuery, \"{}\");\r\n        const jsonResult = JSON.parse(result);\r\n\r\n        if (jsonResult.ok) {\r\n            log(\"SUCCESS\", \"Basic schema created successfully\");\r\n        } else {\r\n            log(\r\n                \"ERROR\",\r\n                \"Basic schema creation failed: \" +\r\n                    JSON.stringify(jsonResult.error),\r\n            );\r\n            return false;\r\n        }\r\n    } catch (e) {\r\n        log(\"ERROR\", \"Basic schema operation failed: \" + e.message);\r\n        return false;\r\n    }\r\n\r\n    // Then, try to create FTS separately\r\n    const ftsQuery = `\r\n    ::fts create memory:content_fts {\r\n        extractor: content,\r\n        tokenizer: Simple,\r\n        filters: [Lowercase]\r\n    } if not exists;\r\n    `;\r\n\r\n    try {\r\n        const ftsResult = await db.run(ftsQuery, \"{}\");\r\n        const ftsJsonResult = JSON.parse(ftsResult);\r\n\r\n        if (ftsJsonResult.ok) {\r\n            log(\"SUCCESS\", \"FTS index created successfully\");\r\n        } else {\r\n            log(\r\n                \"WARNING\",\r\n                \"FTS creation failed (search will be limited): \" +\r\n                    JSON.stringify(ftsJsonResult.error),\r\n            );\r\n            // Don't return false here - basic functionality still works\r\n        }\r\n    } catch (e) {\r\n        log(\r\n            \"WARNING\", \r\n            \"FTS creation failed (search will be limited): \" + e.message\r\n        );\r\n        // Don't return false - basic functionality still works\r\n    }\r\n\r\n    return true;\r\n}\r\n```\r\n\r\n## Additional Improvements\r\n1. Better error handling in testQuery function\r\n2. More robust database initialization with fallbacks\r\n3. Improved error messages that don't return \"undefined\"\r\n\r\n## Status\r\nThis fix needs to be manually applied to ghost.html in the tools/ directory."
    tokens: 1118
    size: 3137
  - path: specs\standards\10-ARCH\032-ghost-engine-initialization-flow.md
    content: "# Standard 032: Ghost Engine Initialization and Ingestion Flow\r\n\r\n## What Happened?\r\nThe Ghost Engine was experiencing race conditions where memory ingestion requests were being processed before the database was fully initialized. This caused errors like \"Cannot read properties of null (reading 'run')\" and inconsistent ingestion behavior between the Bridge API logs and the Ghost Engine logs.\r\n\r\n## The Cost\r\n- Database ingestion failures when Ghost Engine connected to Bridge before database initialization completed\r\n- Inconsistent logging between Bridge and Ghost Engine (Bridge showing success, Ghost Engine showing failures)\r\n- Race conditions where ingestion requests arrived before database was ready\r\n- Poor user experience with failed memory operations\r\n- Confusing error messages in the UI\r\n\r\n## The Rule\r\n1. **Sequential Initialization**: The Ghost Engine must initialize the database completely before signaling readiness to the Bridge.\r\n\r\n2. **Database Readiness Checks**: All ingestion and search operations must verify that the database object is properly initialized before attempting operations.\r\n\r\n3. **Proper Error Handling**: When database is not ready, the Ghost Engine must return appropriate error messages to the Bridge instead of failing silently.\r\n\r\n4. **Synchronous Connection Flow**: WebSocket connection must follow: Connect  Initialize Database  Signal Ready  Process Requests.\r\n\r\n5. **Graceful Degradation**: If database initialization fails, the Ghost Engine must report the error to the Bridge and not attempt to process requests.\r\n\r\n6. **Message Type Handling**: The system must properly handle all message types including `engine_error` responses.\r\n\r\n## Implementation\r\n- Modified WebSocket connection flow to initialize database before signaling readiness\r\n- Added database readiness checks in `handleIngest` and `handleSearch` functions\r\n- Implemented proper error responses when database is not ready\r\n- Added support for `engine_error` message type handling\r\n- Enhanced error logging with fallbacks to prevent \"undefined\" messages\r\n- Ensured sequential processing: Connect  DB Init  Ready Signal  Process Requests"
    tokens: 819
    size: 2172
  - path: specs\standards\10-ARCH\034-nodejs-monolith-migration.md
    content: "# Standard 034: Node.js Monolith Migration\r\n\r\n## What Happened?\r\nThe system was migrated from a Python/Browser Bridge architecture (V2) to a Node.js Monolith architecture (V3). This involved:\r\n- Archiving legacy Python infrastructure (webgpu_bridge.py, anchor_watchdog.py, etc.)\r\n- Creating a new Node.js server with CozoDB integration\r\n- Implementing autonomous execution protocols\r\n- Converting Python scripts to JavaScript equivalents\r\n\r\n## The Cost\r\n- Fragile headless browser architecture with WebGPU dependencies\r\n- Complex Python/JavaScript bridge with multiple failure points\r\n- Resource-intensive browser processes\r\n- Platform compatibility issues (especially on ARM/Android)\r\n- Complex deployment and dependency management\r\n\r\n## The Rule\r\n1. **Node.js Monolith**: Use Node.js as the primary runtime environment for the Context Engine.\r\n\r\n2. **CozoDB Integration**: Integrate CozoDB directly using `cozo-node` for persistent storage.\r\n\r\n3. **Autonomous Execution**: Implement Protocol 001 for detached service execution with proper logging and verification.\r\n\r\n4. **File Watchdog**: Use `chokidar` for efficient file system monitoring and automatic ingestion.\r\n\r\n5. **API Endpoints**: Implement standardized endpoints:\r\n   - `POST /v1/ingest` - Content ingestion\r\n   - `POST /v1/query` - CozoDB query execution\r\n   - `GET /health` - Service health verification\r\n\r\n6. **Legacy Archival**: Archive all V2 Python infrastructure to `archive/v2_python_bridge/`.\r\n\r\n7. **JavaScript Conversion**: Convert Python utility scripts to JavaScript equivalents for consistency.\r\n\r\n8. **Termux Compatibility**: Ensure architecture works on Termux/Linux environments.\r\n\r\n## Implementation\r\n- Created Node.js server in `server/` directory\r\n- Implemented CozoDB with RocksDB backend\r\n- Added file watching functionality with chokidar\r\n- Created migration script for legacy session data\r\n- Converted read_all.py to read_all.js in both locations\r\n- Added proper error handling and logging\r\n- Implemented Protocol 001 for safe service execution"
    tokens: 758
    size: 2032
  - path: specs\standards\20-DATA\017-file-ingestion-debounce-hash-checking.md
    content: "# Standard 017: File Ingestion Debounce and Hash Checking\r\n\r\n## What Happened?\r\nThe Watchdog service was triggering excessive memory ingestion when modern editors (VS Code, Obsidian) would autosave files frequently. This caused \"Memory Churn\" in CozoDB with duplicate content being ingested repeatedly, fragmenting the database and spiking CPU usage.\r\n\r\n## The Cost\r\n- High CPU usage from repeated ingestion of unchanged content\r\n- Database fragmentation from duplicate entries\r\n- Poor performance during active editing sessions\r\n- 2+ hours spent implementing debounce and hash checking to prevent \"Autosave Flood\"\r\n\r\n## The Rule\r\n1. **Debounce File Events**: Implement a debounce mechanism that waits for a period of silence before processing file changes:\r\n   ```python\r\n   # Wait for debounce period before processing\r\n   debounce_time = 2.0  # seconds\r\n   ```\r\n\r\n2. **Content Hash Verification**: Calculate MD5 hash of file content before ingestion and compare with previously ingested version:\r\n   ```python\r\n   import hashlib\r\n   current_hash = hashlib.md5(content).hexdigest()\r\n   if file_path in self.file_hashes and self.file_hashes[file_path] == current_hash:\r\n       # Skip ingestion - content hasn't changed\r\n       return\r\n   ```\r\n\r\n3. **Cancel Pending Operations**: Cancel any existing debounce timer when a new file event occurs for the same file:\r\n   ```python\r\n   if file_path in self.debounce_timers:\r\n       self.debounce_timers[file_path].cancel()\r\n   ```\r\n\r\n4. **Proper Cleanup**: Clean up debounce timer references after processing:\r\n   ```python\r\n   if file_path in self.debounce_timers:\r\n       del self.debounce_timers[file_path]\r\n   ```\r\n\r\nThis standard prevents excessive database writes from autosave features while ensuring all actual changes are captured."
    tokens: 668
    size: 1785
  - path: specs\standards\20-DATA\019-code-file-ingestion-comprehensive-context.md
    content: "# Standard 019: Code File Ingestion for Comprehensive Context\r\n\r\n## What Happened?\r\nThe Watchdog service was only monitoring text files (.txt, .md, .markdown) but ignoring code files which represent a significant portion of developer context. This created an \"Ingestion Blind Spot\" where the system was blind to codebase context.\r\n\r\n## The Cost\r\n- Limited context ingestion for developers\r\n- Missing important code-related information\r\n- 30 minutes spent updating watchdog.py to include code extensions\r\n\r\n## The Rule\r\n1. **Expand File Extensions**: Include common programming language extensions in file monitoring:\r\n   ```python\r\n   enabled_extensions = {\".txt\", \".md\", \".markdown\", \".py\", \".js\", \".html\", \".css\", \r\n                         \".json\", \".yaml\", \".yml\", \".sh\", \".bat\", \".ts\", \".tsx\", \r\n                         \".jsx\", \".xml\", \".sql\", \".rs\", \".go\", \".cpp\", \".c\", \".h\", \".hpp\"}\r\n   ```\r\n\r\n2. **Comprehensive Coverage**: Monitor all relevant text-based file types that contain context\r\n\r\n3. **Maintain Performance**: Ensure file size limits still apply to prevent performance issues with large code files\r\n\r\nThis standard ensures that developer context is fully captured by including code files in passive ingestion."
    tokens: 462
    size: 1229
  - path: specs\standards\20-DATA\021-chat-session-persistence-context-continuity.md
    content: "# Standard 021: Chat Session Persistence for Context Continuity\r\n\r\n## What Happened?\r\nThe anchor.py CLI client maintained conversation history only in memory. If the terminal was closed or the CLI crashed, the entire conversation history was lost. This created a \"Lost Context\" risk where valuable conversation history was not preserved.\r\n\r\n## The Cost\r\n- Loss of conversation history on CLI crashes or termination\r\n- Broken loop between active chatting and long-term memory\r\n- 45 minutes spent implementing chat session persistence to context folder\r\n\r\n## The Rule\r\n1. **Auto-Save Sessions**: Automatically save each chat message to a session file:\r\n   ```python\r\n   def save_message_to_session(role, content):\r\n       # Create timestamped session file in context/sessions/\r\n       # Append each message as it occurs\r\n   ```\r\n\r\n2. **Session Directory**: Create a dedicated `context/sessions/` directory for chat logs:\r\n   ```python\r\n   SESSIONS_DIR = os.path.join(CONTEXT_DIR, \"sessions\")\r\n   os.makedirs(SESSIONS_DIR, exist_ok=True)\r\n   ```\r\n\r\n3. **Markdown Format**: Save conversations in markdown format for easy reading and processing:\r\n   ```python\r\n   # Format: ## Role\\nContent\\n\\n for each message\r\n   ```\r\n\r\n4. **Loop Closure**: Ensure that chat sessions become text files that the Watchdog service can monitor and ingest into long-term memory automatically.\r\n\r\nThis standard ensures conversation history survives CLI crashes and becomes part of the persistent context."
    tokens: 566
    size: 1479
  - path: specs\standards\20-DATA\022-text-file-source-of-truth-cross-machine-sync.md
    content: "# Standard 022: Text-File Source of Truth for Cross-Machine Sync\r\n\r\n## What Happened?\r\nThe CozoDB database lives in IndexedDB inside the headless browser profile, making it impossible to sync between machines. Chat history and learned connections were trapped in the browser instance and lost when switching laptops. The system needed a \"Text-File Source of Truth\" approach where the database is treated as a cache and all important data is stored in text files.\r\n\r\n## The Cost\r\n- Lost conversation history when switching between machines\r\n- Inability to sync learned connections and context across devices\r\n- 1 hour spent implementing daily session files and text-file persistence\r\n\r\n## The Rule\r\n1. **Database is Cache**: Treat CozoDB as a cache, not the source of truth:\r\n   ```python\r\n   # All important data must exist in text files first\r\n   # Database is rebuilt from text files on each machine\r\n   ```\r\n\r\n2. **Daily Session Files**: Create daily markdown files for chat persistence:\r\n   ```python\r\n   def ensure_session_file():\r\n       date_str = datetime.now().strftime(\"%Y-%m-%d\")\r\n       filename = f\"chat_{date_str}.md\"\r\n       # Creates daily consolidated session files\r\n   ```\r\n\r\n3. **Text-File First**: All important information must be written to text files:\r\n   ```python\r\n   # Every chat message gets saved to markdown file\r\n   # Files are automatically ingested by watchdog service\r\n   # Creates infinite loop: Chat -> File -> Ingestion -> Memory -> Next Chat\r\n   ```\r\n\r\n4. **Cross-Machine Sync**: Use Dropbox/Git for file synchronization:\r\n   ```python\r\n   # Text files sync automatically via Dropbox/Git\r\n   # Database rebuilds from text files on each machine\r\n   # Ensures consistent context across all devices\r\n   ```\r\n\r\n5. **Timestamped Entries**: Format messages with timestamps for tracking:\r\n   ```python\r\n   # Format: ### ROLE [HH:MM:SS]\\n{content}\\n\\n\r\n   ```\r\n\r\nThis standard ensures that all important context is persisted in text files that can be synced across machines, making the database truly portable."
    tokens: 791
    size: 2039
  - path: specs\standards\20-DATA\024-context-ingestion-pipeline-fix.md
    content: "# Standard 024: Context Ingestion Pipeline - Field Name Alignment Protocol\r\n\r\n## What Happened?\r\nThe context ingestion pipeline was failing silently due to field name mismatches between the watchdog service and the ghost engine. The watchdog was sending `filetype` but the memory ingestion endpoint expected `file_type`, and the ghost engine was looking for `msg.filetype` instead of `msg.file_type`. This caused the database to appear empty even though files were being processed, resulting in failed context searches for terms like \"Dory\" or \"Coda\".\r\n\r\n## The Cost\r\n- 2+ hours spent debugging why context files weren't appearing in the database\r\n- Confusion from \"Database appears empty!\" messages in ghost engine logs\r\n- Failed context searches returning no results despite files existing in context directory\r\n- Misleading \"Ingested\" messages in watchdog logs that masked the actual field name mismatch\r\n- Users experiencing broken context retrieval functionality\r\n\r\n## The Rule\r\n1. **Field Name Consistency**: All components in the ingestion pipeline must use consistent field names:\r\n   - Watchdog sends: `file_type`, `source`, `content`, `filename`\r\n   - Bridge expects: `file_type`, `source`, `content`, `filename`\r\n   - Ghost engine receives: `file_type`, `source`, `content`, `filename`\r\n\r\n2. **Payload Validation**: Always validate that field names match across the entire pipeline:\r\n   ```javascript\r\n   // In ghost.html handleIngest function\r\n   await runQuery(query, { \r\n       data: [[id, ts, msg.content, msg.source || msg.filename, msg.file_type || \"text\"]] \r\n   });\r\n   ```\r\n\r\n3. **Source Identification**: The watchdog must send a proper source identifier instead of relying on default \"unknown\" values\r\n\r\n4. **Error Reporting**: Include detailed error messages when ingestion fails to help with debugging"
    tokens: 694
    size: 1824
  - path: specs\standards\20-DATA\029-consolidated-data-aggregation.md
    content: "# Standard 029: Consolidated Data Aggregation with YAML Support\r\n\r\n## What Happened?\r\nThe system had multiple scripts performing similar functions for data aggregation and migration:\r\n- `migrate_history.py` - Legacy session migration to YAML\r\n- `read_all.py` in context directory - Data aggregation to JSON\r\n- Multiple overlapping data processing scripts\r\n\r\nThis created redundancy and confusion about which script to use for data aggregation. The functionality has been consolidated into a single authoritative script: `context/Coding-Notes/Notebook/read_all.py` which now supports all three output formats (text, JSON, YAML).\r\n\r\n## The Cost\r\n- Multiple scripts with overlapping functionality\r\n- Confusion about which script to use for data aggregation\r\n- Maintenance burden of multiple similar scripts\r\n- Inconsistent output formats across scripts\r\n- Redundant code that needed to be updated in multiple places\r\n\r\n## The Rule\r\n1. **Single Authority**: Use `context/Coding-Notes/Notebook/read_all.py` as the single authoritative script for data aggregation from the context directory.\r\n\r\n2. **Multi-Format Output**: The script must generate three output formats:\r\n   - `combined_text.txt` - Human-readable text corpus\r\n   - `combined_memory.json` - Structured JSON for database ingestion\r\n   - `combined_memory.yaml` - Structured YAML for easier processing and migration\r\n\r\n3. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.\r\n\r\n4. **Encoding Handling**: The script must handle various file encodings using chardet for reliable processing.\r\n\r\n5. **Recursive Processing**: The script must process all subdirectories while respecting exclusion rules.\r\n\r\n6. **Metadata Preservation**: File metadata (path, timestamp) must be preserved in structured outputs.\r\n\r\n## Implementation\r\n- Consolidated migrate_history.py functionality into read_all.py\r\n- Moved migrate_history.py to archive/tools/\r\n- Updated read_all.py to generate YAML output with proper multiline formatting\r\n- Used yaml.dump() with custom representer for multiline strings\r\n- Maintained all existing functionality while adding YAML support\r\n- Preserved the same exclusion rules and file type filtering"
    tokens: 858
    size: 2268
  - path: specs\standards\20-DATA\030-multi-format-output.md
    content: "# Standard 030: Multi-Format Output for Project Aggregation\r\n\r\n## What Happened?\r\nThe `read_all.py` script in the root directory was only generating text and JSON outputs for project aggregation. To improve compatibility with various processing tools and follow the documentation policy of supporting YAML format, the script was updated to also generate a YAML version of the memory records.\r\n\r\n## The Cost\r\n- Limited output format options for downstream processing\r\n- Inconsistency with the documentation policy that prefers YAML for configuration and data exchange\r\n- Missing opportunity to provide easily readable structured data in YAML format\r\n- Users had to convert JSON to YAML if they needed that format\r\n\r\n## The Rule\r\n1. **Multi-Format Output**: The `read_all.py` script must generate both JSON and YAML versions of memory records.\r\n\r\n2. **YAML Formatting**: YAML output must use proper multiline string formatting (literal style with `|`) for content with line breaks to ensure readability.\r\n\r\n3. **Consistent Naming**: Output files should follow consistent naming patterns:\r\n   - `combined_text.txt` - Aggregated text content\r\n   - `combined_memory.json` - Structured JSON memory records\r\n   - `combined_text.yaml` - Structured YAML memory records\r\n\r\n4. **Custom Representers**: Use custom YAML representers to handle multiline strings appropriately with the `|` indicator.\r\n\r\n5. **Encoding Handling**: Ensure proper UTF-8 encoding for both input and output operations.\r\n\r\n## Implementation\r\n- Updated `read_all.py` to import and use the `yaml` module\r\n- Added custom string representer for multiline content\r\n- Created separate YAML output file with proper formatting\r\n- Maintained all existing functionality while adding YAML support\r\n- Used `yaml.dump()` with appropriate parameters for clean output"
    tokens: 698
    size: 1814
  - path: specs\standards\20-DATA\033-cozodb-syntax-compliance.md
    content: "# Standard 033: CozoDB Schema and Query Syntax Compliance\r\n\r\n## What Happened?\r\nThe Ghost Engine was experiencing continued ingestion failures due to incorrect CozoDB query syntax. Issues included:\r\n- Incorrect schema creation syntax with improper line breaks\r\n- Wrong insertion query syntax (`:put` vs `:insert`)\r\n- Improper parameter formatting for data insertion\r\n- Schema validation that didn't properly propagate failure status\r\n\r\n## The Cost\r\n- Persistent ingestion failures despite initialization flow fixes\r\n- Incorrect CozoDB query syntax causing \"Unknown error\" messages\r\n- Failed schema creation preventing proper database operations\r\n- Misleading success messages when operations were actually failing\r\n- Continued inconsistency between Bridge and Ghost Engine logs\r\n\r\n## The Rule\r\n1. **Schema Creation Syntax**: Use proper CozoDB schema creation syntax without line breaks in the schema definition: `:create memory {id: String => timestamp: Int, content: String, source: String, type: String} if not exists;`\r\n\r\n2. **FTS Creation Syntax**: Use proper CozoDB FTS creation syntax: `::fts create memory:content_fts {extractor: content, tokenizer: Simple, filters: [Lowercase]} if not exists;`\r\n\r\n3. **Insert Query Syntax**: Use `:insert` or `:replace` with proper parameter binding syntax: `:insert memory {id, timestamp, content, source, type} <- $data`\r\n\r\n4. **Parameter Formatting**: Format parameters correctly as nested arrays for bulk operations: `{data: [[id, timestamp, content, source, type]]}`\r\n\r\n5. **Schema Validation**: Properly propagate schema creation success/failure status to prevent operations on uninitialized database\r\n\r\n6. **Error Propagation**: Ensure all database operations properly handle and report errors to maintain consistency between Bridge and Ghost Engine\r\n\r\n## Implementation\r\n- Fixed schema creation queries to use correct CozoDB syntax without line breaks\r\n- Updated insertion queries from `:put` to `:insert` with proper syntax\r\n- Corrected parameter formatting for data insertion operations\r\n- Enhanced schema validation to properly return success/failure status\r\n- Improved error propagation throughout database operations\r\n- Maintained backward compatibility while fixing syntax issues"
    tokens: 839
    size: 2235
  - path: specs\standards\20-DATA\037-database-hydration-snapshot-portability.md
    content: "# Standard 037: Database Hydration & Snapshot Portability\r\n\r\n**Status:** Active | **Domain:** DATA | **Number:** 037\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nLarge context libraries (89MB+) were difficult to move between machines because the \"Filesystem as Source of Truth\" required re-ingesting thousands of files. This process was slow, resource-intensive, and prone to file watcher errors or race conditions during initial indexing.\r\n\r\n### 2. The Cost\r\nHours of developer time lost to \"ingestion loops,\" \"missing context\" errors after migration, and inconsistent database states across different environments. The system was \"brittle\" during the first 10 minutes of startup on a new machine.\r\n\r\n### 3. The Rule\r\nUse **YAML Snapshots** as the primary portable artifact for the database state.\r\n- **Eject**: Export the current database state to a single YAML file for backup or migration.\r\n- **Auto-Hydrate**: On startup, the engine automatically checks if the database is empty. If it is, it picks the **latest** snapshot from the `backups/` folder and performs a bulk, idempotent restore.\r\n- **Manual Control**: Users can manually move old snapshots to a separate folder to prevent them from being picked, or delete the `engine/context.db` folder to force a fresh hydration from the latest snapshot.\r\n- **Persistence**: Once hydrated, the data lives in the persistent `engine/context.db` (RocksDB). The original source files are no longer required for the engine to function, enabling a \"Docker-style\" build-and-ship workflow.\r\n\r\n---\r\n*Verified by Architecture Council. Created after 89MB Context Migration.*\r\n"
    tokens: 642
    size: 1624
  - path: specs\standards\30-OPS\001-windows-console-encoding.md
    content: "# Standard 001: Windows Console Encoding\r\n\r\n## What Happened?\r\nThe Python Bridge (`webgpu_bridge.py`) crashed immediately upon launch on Windows 11. The error was `UnicodeEncodeError: 'charmap' codec can't encode character...`.\r\n\r\n## The Cost\r\n- 3 failed integration attempts.\r\n- \"Integration Hell\" state requiring full manual intervention.\r\n- Bridge stability compromised during demos.\r\n\r\n## The Rule\r\n1. **Explicit Encoding:** All Python scripts outputting to stdout must explicitly handle encoding.\r\n2. **The Fix:** Include this snippet at the top of all entry points:\r\n   ```python\r\n   import sys\r\n   if sys.platform == \"win32\":\r\n       sys.stdout.reconfigure(encoding='utf-8')\r\n   ```\r\n\r\n3. **Validation:** CI/CD or startup scripts must verify the bridge launches without encoding errors."
    tokens: 302
    size: 793
  - path: specs\standards\30-OPS\011-comprehensive-testing-verification.md
    content: "# Standard 011: Comprehensive Testing and Verification Protocol\r\n\r\n## What Happened?\r\nThe Anchor Core system required a comprehensive testing approach to prevent issues like missing endpoints, function syntax errors, model loading failures, and data pipeline problems. Previously, these issues were discovered reactively during development or deployment, causing delays and debugging overhead.\r\n\r\n## The Cost\r\n- Hours spent debugging missing endpoints after deployment\r\n- Time wasted on syntax errors in critical files\r\n- Model loading failures discovered during user testing\r\n- Data pipeline issues found late in the development cycle\r\n- Lack of systematic verification leading to inconsistent quality\r\n\r\n## The Rule\r\n1. **Dedicated Test Directory**: All test files must be organized in a dedicated `tests/` directory in the project root\r\n    ```bash\r\n    tests/\r\n     comprehensive_test_suite.py\r\n     endpoint_syntax_verification.py\r\n     test_model_loading.py\r\n     test_model_availability.py\r\n     test_gpu_fixes.py\r\n     test_orchestrator.py\r\n     model_test.html\r\n     README.md\r\n    ```\r\n\r\n2. **Comprehensive Test Coverage**: Tests must cover:\r\n   - Model loading functionality\r\n   - Data pipeline verification\r\n   - Endpoint accessibility\r\n   - Missing endpoint detection\r\n   - Function syntax error detection\r\n   - System health verification\r\n\r\n3. **Endpoint Verification Protocol**: All critical endpoints must be tested for accessibility:\r\n   ```python\r\n   # Example endpoint test pattern\r\n   critical_endpoints = [\r\n       (\"/health\", \"GET\", 200),\r\n       (\"/v1/chat/completions\", \"POST\", 400),  # Expected 400 due to missing body\r\n       (\"/v1/gpu/status\", \"GET\", 200),\r\n       # ... add all critical endpoints\r\n   ]\r\n   ```\r\n\r\n4. **Syntax Verification**: Critical Python files must be checked for syntax errors:\r\n   ```python\r\n   # Use AST parsing to verify syntax\r\n   import ast\r\n   with open(file_path, 'r') as f:\r\n       source_code = f.read()\r\n   ast.parse(source_code)  # Will raise SyntaxError if invalid\r\n   ```\r\n\r\n5. **Test Documentation**: All test files must be documented in `tests/README.md` with:\r\n   - Purpose of each test file\r\n   - How to run the tests\r\n   - Test coverage details\r\n   - Expected outputs\r\n\r\n6. **Pre-Deployment Verification**: Before any deployment, run the comprehensive test suite:\r\n   ```bash\r\n   python tests/comprehensive_test_suite.py\r\n   ```\r\n\r\n7. **Continuous Verification**: Implement automated testing in CI/CD pipelines to catch issues early\r\n\r\n## Implementation Example\r\n\r\n### Running the Comprehensive Test Suite:\r\n```bash\r\n# Basic test run\r\npython tests/comprehensive_test_suite.py\r\n\r\n# With custom parameters\r\npython tests/comprehensive_test_suite.py --url http://localhost:8000 --token sovereign-secret --output report.json\r\n\r\n# Endpoint and syntax verification only\r\npython tests/endpoint_syntax_verification.py\r\n```\r\n\r\n### Expected Test Coverage:\r\n- Model loading: 100% coverage of model files and configurations\r\n- API endpoints: 100% verification of all documented endpoints\r\n- Syntax: 100% verification of critical Python files\r\n- Data pipeline: End-to-end verification of data flow\r\n- System health: Verification of all core services\r\n\r\n## Verification Checklist\r\n- [ ] All test files organized in `tests/` directory\r\n- [ ] Comprehensive test suite covers all major components\r\n- [ ] Endpoint verification tests all critical endpoints\r\n- [ ] Syntax verification tests all critical Python files\r\n- [ ] Tests are documented in `tests/README.md`\r\n- [ ] Test suite runs without errors\r\n- [ ] Test reports are generated and reviewed"
    tokens: 1329
    size: 3668
  - path: specs\standards\30-OPS\013-universal-log-collection.md
    content: "# Standard 013: Universal Log Collection System\r\n\r\n## What Happened?\r\nThe system had fragmented logging across multiple sources (browser console, Python stdout, WebSocket events) making debugging difficult. Users had to check multiple places to understand system behavior.\r\n\r\n## The Cost\r\n- 4+ hours spent debugging connection issues by checking browser console, Python terminal, and WebSocket messages separately\r\n- Inefficient troubleshooting workflow requiring multiple monitoring tools\r\n- Missed error correlations between different system components\r\n- Poor visibility into system-wide operation\r\n\r\n## The Rule\r\n1. **Universal Collection**: All system logs (Python, JavaScript, WebSocket, browser, model loading, GPU status) must be aggregated in a single location: `tools/log-viewer.html`\r\n2. **Broadcast Channel Protocol**: All components must use the `sovereign-logs` or `coda_logs` BroadcastChannel to send messages to the log viewer:\r\n   ```javascript\r\n   // From browser components\r\n   const logChannel = new BroadcastChannel('sovereign-logs');\r\n   logChannel.postMessage({\r\n       source: 'component-name',\r\n       type: 'info|success|error|warning|debug',\r\n       time: new Date().toISOString(),\r\n       msg: 'message content'\r\n   });\r\n   ```\r\n\r\n3. **Python Integration**: Python scripts must send log data via API endpoints that feed into the log viewer\r\n4. **Centralized Access**: The single point of truth for all system diagnostics is `http://localhost:8000/log-viewer.html`\r\n5. **File-based Logging**: Each component must also write to its own log file in the `logs/` directory for persistent storage\r\n6. **Log Truncation**: Individual log files must be truncated to last 1000 lines to prevent disk space issues\r\n7. **GPU Resource Queuing**: All GPU operations must use the queuing system (`/v1/gpu/lock`, `/v1/gpu/unlock`) to prevent resource conflicts\r\n8. **Source Tagging**: All log entries must be clearly tagged with their source for easy identification"
    tokens: 757
    size: 1976
  - path: specs\standards\30-OPS\016-process-management-auto-resurrection.md
    content: "# Standard 016: Process Management and Auto-Resurrection for Browser Engines\r\n\r\n## What Happened?\r\nThe Ghost Engine (headless browser) would sometimes crash or hang, leaving zombie processes that would prevent new browser instances from starting. When the ResurrectionManager tried to launch a new browser, it would fail because the previous process was still holding onto resources like the remote debugging port (9222).\r\n\r\n## The Cost\r\n- 2+ hours spent debugging \"Zombie Process\" risk where browser resurrection would fail\r\n- Multiple failed attempts to restart the Ghost Engine\r\n- System becoming unresponsive when browser processes hung\r\n- Users experiencing \"Total System Failure\" when new browsers couldn't start due to port conflicts\r\n\r\n## The Rule\r\n1. **Process Cleanup First**: Before launching a new browser process, always kill any existing browser processes:\r\n   ```python\r\n   async def kill_existing_browsers(self):\r\n       import psutil\r\n       for proc in psutil.process_iter(['pid', 'name']):\r\n           if proc.info['name'].lower() in ['msedge.exe', 'chrome.exe', 'chromium-browser']:\r\n               proc.kill()\r\n   ```\r\n\r\n2. **Explicit Port Assignment**: Always specify a consistent remote debugging port to avoid conflicts:\r\n   ```python\r\n   # Add to browser launch command\r\n   \"--remote-debugging-port=9222\"\r\n   ```\r\n\r\n3. **Wait for Full Initialization**: Increase wait time after launching browser to ensure full initialization before checking connection:\r\n   ```python\r\n   await asyncio.sleep(5)  # Increased from 3 seconds\r\n   ```\r\n\r\n4. **Proper Process Termination**: Ensure browser processes are fully terminated before launching new ones:\r\n   ```python\r\n   # Always call terminate() then wait() or kill() if needed\r\n   process.terminate()\r\n   process.wait(timeout=5)\r\n   ```\r\n\r\nThis standard ensures that browser resurrection works reliably by preventing port conflicts and zombie processes."
    tokens: 726
    size: 1919
  - path: specs\standards\30-OPS\020-browser-profile-management-cleanup.md
    content: "# Standard 020: Browser Profile Management and Cleanup\r\n\r\n## What Happened?\r\nRepeatedly launching and killing modern browsers (Edge/Chrome) could leave orphaned child processes or fill up the temporary directory with user data profiles. This created a \"Memory Leak\" risk where temporary browser profiles could accumulate over time and crash the host OS.\r\n\r\n## The Cost\r\n- Potential disk space issues from accumulated temporary browser profiles\r\n- Risk of system instability from orphaned processes\r\n- 1 hour spent implementing proper browser profile management and cleanup\r\n\r\n## The Rule\r\n1. **Unique Profile Directories**: Use unique temporary directories for each browser instance:\r\n   ```python\r\n   import tempfile\r\n   temp_dir = tempfile.gettempdir()\r\n   f\"--user-data-dir={temp_dir}/anchor_ghost_{int(time.time())}\"\r\n   ```\r\n\r\n2. **Performance Optimization Flags**: Include performance optimization flags to reduce resource usage:\r\n   ```python\r\n   # Add these flags to browser launch command\r\n   \"--disable-background-timer-throttling\",\r\n   \"--disable-backgrounding-occluded-windows\", \r\n   \"--disable-renderer-backgrounding\",\r\n   \"--disable-ipc-flooding-protection\",\r\n   \"--disable-background-media-suspend\"\r\n   ```\r\n\r\n3. **Cleanup Old Profiles**: Implement automatic cleanup of old temporary profiles:\r\n   ```python\r\n   async def _cleanup_old_profiles(self):\r\n       # Remove directories older than 1 day\r\n       cutoff_time = datetime.now() - timedelta(days=1)\r\n       # Implementation to remove old temporary directories\r\n   ```\r\n\r\n4. **Proper Process Termination**: Ensure browser processes are fully terminated before launching new ones\r\n\r\nThis standard prevents disk space issues and system instability from temporary browser profiles."
    tokens: 645
    size: 1747
  - path: specs\standards\30-OPS\024-detached-logging-standard.md
    content: "# Standard 024: Detached Logging and Process Management for LLM Systems\r\n\r\n## What Happened?\r\nThe system had issues with scripts blocking execution and logs not being properly managed, causing system instability and difficulty in debugging. Scripts were running in attached mode causing blocking operations, and log files were growing without bounds, consuming excessive disk space.\r\n\r\n## The Cost\r\n- System instability due to blocking operations\r\n- Excessive disk space consumption from unbounded log files\r\n- Difficulty in debugging due to missing or improperly managed logs\r\n- Process conflicts and hanging operations\r\n\r\n## The Rule\r\n1. **Detached Mode Execution**: All scripts, especially those related to LLM models, must run in detached mode to prevent blocking operations:\r\n   ```python\r\n   # For Python scripts launched from batch files\r\n   start \"Process Name\" /min cmd /c \"python script.py > logs/script_output.log 2>&1\"\r\n   ```\r\n\r\n2. **Universal Log Output**: All scripts must output logs to the designated logs directory (`logs/`) with proper file naming conventions:\r\n   - Python scripts: `python_stdout.log`, `python_stderr.log`\r\n   - Custom components: `{component_name}.log`\r\n   - Error logs: `{component_name}_error.log`\r\n\r\n3. **Log Truncation**: All log files must implement automatic truncation to prevent excessive disk usage:\r\n   - Truncate after 5000 lines OR 10000 characters, whichever comes first\r\n   - Keep most recent entries when truncating\r\n   - Implement rotation if needed for high-volume logs\r\n\r\n4. **Process Management**: Scripts must properly manage child processes and ensure cleanup:\r\n   - Use proper subprocess management with error handling\r\n   - Implement graceful shutdown procedures\r\n   - Monitor and terminate orphaned processes\r\n\r\n5. **Error Handling**: All scripts must implement proper error handling and logging:\r\n   - Catch exceptions and log them appropriately\r\n   - Use structured logging with timestamps and severity levels\r\n   - Ensure logs are written even during error conditions\r\n\r\n6. **Resource Management**: Scripts must manage system resources efficiently:\r\n   - Close file handles properly\r\n   - Release memory when possible\r\n   - Monitor resource usage and implement limits\r\n\r\nThis standard ensures that all system components run reliably in detached mode while maintaining proper logging practices for debugging and monitoring."
    tokens: 906
    size: 2387
  - path: specs\standards\30-OPS\025-script-logging-protocol.md
    content: "# Standard 025: Script Logging Protocol for LLM Development\r\n\r\n## What Happened?\r\nThe system needed a standardized approach for running scripts that generate data for the context engine. Previously, scripts were run directly in the terminal, making it difficult to track their execution, capture their output, and ensure they could run reliably in production environments. The documentation policy specifies that LLM dev scripts should never run in non-detached mode and should output to log files in the logs/ directory.\r\n\r\n## The Cost\r\n- Difficulty tracking script execution and debugging issues\r\n- Lack of persistent logs for script runs\r\n- Scripts not designed to run in detached mode as required by the architecture\r\n- Inconsistent logging approaches across different scripts\r\n\r\n## The Rule\r\n1. **Detached Mode Only**: All scripts that process data for the context engine must be designed to run in detached mode, not requiring user interaction.\r\n\r\n2. **Log File Output**: All scripts must write their execution logs to files in the `logs/` directory with timestamped names following the pattern `scriptname_YYYYMMDD_HHMMSS.log`.\r\n\r\n3. **Log Format**: Script logs must follow the same format as other system logs:\r\n   ```\r\n   [YYYY-MM-DD HH:MM:SS] [LEVEL] Message content\r\n   ```\r\n   Where LEVEL is one of: INFO, SUCCESS, ERROR, WARNING, DEBUG\r\n\r\n4. **Self-Contained Logging**: Scripts must handle their own logging setup, creating the logs directory if needed and writing to their own log files.\r\n\r\n5. **Progress Tracking**: Long-running scripts should log progress indicators at regular intervals to show they are still active.\r\n\r\n6. **Error Handling**: Scripts must log errors and exceptions to their log files and continue or exit gracefully as appropriate.\r\n\r\n7. **File Path Detection**: Scripts that depend on system executables (like browsers) should implement robust path detection with fallbacks to standard installation locations.\r\n\r\n## Implementation Example:\r\n```python\r\nimport datetime\r\nimport os\r\nfrom pathlib import Path\r\n\r\ndef setup_logging(script_name):\r\n    logs_dir = Path(__file__).parent.parent / \"logs\"\r\n    logs_dir.mkdir(exist_ok=True)\r\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\r\n    log_file = logs_dir / f\"{script_name}_{timestamp}.log\"\r\n    return log_file\r\n\r\ndef log_message(log_file, level, message):\r\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n    log_entry = f\"[{timestamp}] [{level}] {message}\\n\"\r\n    with open(log_file, 'a', encoding='utf-8') as f:\r\n        f.write(log_entry)\r\n\r\ndef detect_executable_path(executable_name, possible_paths):\r\n    \"\"\"\r\n    Detect the path of an executable with fallbacks to standard installation locations.\r\n\r\n    Args:\r\n        executable_name: Name of the executable as fallback (e.g., 'msedge')\r\n        possible_paths: List of possible installation paths to check\r\n\r\n    Returns:\r\n        Path to the executable if found, otherwise the fallback name\r\n    \"\"\"\r\n    for path in possible_paths:\r\n        if os.path.exists(path):\r\n            return path\r\n    return executable_name  # fallback\r\n```\r\n\r\n## Migration Tool Compliance\r\nThe `migrate_history.py` tool follows this standard by:\r\n- Creating timestamped log files in the logs/ directory\r\n- Using the standard log format\r\n- Running in detached mode without requiring user interaction\r\n- Providing detailed progress information during execution\r\n- Handling errors gracefully and continuing operation when possible\r\n- Implementing robust file path detection with multiple fallback options"
    tokens: 1369
    size: 3567
  - path: specs\standards\30-OPS\035-never-attached-mode.md
    content: "# Standard 035: Never Attached Mode for Long-Running Services\r\n\r\n## What Happened?\r\nLong-running services and scripts were being executed in attached mode, causing command-line interfaces to block for extended periods (sometimes hours). This happened during server startup when `npm start` was run directly in the terminal, causing the process to hang and occupy the command line for extended periods.\r\n\r\n## The Cost\r\n- Command-line interfaces blocked for hours preventing other operations\r\n- Resource waste from keeping terminals open unnecessarily\r\n- Poor developer experience with unresponsive command prompts\r\n- Risk of accidental process termination when closing terminals\r\n- Violation of the principle that long-running services should operate independently\r\n\r\n## The Rule\r\n1. **Detached Execution Only**: All long-running services (servers, daemons, watchers) must be started in detached mode using appropriate backgrounding techniques.\r\n\r\n2. **No Attached Mode**: Never run services like `npm start`, `python server.py`, or similar long-running processes directly in an attached terminal session.\r\n\r\n3. **Proper Logging**: All detached processes must log to the designated `logs/` directory for monitoring and debugging.\r\n\r\n4. **Platform-Specific Detaching**:\r\n   - *Linux/Mac:* Use `nohup command > logs/output.log 2>&1 &` or systemd services\r\n   - *Windows:* Use `start /min cmd /c \"command > ..\\logs\\output.log 2>&1\"` or similar backgrounding\r\n   - *Cross-platform:* Use process managers like pm2 or nodemon with background options\r\n\r\n5. **Verification Method**: After starting a service in detached mode, verify it's running by checking logs or attempting to connect to its interface, not by waiting for terminal output.\r\n\r\n6. **Documentation Requirement**: All startup procedures must specify detached execution methods, never attached execution.\r\n\r\n## Implementation\r\n- Updated all documentation to specify detached execution methods\r\n- Created background startup scripts where appropriate\r\n- Added proper error handling and logging to detached processes\r\n- Established monitoring procedures for detached services\r\n- Educated team members on detached vs attached execution differences"
    tokens: 845
    size: 2198
  - path: specs\standards\30-OPS\036-log-file-management-protocol.md
    content: "# Standard 036: Log File Management Protocol\r\n\r\n**Status:** Active | **Category:** Operations (30-OPS) | **Authority:** Human-Locked\r\n\r\n## The Triangle of Pain\r\n\r\n### 1. What Happened\r\nLLM agents and developers were unable to monitor long-running processes effectively because output was directed to terminal sessions that became unresponsive. Without proper log file management, debugging and monitoring of detached processes became impossible, leading to system state uncertainty.\r\n\r\n### 2. The Cost\r\n- **Inability to Monitor:** No visibility into detached process status\r\n- **Debugging Difficulty:** Impossible to troubleshoot stuck processes\r\n- **Resource Management:** Unclear which processes were running or failing\r\n- **System State Confusion:** No clear way to verify process completion or errors\r\n\r\n### 3. The Rule\r\n**All detached processes MUST write to specific log files in the `logs/` directory with proper naming conventions and rotation.**\r\n\r\n#### Specific Requirements:\r\n- **Centralized Logging:** All process output goes to `logs/` directory at project root\r\n- **Descriptive Naming:** Log files named after the process with optional timestamp: `process_name.log`\r\n- **Format Consistency:** All logs must be human and machine readable (text format)\r\n- **Size Management:** Implement log rotation or truncation to prevent infinite growth\r\n- **Access Path:** Standard path `logs/process_name.log` for all processes\r\n\r\n#### Examples:\r\n```bash\r\n# CORRECT: Proper logging\r\nnode server.js > logs/server.log 2>&1 &\r\npython data_process.py > logs/data_process.log 2>&1 &\r\n\r\n# Log file structure\r\nlogs/\r\n server.log\r\n data_process.log\r\n context_read.log\r\n backup_operation.log\r\n```\r\n\r\n#### Verification:\r\n- Log files must be created before process execution\r\n- Log files must be accessible and writable\r\n- Log content must reflect actual process output\r\n- Log files must be monitored instead of terminal sessions\r\n\r\n## Cross-References\r\n- Standard 035: Script Running Protocol - Detached Execution with Logging\r\n- Related to: Standard 013 (Universal Logging)\r\n\r\n## Implementation\r\n- Created `logs/` directory in project root\r\n- All startup scripts now direct output to log files\r\n- Log files are monitored for process status verification\r\n- Documentation updated to reference log file checking\r\n\r\n---\r\n*Verified by Architecture Council. Edited by Humans Only.*"
    tokens: 900
    size: 2403
  - path: specs\standards\README.md
    content: "# The Sovereign Engineering Code (SEC)\r\n\r\nThis is the authoritative reference manual for the External Context Engine (ECE) project. Standards are organized by domain to facilitate navigation and understanding.\r\n\r\n## Domain 00: CORE (Philosophy & Invariants)\r\nPhilosophy, Privacy, and \"Local-First\" invariants that govern the fundamental principles of the system.\r\n\r\n### Standards:\r\n- [012-context-utility-manifest.md](012-context-utility-manifest.md) - Context utility manifest and philosophical foundations\r\n- [027-no-resurrection-mode.md](027-no-resurrection-mode.md) - Manual control via NO_RESURRECTION_MODE flag\r\n- [028-default-no-resurrection-mode.md](028-default-no-resurrection-mode.md) - Default behavior for Ghost Engine resurrection\r\n\r\n## Domain 10: ARCH (System Architecture)\r\nNode.js Monolith, CozoDB, Termux, Hardware limits, and system architecture decisions.\r\n\r\n### Standards:\r\n- [003-webgpu-initialization-stability.md](003-webgpu-initialization-stability.md) - WebGPU initialization stability\r\n- [004-wasm-memory-management.md](004-wasm-memory-management.md) - WASM memory management\r\n- [014-async-best-practices.md](014-async-await-best-practices.md) - Async/await patterns for system integration\r\n- [023-anchor-lite-simplification.md](023-anchor-lite-simplification.md) - Anchor Lite architectural simplification\r\n- [031-ghost-engine-stability-fix.md](031-ghost-engine-stability-fix.md) - CozoDB schema FTS failure handling\r\n- [032-ghost-engine-initialization-flow.md](032-ghost-engine-initialization-flow.md) - Database initialization race condition prevention\r\n- [034-nodejs-monolith-migration.md](034-nodejs-monolith-migration.md) - Migration to Node.js monolith architecture\r\n- [048-epochal-historian-recursive-decomposition.md](048-epochal-historian-recursive-decomposition.md) - Epochal Historian & Recursive Decomposition\r\n- [051-service-module-path-resolution.md](051-service-module-path-resolution.md) - Service Module Path Resolution\r\n- [057-enterprise-library-architecture.md](057-enterprise-library-architecture.md) - Enterprise Library Architecture (Logical Notebooks/Cartridges)\r\n- [060-worker-system.md](060-worker-system.md) - High-performance worker architecture\r\n\r\n## Domain 20: DATA (Data, Memory, Filesystem)\r\nSource of Truth, File Ingestion, Schemas, YAML Snapshots, and all data-related concerns.\r\n\r\n### Standards:\r\n- [017-file-ingestion-debounce-hash-checking.md](017-file-ingestion-debounce-hash-checking.md) - File ingestion with debouncing and hash checking\r\n- [019-code-file-ingestion-comprehensive-context.md](019-code-file-ingestion-comprehensive-context.md) - Comprehensive context for code file ingestion\r\n- [021-chat-session-persistence-context-continuity.md](021-chat-session-persistence-context-continuity.md) - Chat session persistence and context continuity\r\n- [022-text-file-source-of-truth-cross-machine-sync.md](022-text-file-source-of-truth-cross-machine-sync.md) - Text files as source of truth\r\n- [024-context-ingestion-pipeline-fix.md](024-context-ingestion-pipeline-fix.md) - Context ingestion pipeline fixes\r\n- [029-consolidated-data-aggregation.md](029-consolidated-data-aggregation.md) - Consolidated data aggregation approach\r\n- [030-multi-format-output.md](030-multi-format-output.md) - JSON, YAML, and text output support\r\n- [033-cozodb-syntax-compliance.md](033-cozodb-syntax-compliance.md) - CozoDB syntax compliance requirements\r\n- [037-database-hydration-snapshot-portability.md](037-database-hydration-snapshot-portability.md) - Database hydration workflow\r\n- [052-schema-evolution-epochal-classification.md](052-schema-evolution-epochal-classification.md) - Schema Evolution & Epochal Classification\r\n- [053-cozodb-pain-points-reference.md](053-cozodb-pain-points-reference.md) - ** CRITICAL**: CozoDB pain points and gotchas\r\n- [059-reliable-ingestion.md](059-reliable-ingestion.md) - Ghost Data Protocol for reliable ingestion\r\n- [061-context-logic.md](061-context-logic.md) - Advanced context window logic\r\n- [063-cozo-db-syntax.md](063-cozo-db-syntax.md) - CozoDB syntax reference\r\n- [064-cozodb-query-stability.md](064-cozodb-query-stability.md) - Query stability and error handling\r\n- [065-graph-associative-retrieval.md](065-graph-associative-retrieval.md) - Tag-Walker: Bridge & Walk phases\r\n- [066-human-readable-mirror.md](066-human-readable-mirror.md) - Filesystem projection of graph data\r\n- [067-cozodb-query-sanitization.md](067-cozodb-query-sanitization.md) - Preventing injection and syntax errors\r\n- [068-tag-infection-protocol.md](068-tag-infection-protocol.md) - Weak Supervision & Tag Infection\r\n- [069-intelligent-query-expansion.md](069-intelligent-query-expansion.md) - Semantic intent translation\r\n- [070-local-discovery.md](070-local-discovery.md) - Local Discovery & NER Standardization\r\n\r\n## Domain 30: OPS (Protocols, Safety, Debugging)\r\nAgent Safety (Protocol 001), Logging, Async handling, and operational procedures.\r\n\r\n### Standards:\r\n- [001-windows-console-encoding.md](001-windows-console-encoding.md) - Windows console encoding handling\r\n- [011-comprehensive-testing-verification.md](011-comprehensive-testing-verification.md) - Testing and verification\r\n- [013-universal-log-collection.md](013-universal-log-collection.md) - Universal log collection system\r\n- [016-process-management-auto-resurrection.md](016-process-management-auto-resurrection.md) - Process management\r\n- [020-browser-profile-management-cleanup.md](020-browser-profile-management-cleanup.md) - Browser profile management\r\n- [024-detached-logging-standard.md](024-detached-logging-standard.md) - Detached execution with logging\r\n- [025-script-logging-protocol.md](025-script-logging-protocol.md) - Script logging protocol (Protocol 001)\r\n- [035-never-attached-mode.md](035-never-attached-mode.md) - Never run services in attached mode\r\n- [036-log-file-management-protocol.md](036-log-file-management-protocol.md) - Log file management and rotation\r\n- [050-windows-background-process-behavior.md](050-windows-background-process-behavior.md) - Windows background process behavior\r\n- [062-inference-stability.md](062-inference-stability.md) - CUDA error handling and model stability\r\n\r\n## Domain 40: BRIDGE (APIs, Extensions, UI)\r\nExtensions, Ports, APIs, and all interface-related concerns.\r\n\r\n### Standards:\r\n- [010-bridge-redirect-implementation.md](010-bridge-redirect-implementation.md) - Bridge redirect implementation\r\n- [015-browser-control-center.md](015-browser-control-center.md) - Unified browser control center\r\n- [018-streaming-cli-client-responsive-ux.md](018-streaming-cli-client-responsive-ux.md) - Responsive UX for streaming CLI clients\r\n- [026-ghost-engine-connection-management.md](026-ghost-engine-connection-management.md) - Ghost Engine connection management\r\n- [058-universal-rag-api.md](058-universal-rag-api.md) - Standard Unified RAG Endpoint\r\n"
    tokens: 2619
    size: 6854
  - path: specs\tasks.md
    content: "# Context-Engine Implementation Tasks\r\n\r\n## Current Work Queue\r\n\r\n## Active Sprint: Sovereign Desktop & Robustness (Jan 10, 2026)\r\n\r\n### Phase 20: Tag-Walker & Mirror 2.0 (Completed )\r\n- [x] **Tag-Walker Protocol**: Replaced Vector Search with 3-phase graph retrieval (FTS -> Pivot -> Walk).\r\n- [x] **Mirror 2.0**: Implemented semantic filesystem projection with `@bucket/#tag` layout.\r\n- [x] **Cleanup logic**: Added wipe-on-sync to `mirrored_brain` to prevent stale data.\r\n- [x] **FTS Hardening**: Implemented `sanitizeFtsQuery` to prevent parser crashes. (Standard 066)\r\n- [x] **Licensing Alignment**: Updated project to Elastic License 2.0.\r\n\r\n###  Critical (Immediate)\r\n- [x] **Fix \"JSON Vomit\" (Session Pollution):** Implement Side-Channel Separation for Intent Translation. (Standard 055)\r\n- [x] **Fix Search Crash:** Handle `null` returns from Intent Translation in `api.js`.\r\n- [x] **Fix \"No Sequences Left\":** Explicitly dispose Side-Channel sessions and increase sequence limit.\r\n- [x] **Sovereign Desktop UI:** Implement \"Frosted Glass\" transparent overlay. (Standard 056)\r\n- [x] **Vision Integration:** detailed screen capture via `desktopCapturer` in the Overlay.\r\n- [x] **Refactor Inference Monolith:** Deconstruct `inference.js` into modular TypeScript services (`provider.ts`, `context.ts`, `inference.ts`).\r\n- [x] **Magic Inbox:** Implement \"Drop-Zone\" pattern in `watcher.ts` (Watch -> Ingest -> Archive).\r\n- [x] **Hybrid Module Stability:** Revert to CJS with Dynamic Imports for robust ESM compatibility.\r\n\r\n###  High Priority (This Week)\r\n- [ ] **Backend Vision Pipeline:** Ensure `inference.js` correctly handles the `{type: image_url}` message format via `node-llama-cpp`.\r\n- [ ] **Context Assembly Speed:** Investigate caching strategies for repeated Large Contexts.\r\n- [ ] **Dreamer Upgrade:** Enable \"Deep Sleep\" logic for aggressive deduplication.\r\n\r\n###  Backlog (Feature Requests)\r\n- [ ] **Voice Input:** Whisper integration for the Desktop Overlay.\r\n- [ ] **Codebase Map:** Visual graph of the `context/` directory.\r\n- [ ] **MCP Server:** Expose ECE as a Model Context Protocol server.\r\n\r\n### Phase 17: Enterprise Library Architecture (In Progress)\r\n- [x] **Context Cartridges UI:** Implemented \"Loadout\" buttons in `index.html` (Architect/Python/Whitepaper).\r\n- [x] **Logical Notebooks:** Updated `context_packer.js` to treat `context/libraries/` as auto-tagged cartridges.\r\n- [x] **Watcher Upgrades:** Updated `watcher.js` to detect Library folders and apply `#{lib}_docs` buckets.\r\n- [x] **Stability Fix:** Patched `inference.js` (Sequences: 15) to prevent VRAM exhaustion with concurrent Dreamer/Search.\r\n- [x] **Whitepaper Context:** Injected `specs/` into the graph as a dedicated `specs` bucket.\r\n- [ ] **Dynamic Loadouts:** Move Loadout config from `sovereign.yaml` (Updated from index.html).\r\n- [ ] **Docs Update:** Create `README_LIBRARIES.md` explaining how to add new cartridges.\r\n\r\n### Phase 19: Enterprise & Advanced RAG (Planned)\r\n- [ ] **Feature 7: Backup & Restore**: Server-side DB dumps (`POST /v1/backup`) and Restore-on-Boot logic.\r\n- [ ] **Feature 8: Rolling Context Slicer**: Middle-Out context budgeting for `ContextManager` (Relevance vs Recency).\r\n- [ ] **Feature 9: Live Context Visualizer**: \"RAG IDE\" in Frontend with real-time budget slider and atom visualization.\r\n- [ ] **Feature 10: Sovereign Provenance**: Trust hierarchy (Sovereign vs External) with bias toggle in Search.\r\n\r\n### Phase 18: Monorepo & Configuration Unification (Active)\r\n- [x] **PNPM Migration:** Converted project to `pnpm` workspace (packages: engine, desktop-overlay, shared).\r\n- [x] **Shared Types:** Created `@ece/shared` for unified TypeScript interfaces.\r\n- [x] **Unified Config:** Implemented `sovereign.yaml` as Single Source of Truth for Models, UI, and Network.\r\n- [x] **Lifecycle Management:** Electron Main now automatically spawns/kills the Engine process.\r\n- [x] **Settings UI:** Added `Settings.tsx` overlay with IPC read/write to `sovereign.yaml`.\r\n- [ ] **Security Hardening:** Migrate IPC to `contextBridge` / `preload.js` (disable `nodeIntegration`).\r\n\r\n### Phase 16: Brain Link & Sovereign Desktop (Done)\r\n- [x] **Schema Introspection Fix**: Use `::columns memory` instead of broken `*columns{...}` query (Standard 053)\r\n- [x] **FTS Persistence**: FTS index now survives restarts (no more migration loop)\r\n- [x] **Brain Link UI**: Auto-context injection in `chat.html` with memory budget slider\r\n- [x] **Personal Memory Ingestion**: Created `add_personal_memories.js` for test data\r\n- [x] **Planning Document**: Created `specs/sovereign-desktop-app.md` with full architecture\r\n- [x] **Chat UI Overhaul**: Simplified chat.html - removed Brain Link (unreliable local), kept Manual Context\r\n- [x] **Streaming Tokens**: Real-time token streaming display as LLM generates response\r\n- [x] **Thinking/Answer Separation**: Model `<think>` blocks displayed separately with purple styling\r\n- [x] **User Message Fix**: User prompts now persist correctly in chat history\r\n\r\n### Phase 12: Production Polish (Completed)\r\n- [x] **Post-Migration Safety**: Implement emergency backups before schema changes (`db.js`).\r\n- [x] **API Fortification**: Add input validation for `ingest` and `search` endpoints (`api.js`).\r\n- [x] **Search Resiliency**: Fix bucket-filtering bypass in `executeSearch`.\r\n- [x] **Verification Suite**: 100% pass rate on `npm test`.\r\n- [x] **Chat Cockpit Enhancement**: Add conversation history persistence to `chat.html`\r\n- [x] **Streaming Responses**: Implement SSE for real-time token streaming\r\n- [x] **One-Click Install**: Create `setup.ps1` / `setup.sh` scripts\r\n\r\n### Phase 11: Markovian Reasoning Engine (Completed)\r\n- [x] **Scribe Service**: Created `engine/src/services/scribe.js` for rolling state\r\n- [x] **Context Weaving**: Upgraded `inference.js` to auto-inject session state\r\n- [x] **Test Suite**: Created `engine/tests/suite.js` for API verification\r\n- [x] **Benchmark Tool**: Created `engine/tests/benchmark.js` for accuracy testing\r\n- [x] **Config Fixes**: Externalized MODELS_DIR, fixed package.json typo\r\n- [x] **API Endpoints**: Added `/v1/scribe/*` and `/v1/inference/status`\r\n- [x] **Standard 041**: Documented Markovian architecture\r\n\r\n### Phase 13: Epochal Historian & Mirror Protocol Enhancement (Completed)\r\n- [x] **Epochal Historian Implementation**: Implement recursive decomposition (Epochs -> Episodes -> Propositions) in Dreamer service\r\n- [x] **Mirror Protocol Enhancement**: Update to prioritize Epoch-based structure in `context/mirrored_brain/[Bucket]/[Epoch]/[Memory_ID].md`\r\n- [x] **Documentation Updates**: Update `specs/spec.md`, `specs/search_patterns.md`, and `specs/context_assembly_findings.md` with Epochal Historian details\r\n- [x] **Watcher Shield**: Ensure file watcher ignores `context/mirrored_brain/` to prevent recursive loops\r\n\r\n### Phase 14: Path Resolution Fixes (Completed)\r\n- [x] **Service Module Path Corrections**: Fix relative import paths in all service files (search, ingest, scribe, dreamer, mirror, inference, watcher, safe-shell-executor)\r\n- [x] **Core Module References**: Correct paths from `'../core/db'` to `'../../core/db'` in services located in subdirectories\r\n- [x] **Configuration Imports**: Standardize all relative imports to properly reference core modules and configuration files\r\n- [x] **Module Loading Verification**: Verify all modules load without \"Cannot find module\" errors\r\n\r\n### Phase 15: Schema Evolution & Epochal Historian Enhancement (Completed)\r\n- [x] **Database Schema Update**: Add `epochs: String` field to memory table schema to store epochal classifications\r\n- [x] **Dreamer Service Update**: Modify database queries and updates to include epochs field in processing\r\n- [x] **Search Service Update**: Modify database queries to include epochs field in search operations\r\n- [x] **Mirror Service Update**: Ensure epochs field is properly handled in mirroring operations\r\n- [x] **Documentation Updates**: Update `specs/spec.md`, `specs/search_patterns.md`, and `specs/context_assembly_findings.md` with schema changes\r\n\r\n### Phase 16: Brain Link & Sovereign Desktop (In Progress)\r\n- [x] **Schema Introspection Fix**: Use `::columns memory` instead of broken `*columns{...}` query (Standard 053)\r\n- [x] **FTS Persistence**: FTS index now survives restarts (no more migration loop)\r\n- [x] **Brain Link UI**: Auto-context injection in `chat.html` with memory budget slider\r\n- [x] **Personal Memory Ingestion**: Created `add_personal_memories.js` for test data\r\n- [x] **Planning Document**: Created `specs/sovereign-desktop-app.md` with full architecture\r\n- [x] **Chat UI Overhaul**: Simplified chat.html - removed Brain Link (unreliable local), kept Manual Context\r\n- [x] **Streaming Tokens**: Real-time token streaming display as LLM generates response\r\n- [x] **Thinking/Answer Separation**: Model `<think>` blocks displayed separately with purple styling\r\n- [x] **User Message Fix**: User prompts now persist correctly in chat history\r\n- [ ] **Sovereign Desktop Prototype**: Electron overlay with hotkey activation\r\n- [ ] **Screen Capture Integration**: Add VL model for screen understanding\r\n- [ ] **Proactive Memory**: Auto-ingest screen context and conversation highlights\r\n- [ ] **Distribution**: Installer, auto-update, first-run wizard\r\n\r\n### Phase 10: Cortex Upgrade (Completed)\r\n- [x] **Multi-Bucket Schema**: Migrate from single `bucket` to `buckets: [String]` (Standard 039).\r\n- [x] **Dreamer Service**: Implement background self-organization via local LLM.\r\n- [x] **Cozo Hardening**: Resolve list-handling and `unnest` syntax errors (Standard 040).\r\n- [x] **ESM Interop**: Fix dynamic import issues for native modules in CJS.\r\n\r\n- [x] **Cross-Machine Sync**: Enable file sync via Dropbox/Git for multi-device access\r\n- [x] **Infinite Loop**: Create feedback loop: Chat -> File -> Ingestion -> Memory -> Next Chat\r\n- [x] **Timestamped Entries**: Format messages with timestamps for better tracking\r\n- [x] **Session Tracking**: Add session file path display in CLI startup\r\n\r\n### Completed - Root Refactor \r\n- [x] **Kernel**: Implement `tools/modules/sovereign.js`.\r\n- [x] **Mic**: Refactor `root-mic.html` to use Kernel.\r\n- [x] **Builder**: Refactor `sovereign-db-builder.html` to use Kernel.\r\n- [x] **Console**: Refactor `model-server-chat.html` to use Kernel (Graph-R1).\r\n- [x] **Docs**: Update all specs to reflect Root Architecture.\r\n\r\n### Completed - Hardware Optimization \r\n- [x] **WebGPU Buffer Optimization**: Implemented 256MB override for Adreno GPUs.\r\n- [x] **Model Profiles**: Added Lite, Mid, High, Ultra profiles.\r\n- [x] **Crash Prevention**: Context clamping for constrained drivers.\r\n- [x] **Mobile Optimization**: Service Worker (`llm-worker.js`) for non-blocking inference.\r\n- [x] **Consciousness Semaphore**: Implemented resource arbitration in `sovereign.js`.\r\n\r\n### Completed - The Subconscious \r\n- [x] **Root Dreamer**: Created `tools/root-dreamer.html` for background memory consolidation.\r\n- [x] **Ingestion Refinement**: Upgraded `read_all.py` to produce LLM-legible YAML.\r\n- [x] **Root Architecture Docs**: Finalized terminology (Sovereign -> Root).\r\n- [x] **Memory Hygiene**: Implemented \"Forgetting Curve\" in `root-dreamer.html`.\r\n\r\n### Completed - Active Cognition \r\n- [x] **Memory Writing**: Implement `saveTurn` to persist chat to CozoDB.\r\n- [x] **User Control**: Add \"Auto-Save\" toggle to System Controls.\r\n- [x] **Temporal Grounding**: Inject System Time into `buildVirtualPrompt`.\r\n- [x] **Multimodal**: Add Drag-and-Drop Image support to Console.\r\n\r\n### Phase 4.1: The Neural Shell (Completed) \r\n**Objective:** Decouple Intelligence (Chat) from Agency (Terminal).\r\n- [x] **Phase 1:** \"Stealth Mode\" Cache Bypass (Completed).\r\n- [x] **Phase 2:** Headless Browser Script (`launch-ghost.ps1`) (Completed).\r\n- [x] **Phase 3:** `sov.py` Native Client Implementation.\r\n- [x] **Phase 3.5:** Ghost Auto-Ignition (Headless auto-start with ?headless=true flag).\r\n- [x] **Phase 4:** Migration to C++ Native Runtime (Removing Chrome entirely).\r\n- [x] **Bridge Repair**: Debug and stabilize `extension-bridge` connectivity.\r\n- [x] **Neural Shell Protocol**: Implement `/v1/shell/exec` in `webgpu_bridge.py`.\r\n- [x] **The \"Coder\" Model**: Add `Qwen2.5-Coder-1.5B` to Model Registry.\r\n- [x] **Terminal UI**: Create `tools/neural-terminal.html` for natural language command execution.\r\n\r\n### Phase 4.2: Agentic Expansion (Deferred)\r\n- [ ] **Agentic Tools**: Port Verifier/Distiller logic to `tools/modules/agents.js`.\r\n- [ ] **Voice Output**: Add TTS to Console.\r\n\r\n## Phase 5: The Specialist Array\r\n- [ ] **Dataset Generation**: Samsung TRM / Distillation.\r\n- [ ] **Unsloth Training Pipeline**: RTX 4090 based fine-tuning.\r\n- [ ] **Model Merging**: FrankenMoE construction.\r\n\r\n## Phase 6: GPU Resource Management (Completed)\r\n- [x] **GPU Queuing System**: Implement `/v1/gpu/lock`, `/v1/gpu/unlock`, and `/v1/gpu/status` endpoints with automatic queuing\r\n- [x] **Resource Conflict Resolution**: Eliminate GPU lock conflicts with proper queue management\r\n- [x] **503 Error Resolution**: Fix \"Service Unavailable\" errors by implementing proper resource queuing\r\n- [x] **Sidecar Integration**: Add GPU status monitoring to sidecar interface\r\n- [x] **Log Integration**: Add GPU resource management logs to centralized logging system\r\n- [x] **Documentation**: Update specs and standards to reflect GPU queuing system\r\n\r\n## Phase 7: Async/Await Best Practices (Completed)\r\n- [x] **Coroutine Fixes**: Resolve \"coroutine was never awaited\" warnings in webgpu_bridge.py\r\n- [x] **Event Loop Integration**: Properly integrate async functions with FastAPI's event loop\r\n- [x] **Startup Sequence**: Ensure logging system initializes properly with application lifecycle\r\n- [x] **Resource Management**: Fix resource cleanup in WebSocket handlers to prevent leaks\r\n- [x] **Error Handling**: Enhance async error handling with proper cleanup procedures\r\n- [x] **Documentation**: Create Standard 014 for async/await best practices\r\n\r\n## Phase 8: Browser-Based Control Center (Completed)\r\n- [x] **Sidecar UI**: Implement `tools/sidecar.html` with dual tabs for retrieval and vision\r\n- [x] **Context UI**: Implement `tools/context.html` for manual context retrieval\r\n- [x] **Vision Engine**: Create `tools/vision_engine.py` for Python-powered image analysis\r\n- [x] **Bridge Integration**: Update `webgpu_bridge.py` to serve UI and handle vision endpoints\r\n- [x] **Endpoint Implementation**: Add `/v1/vision/ingest`, `/v1/memory/search`, `/logs/recent` endpoints\r\n- [x] **File-based Logging**: Implement persistent logging to `logs/` directory with truncation\r\n- [x] **Documentation**: Update specs and standards to reflect new architecture\r\n\r\n### Phase 9: Anchor Lite Refactor (Completed)\r\n- [x] **Consolidation**: Simplified system to Single Source of Truth (`context/`) -> Single Index (CozoDB) -> Single UI (`context.html`).\r\n- [x] **Cleanup**: Archived unused tools (`db_builder`, `memory-builder`, `sidecar`, `mobile-chat`).\r\n- [x] **Engine Refactor**: Created headless `ghost.html` engine with WebSocket bridge.\r\n- [x] **Launch Logic**: Unified startup in `start-anchor.bat` and `webgpu_bridge.py`.\r\n- [x] **Standard 023**: Documented \"Anchor Lite\" architecture and \"Triangle of Pain\".\r\n\r\n### Phase 10: Context Ingestion Pipeline Fixes (Completed)\r\n- [x] **Field Name Alignment**: Fixed mismatch between watchdog payload (`filetype`) and endpoint expectation (`file_type`)\r\n- [x] **Source Identification**: Updated watchdog to send proper source identifiers instead of \"unknown\"\r\n- [x] **Ghost Engine Update**: Fixed handleIngest function to use correct field names (`msg.file_type`)\r\n- [x] **Error Handling**: Enhanced error reporting in watchdog for better debugging\r\n- [x] **Database Initialization**: Verified CozoDB schema creation and memory table initialization\r\n- [x] **Ingestion Verification**: Confirmed context files are properly ingested and searchable\r\n\r\n### Phase 11: Script Running Protocol Implementation (Completed)\r\n- [x] **Protocol Creation**: Created `SCRIPT_PROTOCOL.md` with guidelines to prevent getting stuck in long-running loops\r\n- [x] **System Optimization**: Fixed database paths and search queries for better performance\r\n- [x] **Documentation Update**: Updated doc_policy to include protocol as allowed root document\r\n- [x] **Standards Creation**: Created Standards 035 and 036 for detached execution and log management\r\n- [x] **Startup Scripts**: Created proper detached startup scripts with logging\r\n\r\n## Backlog\r\n- [ ] **Federation Protocol**: P2P sync.\r\n- [ ] **Android App**: Wrapper for Root Coda."
    tokens: 6243
    size: 16630
  - path: specs\TROUBLESHOOTING.md
    content: "# Forensic Restoration & Annotated Cleaning\r\n\r\n## Objectives\r\n- Preserve raw data (`m.content`) for forensic and retrieval purposes.\r\n- Use `content_cleaned` for indexing and linking to avoid spam/garbage embedding.\r\n- Annotate sanitized technical context rather than removing it entirely (e.g., replacing ANSI codes with `[Context: Terminal Output]`).\r\n- Identify and quarantine `token-soup` nodes to avoid using them for embeddings or graph repairs.\r\n- Provide a regeneration path that normalizes and re-distills quarantined nodes.\r\n\r\n## Key Tools & Functions\r\n- `src/content_utils.normalize_technical_content(text)`\r\n  - Detects and annotates: ANSI, Unix/Windows paths, hex dumps;\r\n  - Produces a normalized text with semantic annotations instead of opaque noise.\r\n\r\n- `src/content_utils.clean_content(text, annotate_technical=False)`\r\n  - A conservative text cleaner; when `annotate_technical=True` it runs normalization first to preserve context tags.\r\n\r\n- `src/distiller_impl.Distiller.distill_moment`\r\n  - Integrates resilience logic: If the distiller detects token-soup, it attempts `normalize_technical_content()` and retries distillation prior to fallback sanitization.\r\n\r\n- Quarantine scripts\r\n  - `scripts/quarantine_token_soup.py`  Scans and optionally tags nodes as `#corrupted`.\r\n  - `scripts/quarantine_regenerate.py`  For quarantined nodes: normalizes raw content, redistills it, and optionally writes `content_cleaned`; it can also replace the `#corrupted` tag with `regenerated`.\r\n\r\n- Repair & Weaver\r\n  - `scripts/neo4j/repair/repair_missing_links_similarity_embeddings.py`  now supports `--exclude-tag` to skip quarantined nodes.\r\n  - `src/maintenance/weaver.py`  now passes `weaver_exclude_tag` to `run_repair` by default.\r\n\r\n## Typical Workflows\r\n\r\n1. Dry-run: identify quarantined nodes\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_token_soup.py --category summary --limit 500 --csv-out logs/token_soup_report.csv --sample 10 --use-cleaned\r\n```\r\n\r\n2. Tag quarantined nodes (write mode)\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_token_soup.py --category summary --limit 500 --write --csv-out logs/token_soup_report.csv --sample 10 --use-cleaned\r\n```\r\n\r\n3. Re-generate summaries for quarantined nodes\r\n\r\n```pwsh\r\npython .\\scripts\\quarantine_regenerate.py --tag '#corrupted' --limit 200 --csv-out logs/regenerate_report.csv --write\r\n```\r\n\r\n4. Run the weaver (repair) excluding corrupted nodes\r\n\r\n```pwsh\r\npython .\\scripts\\neo4j\\repair\\repair_missing_links_similarity_embeddings.py --dry-run --csv-out logs/weaver_review_chunked.csv --exclude-tag '#corrupted' --limit 200 --candidate-limit 100 --top-n 3 --export-top 25\r\n```\r\n\r\n## Rollback & Auditability\r\n- CSV logs are produced for every step to review proposed repairs and regeneration results.\r\n- Relationships created by the Weaver include `r.auto_commit_*` fields enabling rollback via existing scripts.\r\n\r\n## Future Directions\r\n- Integrate regeneration automatically within the Distiller or as a scheduled job under the Archivist.\r\n- Add a quarantine UI or a triage CLI to quickly inspect and approve re-distilled nodes.\r\n- Improve chunk-weighted averaging for embeddings and add more E2E tests for the regeneration process.\r\n\r\n## Summary\r\nThis design preserves both the raw, forensic truth and the usable, sanitized indexable text. We now have a robust path to identify token-soup failures, protect the graph's signal, and reprocess nodes to recover valid summaries with contextual tags.\r\n"
    tokens: 1296
    size: 3472
  - path: specs\vscode_integration.md
    content: "# VSCode Integration\r\n\r\n## Configure VSCode (example for 'Custom OpenAI endpoint')\r\n- Open `Settings`  `Extensions`  `Chat` or the settings for the Chat provider you use\r\n- Add a custom endpoint with URL: `http://localhost:8000/v1/chat/completions`\r\n- Model: `ece-core`\r\n- If API key is required, set a secret with key `Authorization` value `Bearer <API_KEY>` for the provider\r\n- Set `stream` to `true` where the provider supports it\r\n\r\n## Quick test with curl\r\n\r\n### Normal (non-streaming)\r\n```powershell\r\n$body = @{\r\n    model = 'ece-core'\r\n    messages = @(\r\n        @{ role = 'system'; content = 'You are a helpful assistant for VSCode.' },\r\n        @{ role = 'user'; content = 'List the top-level files in the repository' }\r\n    )\r\n} | ConvertTo-Json -Depth 4\r\n\r\nInvoke-RestMethod -Method Post -Uri 'http://localhost:8000/v1/chat/completions' -Body $body -ContentType 'application/json' -Headers @{ Authorization = 'Bearer <API_KEY_HERE>' }\r\n```\r\n\r\n### Streaming (SSE)\r\n```powershell\r\n$body = @{\r\n    model = 'ece-core'\r\n    messages = @(\r\n        @{ role = 'system'; content = 'You are a helpful assistant for VSCode.' },\r\n        @{ role = 'user'; content = 'Summarize the repository' }\r\n    )\r\n    stream = $true\r\n} | ConvertTo-Json -Depth 4\r\n\r\n# Using curl you can receive SSE chunks as they arrive:\r\ncurl -N -H \"Authorization: Bearer <API_KEY_HERE>\" -H \"Content-Type: application/json\" -X POST \"http://localhost:8000/v1/chat/completions\" -d $body\r\n```\r\n"
    tokens: 549
    size: 1469
  - path: tools\list_tags.ts
    content: "\r\nimport { db } from '../engine/src/core/db.js';\r\nimport { config } from '../engine/src/config/index.js';\r\n\r\nasync function listTags() {\r\n    try {\r\n        console.log('Initializing DB...');\r\n        await db.init();\r\n\r\n        console.log('Querying tags...');\r\n        // Query to get all tags from all atoms\r\n        // traversing the list in 'tags' column\r\n        const query = `\r\n            ?[tag] := *memory{tags}, tag_item in tags, tag = tag_item\r\n            :distinct tag\r\n        `;\r\n\r\n        const result = await db.run(query);\r\n\r\n        if (!result.rows || result.rows.length === 0) {\r\n            console.log('No tags found in the database.');\r\n        } else {\r\n            console.log(`Found ${result.rows.length} unique tags:`);\r\n            console.log('----------------------------------------');\r\n            result.rows.forEach(row => {\r\n                console.log(`- ${row[0]}`);\r\n            });\r\n            console.log('----------------------------------------');\r\n        }\r\n\r\n        // Also count atoms per tag\r\n        const countQuery = `\r\n            ?[tag, count(id)] := *memory{id, tags}, tag in tags\r\n        `;\r\n        const countResult = await db.run(countQuery);\r\n        if (countResult.rows && countResult.rows.length > 0) {\r\n            console.log('\\nTag Counts:');\r\n            console.log('----------------------------------------');\r\n            countResult.rows.forEach(row => {\r\n                console.log(`${row[0]}: ${row[1]}`);\r\n            });\r\n        }\r\n\r\n\r\n    } catch (e) {\r\n        console.error('Error listing tags:', e);\r\n    }\r\n}\r\n\r\nlistTags();\r\n"
    tokens: 556
    size: 1610
metadata:
  total_files: 176
  total_tokens: 221047
  token_limit: 1000000
  token_limit_reached: false
  timestamp: "2026-01-23T17:40:24.360Z"
  root_directory: C:\Users\rsbiiw\Projects\ECE_Core
  config:
    tokenLimit: 1000000
    maxFileSize: 5242880
    maxLinesPerFile: 5000
    outputDir: codebase
    outputFile: combined_context.yaml
    includeExtensions:
      - .js
      - .ts
      - .jsx
      - .tsx
      - .py
      - .java
      - .cpp
      - .c
      - .h
      - .cs
      - .go
      - .rs
      - .rb
      - .php
      - .html
      - .css
      - .scss
      - .sass
      - .less
      - .json
      - .yaml
      - .yml
      - .xml
      - .sql
      - .sh
      - .bash
      - .zsh
      - .md
      - .txt
      - .csv
      - .toml
      - .ini
      - .cfg
      - .conf
      - .env
      - .dockerfile
      - dockerfile
      - .gitignore
      - .npmignore
      - .prettierignore
      - makefile
      - cmakelists.txt
      - readme.md
      - readme.txt
      - readme
      - license
      - license.md
      - changelog
      - changelog.md
      - contributing
      - contributing.md
      - code_of_conduct
      - code_of_conduct.md
    excludeExtensions:
      - .png
      - .jpg
      - .jpeg
      - .gif
      - .bmp
      - .ico
      - .svg
      - .webp
      - .exe
      - .bin
      - .dll
      - .so
      - .dylib
      - .zip
      - .tar
      - .gz
      - .rar
      - .7z
      - .pdf
      - .doc
      - .docx
      - .xls
      - .xlsx
      - .ppt
      - .pptx
      - .mp3
      - .mp4
      - .avi
      - .mov
      - .wav
      - .flac
      - .ttf
      - .otf
      - .woff
      - .woff2
      - .o
      - .obj
      - .a
      - .lib
      - .out
      - .class
      - .jar
      - .war
      - .swp
      - .swo
      - .lock
      - .cache
      - .log
      - .tmp
      - .temp
      - .DS_Store
      - Thumbs.db
    excludeDirectories:
      - .git
      - node_modules
      - archive
      - backups
      - logs
      - context
      - .vscode
      - .idea
      - .pytest_cache
      - __pycache__
      - dist
      - build
      - target
      - venv
      - env
      - .venv
      - .env
      - Pods
      - Carthage
      - CocoaPods
      - .next
      - .nuxt
      - public
      - static
      - assets
      - images
      - img
      - codebase
    excludeFiles:
      - combined_context.yaml
      - package-lock.json
      - yarn.lock
      - pnpm-lock.yaml
      - Gemfile.lock
      - Pipfile.lock
      - Cargo.lock
      - composer.lock
      - go.sum
      - go.mod
      - requirements.txt
      - poetry.lock
      - "*.db"
      - "*.sqlite"
      - "*.sqlite3"
      - "*.fdb"
      - "*.mdb"
      - "*.accdb"
      - "*~"
      - "*.tmp"
      - "*.temp"
      - "*.cache"
      - "*.swp"
      - "*.swo"
